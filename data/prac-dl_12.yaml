- en: '**12'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**12'
- en: INTRODUCTION TO CONVOLUTIONAL NEURAL NETWORKS**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络介绍**
- en: '![image](Images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/common.jpg)'
- en: In this chapter, we’ll introduce a new and potent approach to dealing with multidimensional
    information. In particular, we’ll work through the theory and high-level operation
    of *convolutional neural networks* (CNNs), a cornerstone of modern deep learning.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍一种新的、有力的方法来处理多维信息。特别是，我们将详细讲解*卷积神经网络*（CNN）的理论及其高级操作，它是现代深度学习的基石。
- en: We’ll begin by presenting the motivations behind the development of CNNs. Convolutions
    are the heart of CNNs, so they’ll come next. We’ll discuss them in some detail,
    in particular how they’re used by the CNN. We’ll then introduce a basic CNN and
    work through its anatomy. We’ll use this basic CNN architecture for the remainder
    of the chapter. After we dissect a CNN, we’ll work through how convolutional layers
    work. Then come pooling layers. We’ll see what they do, what benefit they offer,
    and what price they exact in return. To round out our discussion of the fundamental
    components of a CNN, we’ll present the fully connected layers, which, in reality,
    are just the layers of a traditional, fully connected, feed-forward neural network
    like those of [Chapter 8](ch08.xhtml#ch08).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先介绍CNN发展的动机。卷积是CNN的核心，因此它将是接下来的内容。我们将详细讨论卷积操作，特别是它在CNN中的应用。接下来，我们将介绍一个基本的CNN，并解析其结构。我们将使用这个基本的CNN架构贯穿本章。在解析完CNN后，我们将讲解卷积层的工作原理。然后是池化层，我们将了解它们的作用、带来的好处以及它们需要付出的代价。为了完善对CNN基本组件的讨论，我们将介绍全连接层，实际上它们只是传统全连接前馈神经网络的层，就像[第8章](ch08.xhtml#ch08)中所讲的那样。
- en: 'One topic will be conspicuously absent from this chapter: the mechanics of
    training a CNN. In part, we’ll gloss over training because it’s messy once convolutional
    layers are introduced, but primarily because we’ve already discussed backpropagation
    in [Chapter 9](ch09.xhtml#ch09), and we use the same algorithm to train a CNN.
    We calculate the weights and biases of all layers from the average loss over the
    training minibatch and use backprop to determine the derivatives we need to update
    the weights and biases for each stochastic gradient descent step.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将明显缺少一个话题：训练CNN的机制。部分原因是，卷积层引入后，训练过程变得复杂，但主要是因为我们已经在[第9章](ch09.xhtml#ch09)讨论过反向传播，并且我们使用相同的算法来训练CNN。我们通过训练小批量的平均损失来计算所有层的权重和偏置，并使用反向传播来确定更新权重和偏置所需的导数，以进行每次随机梯度下降步骤。
- en: Why Convolutional Neural Networks?
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为什么选择卷积神经网络？
- en: CNNs have several advantages over traditional neural networks. First, the convolutional
    layers of a CNN require vastly fewer parameters than fully connected neural networks,
    as we’ll see later in the chapter. CNNs require fewer parameters because the convolution
    operation applies parameters in each layer to small subsets of the input instead
    of the entire input at once, as is done with a traditional neural network.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: CNN相比传统神经网络有多个优势。首先，CNN的卷积层所需的参数远少于全连接神经网络，正如我们将在本章后续看到的那样。CNN需要更少的参数，因为卷积操作在每一层中将参数应用于输入的小子集，而不是像传统神经网络那样一次性应用于整个输入。
- en: Second, CNNs introduce the idea of *spatial invariance*, the ability to detect
    a spatial relationship in the input regardless of where it appears. For example,
    if the input to a neural network is an image of a cat, a traditional neural network
    will take the image in as a single feature vector, meaning that if a cat appears
    in the upper-left corner of the image, the network will learn that cats can appear
    in the upper-left corners of the image but not that they can also appear in the
    lower-right corners (unless the training data contains examples with cats in the
    lower-right corners). For a CNN, however, the convolution operation can detect
    cats anywhere they appear.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，卷积神经网络（CNN）引入了*空间不变性*的概念，即能够检测输入中出现的空间关系，而不管它出现在哪里。例如，如果神经网络的输入是一张猫的图片，传统的神经网络会将图像作为一个单一的特征向量输入，这意味着如果猫出现在图像的左上角，网络会学习到猫可以出现在左上角，但不会学习到猫也可以出现在右下角（除非训练数据包含右下角有猫的例子）。然而，对于CNN，卷积操作可以在图像的任何位置检测到猫的存在。
- en: While CNNs are usually used with two-dimensional inputs, they can also be used
    with one-dimensional inputs, like the feature vectors we have worked with up to
    now. However, the feature vectors we’ve worked with, like the iris measurements,
    don’t reflect any sort of spatial relationship as the parts of an image of a cat
    do. There’s nothing there for the convolution operation to take advantage of.
    This doesn’t mean that a CNN won’t work, but it does mean that it might not be
    the best sort of model to use. As always, we need to understand how various model
    types operate so we select the best model for the task at hand.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然CNN通常用于二维输入，但它们也可以用于一维输入，比如我们之前处理过的特征向量。然而，我们处理过的特征向量，如虹膜测量数据，并没有反映图像中各部分之间的空间关系。与猫的图像不同，它们没有提供卷积操作可以利用的空间信息。这并不意味着CNN无法工作，但它可能不是最合适的模型类型。正如往常一样，我们需要了解各种模型类型的工作原理，从而选择最适合当前任务的模型。
- en: '**Note** *Depending on who you ask, CNNs were either developed in 1980 by Fukushima
    to implement the Neocognitron model or in 1998 by LeCun et al. as presented in
    their famous paper “Gradient-Based Learning Applied to Document Recognition,”
    which, as of this writing, has been referenced over 21,000 times. My take is that
    both deserve credit, though LeCun used the phrase* convolutional neural network
    *or* convnet *as they are still sometimes called, and what is described in the
    paper is what we will work with in this book. The Neocognitron reflected some
    of the ideas in a CNN, but not CNNs themselves.*'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意** *根据不同的观点，CNN要么是在1980年由福岛开发，用于实现Neocognitron模型，要么是在1998年由LeCun等人开发，并在他们著名的论文《基于梯度的学习应用于文档识别》中提出，截至目前，这篇论文已被引用超过21,000次。我的观点是，两者都值得称赞，尽管LeCun使用了卷积神经网络（convolutional
    neural network）或有时称为convnet这个词，论文中描述的正是我们在本书中将要使用的内容。Neocognitron反映了CNN中的一些思想，但不是CNN本身。*'
- en: Convolution
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 卷积
- en: '*Convolution* involves sliding one thing over another. For us, this means sliding
    a *kernel*, a small 2D array, over the input, which might be the input image to
    the CNN or the output of a lower convolutional layer. There is a formal mathematical
    definition of convolution, but it really won’t help us right now. Luckily, all
    our inputs are discrete, which means we can get away with a bit of a hand-waving.
    For simplicity, we’ll focus on only the two-dimensional case.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*卷积*涉及将一个东西滑动到另一个东西上。对我们来说，这意味着将一个*卷积核*，一个小的二维数组，滑过输入数据，这个输入数据可能是CNN的输入图像，或者是较低层卷积层的输出。卷积有一个正式的数学定义，但现在它对我们并没有太大帮助。幸运的是，我们的所有输入都是离散的，这意味着我们可以稍微“手动演示”一下。为了简化起见，我们将只关注二维的情况。'
- en: Scanning with the Kernel
  id: totrans-13
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用卷积核进行扫描
- en: The *kernel* is the thing we are asking the convolutional layer to learn during
    training. It’s a collection of small 2D arrays that we move over the input. Ultimately,
    the kernels become the weights of a convolutional layer in a CNN.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*卷积核*是我们在训练过程中要求卷积层学习的内容。它是一个由小的二维数组组成的集合，我们将其在输入数据上滑动。最终，这些卷积核会成为CNN中卷积层的权重。'
- en: The essential operation of convolution is taking some small section of the input,
    the same size as the kernel, covering it with the kernel, performing some operation
    on the set of numbers to produce a single output number, and then repeating the
    process after moving the kernel to a new position in the input. Just how far the
    kernel is moved is known as the *stride*. Typically, the stride is 1, meaning
    the kernel slides over one element of the input.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积的基本操作是：取输入数据的一个小部分，大小与卷积核相同，用卷积核覆盖它，对这组数字进行某种操作以产生一个输出数字，然后将卷积核移到输入数据的新位置后重复这一过程。卷积核的移动距离称为*步幅*。通常，步幅为1，意味着卷积核在输入的每个元素上滑动一次。
- en: '[Figure 12-1](ch12.xhtml#ch12fig1) shows the effect of convolution on part
    of an MNIST digit image.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[图12-1](ch12.xhtml#ch12fig1)展示了卷积对MNIST数字图像部分的影响。'
- en: '![image](Images/12fig01.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/12fig01.jpg)'
- en: '*Figure 12-1: Convolving a kernel with an image*'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*图12-1：卷积卷积核与图像*'
- en: The image portion is on the left of [Figure 12-1](ch12.xhtml#ch12fig1), where
    you can see part of a handwritten 8\. The boxes correspond to pixel intensities,
    though for presentation purposes, we’ve expanded the original image so that many
    shades of gray are visible in each “pixel” box. The actual pixel values the convolution
    works with are given next, after the arrow.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[图12-1](ch12.xhtml#ch12fig1)左侧显示的是图像的一部分，您可以看到一个手写的8。框表示像素强度，不过为了展示的目的，我们已经扩展了原始图像，因此每个“像素”框中可见多种灰度色调。卷积操作使用的实际像素值如下所示，箭头之后是对应的值。'
- en: Here, the kernel is
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，卷积核是
- en: '![image](Images/285equ01.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/285equ01.jpg)'
- en: This is the set of numbers we’ll slide over the input pixels. This is a 3 ×
    3 matrix, so we need to cover a 3 × 3 region of the input image. The first 3 ×
    3 region, the upper-left corner, is
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们将滑动覆盖输入像素的一组数字。这是一个 3 × 3 的矩阵，因此我们需要覆盖输入图像的 3 × 3 区域。第一个 3 × 3 区域是左上角，
- en: '![image](Images/285equ02.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/285equ02.jpg)'
- en: 'We said that convolution performs an operation with the kernel and the covered
    region as the input. The operation is straightforward: multiply corresponding
    entries and sum them. Finding the first output value of the convolution begins
    with'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们说过，卷积是通过卷积核和所覆盖区域作为输入进行操作的。操作非常简单：将对应的条目相乘并求和。寻找卷积的第一个输出值开始于
- en: '![image](Images/286equ01.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/286equ01.jpg)'
- en: When the preceding elements are summed, this gives the output value as
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 当前面的元素求和后，得到的输出值是
- en: 0 + (–248) + 0 + (–145) + 759 + (–54) + 0 + (–253) + 0 = 59
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 0 + (–248) + 0 + (–145) + 759 + (–54) + 0 + (–253) + 0 = 59
- en: Okay, the output of the first convolution operation is 59\. What do we do with
    that number? The kernel is 3 × 3, an odd number along each side. This means that
    there is a middle element, the one with the 3 in it. The place in the output array
    where the middle number is gets replaced with the output value, the 59\. [Figure
    12-1](ch12.xhtml#ch12fig1) shows the full output of the convolution. Sure enough,
    the first element of the output is 59, located at the center of the kernel when
    the kernel is covering the upper-left corner.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，第一次卷积操作的输出是 59。那么我们该如何处理这个数字呢？卷积核是 3 × 3，每边都是奇数。这意味着有一个中间元素，就是其中的 3。输出数组中该中间数字的位置将被替换为输出值，即
    59。 [图 12-1](ch12.xhtml#ch12fig1)展示了卷积的完整输出。果然，当卷积核覆盖左上角时，输出的第一个元素就是 59，位于卷积核的中心。
- en: The remaining output values are calculated in precisely the same way but by
    moving the kernel over 1 pixel each time. When the end of a row is reached, the
    kernel moves back to the left side but down 1 pixel. In this way, it slides over
    the entire input image to produce the output shown in [Figure 12-1](ch12.xhtml#ch12fig1),
    just like the scan lines of an old analog television.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 其余的输出值以完全相同的方式计算，但每次将卷积核移动 1 像素。当到达一行的末尾时，卷积核会移回左侧，但会下移 1 像素。通过这种方式，卷积核会滑过整个输入图像，生成[图
    12-1](ch12.xhtml#ch12fig1)中显示的输出，就像老式模拟电视的扫描线一样。
- en: The next output value is
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个输出值是
- en: '![image](Images/286equ02.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/286equ02.jpg)'
- en: which sums to *–* 212, as we see on the right side of [Figure 12-1](ch12.xhtml#ch12fig1).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 其和为*–* 212，正如我们在[图 12-1](ch12.xhtml#ch12fig1)的右侧看到的那样。
- en: Repeating the convolution operation produces the output shown in [Figure 12-1](ch12.xhtml#ch12fig1).
    Notice the empty boxes around the output. These values are empty because the middle
    of our 3 × 3 kernel does not cover the edge of the input array. Therefore, the
    output matrix of numbers is two smaller in each dimension than the input. If the
    kernel were 5 × 5, there would be a border 2 pixels wide instead of 1.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 重复卷积操作会生成[图 12-1](ch12.xhtml#ch12fig1)中显示的输出。注意输出周围的空框。这些值为空，因为我们 3 × 3 的卷积核的中间部分没有覆盖输入数组的边缘。因此，输出矩阵在每个维度上比输入小两个单位。如果卷积核是
    5 × 5，那么边界将是 2 像素宽，而不是 1 像素。
- en: Implementations of 2D convolution need to make a decision about these border
    pixels. There are options, and most toolkits support several of them. One is to
    simply ignore these pixels and make the output smaller than the input, as we’ve
    shown in [Figure 12-1](ch12.xhtml#ch12fig1). This approach is often known as *exact*
    or *valid* because we retain only values that are actually output by the operation.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 2D 卷积的实现需要对这些边界像素做出决策。这里有几种选择，大多数工具包都支持其中的几种。一种方法是简单地忽略这些像素，并使输出比输入小，正如我们在[图
    12-1](ch12.xhtml#ch12fig1)中所示。这种方法通常被称为*精确*或*有效*，因为我们仅保留实际由操作输出的值。
- en: Another approach is to imagine that a border of 0 values surrounds the input
    image. The border is as thick as is needed so that the kernel fits with its middle
    value matching the upper-left pixel of the input. For our example in [Figure 12-1](ch12.xhtml#ch12fig1),
    this means a border of 1 pixel because the kernel is 3 × 3 and there is one element
    on either side of the kernel’s center value. If the kernel were 5 × 5, the border
    would be 2 pixels since there are two values on either side of the kernel center.
    This is known as *zero-padding* and gives an output that is the same size as the
    input. Instead of convolving a 28×28 pixel MNIST digit image with a 3 × 3 kernel
    and getting a 26×26 pixel output as shown in [Figure 12-1](ch12.xhtml#ch12fig1),
    we get an output that is also 28×28 pixels.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是想象输入图像的周围有一个0值的边框。边框的厚度是根据需要的，以便卷积核能够对准输入图像的左上角像素。对于[图12-1](ch12.xhtml#ch12fig1)中的示例，这意味着一个1像素的边框，因为卷积核是3×3的，卷积核中心的两侧各有一个元素。如果卷积核是5×5，那么边框将是2像素，因为卷积核中心的两侧各有两个元素。这种做法被称为*零填充*，它产生的输出与输入图像的大小相同。这样一来，卷积一个28×28像素的MNIST数字图像与一个3×3的卷积核时，输出仍然是28×28像素，而不是如[图12-1](ch12.xhtml#ch12fig1)中那样得到一个26×26像素的输出。
- en: If we zero pad the example image in [Figure 12-1](ch12.xhtml#ch12fig1), we can
    fill in the first empty output square like so
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们对[图12-1](ch12.xhtml#ch12fig1)中的示例图像进行零填充，我们可以像这样填充第一个空的输出方块。
- en: '![image](Images/287equ01.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/287equ01.jpg)'
- en: which sums to *–* 213\. This means that the upper-left corner of the output
    matrix in [Figure 12-1](ch12.xhtml#ch12fig1), which currently has an empty box,
    could be replaced by *–* 213\. Similarly, the rest of the empty boxes would have
    values, and the output of the convolution operation would be 28×28 pixels.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 结果加和为*–* 213。这意味着在[图12-1](ch12.xhtml#ch12fig1)中输出矩阵的左上角（当前为空的方框）可以被*–* 213替代。类似地，其余的空方框也将有相应的值，卷积操作的输出将是28×28像素。
- en: Convolution for Image Processing
  id: totrans-39
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 图像处理的卷积
- en: Convolution, when used in a neural network, is sometimes viewed as magical,
    a special operation that lets convolutional neural networks do the wonderful things
    that they can do. This is more or less true, but the convolution operation is
    certainly not anything new. Even if we ignore mathematics entirely and think only
    of the discrete convolution of 2D images, we see that image scientists were using
    convolution for image processing decades before convolution was applied to machine
    learning.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 当卷积应用于神经网络时，有时被视为一种神奇的操作，这使得卷积神经网络能够做出它们所能做到的精彩事情。这在某种程度上是正确的，但卷积操作绝对不是新鲜事物。即使我们完全忽略数学，单纯考虑二维图像的离散卷积，我们也会发现图像科学家在卷积被应用于机器学习之前几十年就已经在图像处理中使用卷积了。
- en: The convolution operation allows for all manner of image processing. For example,
    consider the images shown in [Figure 12-2](ch12.xhtml#ch12fig2).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积操作可以用于各种图像处理。例如，考虑[图12-2](ch12.xhtml#ch12fig2)中显示的图像。
- en: '![image](Images/12fig02.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/12fig02.jpg)'
- en: '*Figure 12-2: 5 × 5 convolution kernels applied to an image*'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*图12-2：应用于图像的5×5卷积核*'
- en: The original moon image is on the upper left. The other three images are the
    output from convolving the moon image with different 5 × 5 kernels. Moving clockwise
    from the upper right, the kernels either emphasize edges, diagonal structures
    (upper left to lower right), or blur the input image. All of this is accomplished
    by changing the values in the kernel, but the convolution operation remains the
    same.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的月球图像位于左上角。其他三个图像是将月球图像与不同的5×5卷积核进行卷积后的输出。从右上角开始按顺时针方向移动，卷积核分别强调边缘、对角线结构（从左上到右下），或是模糊输入图像。所有这些都是通过改变卷积核中的值来实现的，但卷积操作保持不变。
- en: From a machine learning perspective, the power of a convolutional approach comes
    partially from the savings in terms of parameters. If a model can learn a set
    of kernels, that is a smaller set of numbers to learn than the weights for a fully
    connected model. This is a good thing on its own. The fact that a convolution
    can pull out other information about an image, such as its slowly changing components
    (the blur of [Figure 12-2](ch12.xhtml#ch12fig2)), its rapidly changing components
    (the edges of [Figure 12-2](ch12.xhtml#ch12fig2)), or even components along a
    specific direction (the diagonals of [Figure 12-2](ch12.xhtml#ch12fig2)), means
    that the model gains insight as to what is in the input. And, since we move the
    kernel over the image, we’re not dependent upon *where* in the image these structures
    occur.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 从机器学习的角度来看，卷积方法的优势部分来自于参数的节省。如果一个模型能够学习一组卷积核，那么它需要学习的数字集合比全连接模型的权重集合要小。这本身就是一件好事。卷积可以提取图像中的其他信息，例如其缓慢变化的部分（[图
    12-2](ch12.xhtml#ch12fig2) 的模糊）、其快速变化的部分（[图 12-2](ch12.xhtml#ch12fig2) 的边缘），甚至是沿特定方向的部分（[图
    12-2](ch12.xhtml#ch12fig2) 的对角线），这意味着模型能够洞察输入内容。而且，由于我们将卷积核在图像上滑动，因此我们不依赖于这些结构在图像中出现的*位置*。
- en: Anatomy of a Convolutional Neural Network
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 卷积神经网络的结构
- en: Medical students learn about anatomy by dissecting a cadaver to see the parts
    and how they relate to each other. In similar, though less challenging, fashion,
    we’ll start with the body of a CNN, an illustration of its basic architecture,
    and then pull it apart to learn what each component is and what it does.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 医学生通过解剖尸体来学习解剖学，看看各个部分及其相互关系。类似地，虽然挑战性较小，我们将从 CNN 的主体开始，展示其基本架构，然后拆解它，学习每个组件是什么以及它的功能。
- en: '[Figure 12-3](ch12.xhtml#ch12fig3) shows us our body. This is the default example
    CNN used by the Keras toolkit to train a model that classifies MNIST digits. We’ll
    use it as our standard for the remainder of this chapter.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 12-3](ch12.xhtml#ch12fig3) 展示了我们的主体。这是 Keras 工具包用于训练一个分类 MNIST 数字模型的默认示例
    CNN。我们将在本章余下部分以它作为标准。'
- en: '![image](Images/12fig03.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/12fig03.jpg)'
- en: '*Figure 12-3: The architecture of a basic convolutional neural network*'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 12-3：基本卷积神经网络的架构*'
- en: How do we interpret this figure? Like a traditional neural network, a CNN has
    an input and an output. In this case, the input is the digit image on the upper
    left. The network then flows left to right, following the arrows. At the end of
    the top row, the network continues on the following row. Note, we’ve duplicated
    the layer at the end of the top row and placed it at the beginning of the next
    row for presentation purposes.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何解读这张图？像传统的神经网络一样，CNN 也有输入和输出。在这种情况下，输入是左上角的数字图像。然后网络从左到右流动，按照箭头的方向。顶部行结束时，网络继续进入下一行。请注意，为了展示的目的，我们将顶部行末尾的层复制并放置在下一行的开头。
- en: The flow continues along the bottom row, again left to right, until the output
    is reached. The output here is a softmax layer to give us the likelihoods of each
    of the possible digits, just as we saw for the traditional neural networks of
    [Chapter 10](ch10.xhtml#ch10).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 数据流继续沿着底行，从左到右，直到到达输出。这里的输出是一个 softmax 层，用来给出每个可能数字的概率，正如我们在[第 10 章](ch10.xhtml#ch10)的传统神经网络中看到的那样。
- en: Different Types of Layers
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 不同类型的层
- en: 'Between each arrow is a layer of the network. The first thing we notice is
    that, unlike a traditional neural network, a CNN has many kinds of layers. Let’s
    list them here. We’ll discuss each in turn:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 每个箭头之间是网络的一层。我们首先注意到，与传统的神经网络不同，CNN 有许多种类的层。我们在这里列出它们，我们将逐一讨论每一种：
- en: Convolutional (*Conv*)
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积（*Conv*）
- en: ReLU
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ReLU
- en: Pooling (*Pool*)
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 池化（*Pool*）
- en: Dropout
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dropout
- en: Flatten
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扁平化
- en: Dense
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 密集层
- en: We should note that we’re using the Keras names for the layers. For instance,
    Keras uses *Dense* for what many other toolkits call *fully connected* or even
    *InnerProduct* layers.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该注意到，我们使用的是 Keras 中层的命名方式。例如，Keras 使用 *Dense* 来表示许多其他工具包称为 *fully connected*
    或甚至 *InnerProduct* 的层。
- en: Several of these layers should already be familiar. We know a ReLU layer implements
    a rectified linear unit that takes each of its inputs and asks if it is greater
    than or less than 0\. If the input is less than 0, the output is 0; otherwise,
    the output is the input. We can express this mathematically as
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这些层中的几个应该已经很熟悉了。我们知道 ReLU 层实现了一个整流线性单元，它对每个输入进行判断，看看它是否大于或小于 0。如果输入小于 0，输出为
    0；否则，输出就是输入值。我们可以用数学公式表示为
- en: ReLU(*x*) = max(0, *x*)
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU(*x*) = max(0, *x*)
- en: where the *max* function returns the largest of its two arguments.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，*max*函数返回其两个参数中的最大值。
- en: Likewise, we mentioned dropout in [Chapter 9](ch09.xhtml#ch09). Dropout selects
    a percentage of its outputs at random during training and sets them to 0\. This
    provides a powerful form of regularization to help the network learn meaningful
    representations of the input data. There are two dropout layers in our basic CNN.
    The first uses a probability of 25 percent, meaning during any minibatch pass
    while training, some 25 percent of the outputs will be set to 0\. The second dropout
    layer uses a probability of 50 percent.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们在[第9章](ch09.xhtml#ch09)中提到过dropout。Dropout在训练过程中随机选择一定比例的输出，并将它们设置为0。这为网络提供了一种强大的正则化形式，帮助网络学习输入数据的有意义表示。我们的基本CNN中有两个dropout层。第一个使用25%的概率，这意味着在训练中的每次小批量传递时，大约25%的输出将被设置为0。第二个dropout层使用50%的概率。
- en: The *Flatten* and *Dense* layers are old friends, though we know them by another
    name and not as independent entities. Our traditional feedforward neural network
    uses fully connected layers to process a one-dimensional vector. Here, Flatten
    and Dense work together to implement a fully connected layer. The Flatten layer
    takes its input—usually a four-dimensional array (we’ll see why later)—and turns
    it into a vector. It does something similar to what we did to construct the vector
    form of the MNIST dataset, where we put the pixels of each row end-to-end to unravel
    the two-dimensional image. The Dense layer implements a traditional neural network
    layer, where each input value is mapped to each node of the Dense layer. Typically,
    the output of the Dense layer is passed to another Dense layer or a softmax layer
    to let the network make predictions.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '*Flatten*层和*Dense*层是老朋友，尽管我们通常不是将它们视为独立的实体，而是以其他名字来认识它们。我们传统的前馈神经网络使用全连接层来处理一维向量。在这里，Flatten和Dense一起实现了一个全连接层。Flatten层接收其输入——通常是一个四维数组（我们稍后会看到为什么）——并将其转换为一个向量。它的工作方式类似于我们在构建MNIST数据集的向量形式时所做的操作，我们将每一行的像素首尾相接，从而展开二维图像。Dense层实现了一个传统的神经网络层，其中每个输入值映射到Dense层的每个节点。通常，Dense层的输出会传递给另一个Dense层或softmax层，让网络进行预测。'
- en: Internally, many layers of a CNN expect four-dimensional arrays as inputs and
    produce four-dimensional arrays as outputs. The first dimension is the number
    of inputs in the minibatch. So, if we have a minibatch of 24, the first dimension
    of the 4D array will be 24.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在内部，许多CNN层期望四维数组作为输入，并产生四维数组作为输出。第一维是小批量中的输入数量。因此，如果我们有一个包含24个样本的小批量，那么四维数组的第一维将是24。
- en: The second and third dimensions are called the *height* and *width*. If the
    input to a layer is the input to the model (say, an image), then these dimensions
    are truly the height and width dimensions of the image. If the input is really
    the output of some other layer, say a (yet to be described) convolutional layer,
    the *height* and *width* refer to the output from applying a convolutional kernel
    to some input. For example, the output in [Figure 12-1](ch12.xhtml#ch12fig1) has
    height and width of 26.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 第二维和第三维被称为*高度*和*宽度*。如果一层的输入是模型的输入（比如一张图像），那么这些维度实际上就是图像的高度和宽度。如果输入实际上是某一层的输出，比如一个（尚未描述的）卷积层的输出，*高度*和*宽度*指的是将卷积核应用于某个输入后的输出。例如，[图
    12-1](ch12.xhtml#ch12fig1)中的输出具有26的高度和宽度。
- en: The last dimension is the number of channels, if an input image; or the number
    of *feature maps*, if the output of a convolutional or pooling layer. The number
    of channels in an image is simply the number of bands, where a grayscale image
    has a single band and a color image typically has three bands, one each for red,
    green, and blue. Some color images also have an alpha channel used to specify
    how transparent a pixel is, but these are typically dropped before passing the
    image through a CNN.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一维是通道的数量，如果是输入图像；或者如果是卷积层或池化层的输出，则是*特征图*的数量。图像中的通道数量就是带的数量，灰度图像有一个带，而彩色图像通常有三个带，分别代表红色、绿色和蓝色。有些彩色图像还包含一个alpha通道，用来指定像素的透明度，但这些通道通常在将图像输入CNN之前被丢弃。
- en: The output in [Figure 12-1](ch12.xhtml#ch12fig1) is called a *feature map* because
    it is the response from convolving a kernel over an input. As we saw in [Figure
    12-2](ch12.xhtml#ch12fig2), convolving a kernel over an image can pull out features
    in the image, so the outputs of the kernels used by a convolutional layer are
    called *feature maps*.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 12-1](ch12.xhtml#ch12fig1)中的输出被称为*特征图*，因为它是通过将一个卷积核作用于输入后得到的响应。正如我们在[图 12-2](ch12.xhtml#ch12fig2)中看到的，将卷积核作用于图像可以提取图像中的特征，因此卷积层使用的卷积核的输出被称为*特征图*。'
- en: 'This leaves two layers to investigate: *Convolutional* and *Pooling*. These
    layers are new.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这剩下两个层需要研究：*卷积层*和*池化层*。这些层是新的。
- en: In our basic CNN, the convolutions operate on sets of two-dimensional inputs
    where by *set* I mean a stack of two-dimensional arrays, where the third dimension
    is the number of channels or feature maps. This means that unlike every other
    model we’ve looked at in this book, the input here really is the full image, not
    a vector created from the image. In terms of CNNs, however, the convolutions need
    not operate on only two-dimensional inputs. Three-dimensional convolutions exist,
    as do one-dimensional, though both are seldom used compared to two-dimensional
    convolutions.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的基础卷积神经网络（CNN）中，卷积操作作用于二维输入的集合，*集合*指的是一堆二维数组，其中第三维是通道数或特征图的数量。这意味着，与本书中我们之前讨论的其他模型不同，这里的输入真的是完整的图像，而不是从图像中创建的向量。然而，就CNN而言，卷积操作不仅仅局限于二维输入。三维卷积存在，一维卷积也有，尽管这两者相比于二维卷积较少使用。
- en: A pooling layer is used to reduce the spatial dimension of its input by combining
    input values according to some rule. The most common rule is *max*, where the
    largest value in the small block moved over the input is kept; the other values
    are discarded. Again, we’ll cover pooling layers at length in this chapter.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 池化层用于通过根据某些规则组合输入值来减少其空间维度。最常见的规则是*最大池化*，即保留小块中最大值，其余值被丢弃。我们将在本章详细讨论池化层。
- en: Many other layer types can be used by modern networks, and many of these are
    directly supported in Keras already, though it’s possible to add your own layers.
    This flexibility is one reason Keras often quickly supports new deep learning
    developments. As with a traditional neural network, for a layer to have weights
    that can be learned, the layer needs to be differentiable in a mathematical sense
    so that the chain rule can continue, and the partial derivatives can be calculated
    to learn how to adjust the weights during gradient descent. If the previous sentence
    is not clear, it’s time to review the backprop section of [Chapter 9](ch09.xhtml#ch09).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现代网络可以使用许多其他类型的层，其中许多层已经在Keras中得到直接支持，尽管你也可以添加自定义层。这种灵活性是Keras通常迅速支持新的深度学习发展的原因之一。与传统神经网络一样，为了让一个层具有可以学习的权重，该层需要在数学意义上可微分，以便链式法则能够继续应用，从而计算偏导数来学习如何在梯度下降过程中调整权重。如果上一句不清楚，是时候复习一下[第9章](ch09.xhtml#ch09)的反向传播部分了。
- en: Passing Data Through the CNN
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 将数据通过CNN传递
- en: Let’s look again at [Figure 12-3](ch12.xhtml#ch12fig3). A lot is happening here
    beyond just the order and names of the layers. Many layers have numbers in italics
    running along the bottom. These numbers represent the dimensions of the output
    of the layer, the height, width, and number of feature maps. If the layer has
    only a single number, it outputs a vector with that many elements.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再来看一下[图 12-3](ch12.xhtml#ch12fig3)。这里发生了很多事情，不仅仅是层的顺序和名称。许多层在底部有以斜体显示的数字，这些数字表示该层输出的维度，即高度、宽度和特征图的数量。如果层只有一个数字，则它输出一个包含该数字元素的向量。
- en: The input to the CNN is a 28 × 28 × 1 image. The output of a convolutional layer
    is a set of feature maps. Thus the output of the first convolutional layer is
    26 × 26 × 32, meaning there are 32 feature maps, each a 26 × 26 image calculated
    from the single 28 × 28 × 1 input image. Similarly, the output of the second convolutional
    layer is 24 × 24 × 64, a set of 64 feature maps derived from the 26 × 26 × 32
    input, which was itself the output of the first convolutional layer.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: CNN的输入是一个28 × 28 × 1的图像。卷积层的输出是一组特征图。因此，第一个卷积层的输出是26 × 26 × 32，意味着有32个特征图，每个特征图是从单一的28
    × 28 × 1输入图像计算出来的26 × 26图像。类似地，第二个卷积层的输出是24 × 24 × 64，这是从26 × 26 × 32的输入中衍生出来的64个特征图，而这个输入本身是第一个卷积层的输出。
- en: We see that the pooling layer at the end of the first row takes its 24 × 24
    × 64 input and reduces it to 12 × 12 × 64\. The “max” label tells us what the
    pooling is doing; it takes a 2 × 2 region of the input and returns the largest
    value. Since the input is 2 × 2 and it returns only one value, this reduces each
    24 × 24 input to a 12 × 12 output. This process is applied to each feature map
    so that the output is 12 × 12 × 64.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到第一行末尾的池化层，它将24 × 24 × 64的输入缩小为12 × 12 × 64。“max”标签告诉我们池化操作的内容；它取输入中的一个2
    × 2区域，并返回其中的最大值。由于输入是2 × 2，并且返回一个值，这将每个24 × 24的输入缩小为12 × 12的输出。这个过程应用于每个特征图，因此输出是12
    × 12 × 64。
- en: Looking at the bottom row of [Figure 12-3](ch12.xhtml#ch12fig3) shows us that
    the Flatten layer takes the 12 × 12 × 64 output of the pooling layer and turns
    it into a vector of 9,216 elements. Why 9,216? Because 12 × 12 × 64 = 9,216\.
    Next, the Dense layer has 128 nodes, and, finally, our output softmax has 10 nodes
    because there are 10 classes, the digits 0 through 9.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 看[图12-3](ch12.xhtml#ch12fig3)底部一行，我们看到Flatten层将池化层的12 × 12 × 64的输出转化为一个9,216元素的向量。为什么是9,216？因为12
    × 12 × 64 = 9,216。接下来，Dense层有128个节点，最后，输出的softmax层有10个节点，因为有10个类别，即数字0到9。
- en: In [Figure 12-3](ch12.xhtml#ch12fig3), the ReLU and Dropout layers have no numbers
    below them. These layers do not alter the shape of their inputs. They simply perform
    some operation on each of the elements regardless of the shape.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图12-3](ch12.xhtml#ch12fig3)中，ReLU和Dropout层下方没有数字。这些层不会改变它们输入的形状。它们只是对每个元素执行某些操作，无论形状如何。
- en: 'The convolutional layers of our basic CNN have other numbers associated with
    them: “3 × 3” and “32” or “64”. The 3 × 3 tells us the size of the convolutional
    kernel, and the 32 or 64 tells us the number of feature maps.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们基本的卷积神经网络（CNN）中的卷积层有其他数字与之相关：“3 × 3”和“32”或“64”。3 × 3表示卷积核的大小，而32或64则表示特征图的数量。
- en: We already alluded to the 2 × 2 part of the pooling layer. This represents the
    size of the pooling kernel, which, much like a convolutional kernel, slides over
    the input, feature map by feature map (or channel by channel), to reduce the size
    of the input. Working with a 2 × 2 pooling kernel implies that, typically, the
    output will be one-half the size of the input in each of the row and column dimensions.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到了池化层中的2 × 2部分。这表示池化核的大小，它类似于卷积核，滑动在输入上，一个特征图接一个特征图（或者逐通道滑动），以减少输入的大小。使用2
    × 2池化核意味着通常输出的大小会在行和列维度上分别是输入的一半。
- en: '[Figure 12-3](ch12.xhtml#ch12fig3) has familiar parts, but the presentation
    is new to us, and we have these mysterious new layers to think about, like convolutional
    and pooling layers, so we are sure to be somewhat nebulous in our understanding
    right now. That is perfectly fine. We have new ideas and some visual indications
    of how they link together to make a CNN. For now, this is all we need. The remainder
    of this chapter will, I hope, be a series of “aha!” moments for you as you think
    back to this figure and its parts. When you understand what each is doing, you’ll
    start to see why they are where they are in the processing chain, leading from
    image input to output softmax predictions.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[图12-3](ch12.xhtml#ch12fig3)有一些熟悉的部分，但呈现方式对我们来说是新的，并且我们有一些神秘的新层需要思考，比如卷积层和池化层，因此我们现在的理解可能会有些模糊。这完全没问题。我们获得了新的想法和一些可视化的提示，帮助我们理解它们是如何联系在一起构成CNN的。现在，这些就是我们所需要的。本章的其余部分，希望能给你带来一系列的“啊哈！”时刻，当你回顾这张图及其各个部分时，你会发现每个部分的作用。理解了它们的功能后，你就会明白它们在处理链中，如何从图像输入到输出softmax预测的过程中，处于什么位置。'
- en: Convolutional Layers
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 卷积层
- en: If our discussion of convolution ended with the preceding sections, we’d understand
    the essential operation but still be in the dark about exactly *how* a convolutional
    layer in a CNN works. Bearing this in mind, let’s look at how the convolution
    idea generalizes across the inputs and outputs of a CNN’s convolutional layer.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们关于卷积的讨论在前面的部分就结束了，我们虽然理解了基本的操作，但仍然不清楚一个CNN中的卷积层究竟是如何工作的。考虑到这一点，我们来看一下卷积思想是如何在CNN的卷积层的输入和输出之间进行推广的。
- en: How a Convolution Layer Works
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 卷积层的工作原理
- en: The input and output of a convolutional layer can both be thought of as stacks
    of 2D arrays (or matrices). The operation of the convolutional layer is best illustrated
    with a simple example showing how to map the input stack of arrays to the output
    stack.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层的输入和输出都可以看作是二维数组（或矩阵）的堆叠。卷积层的操作最好通过一个简单的示例来说明，展示如何将输入数组堆栈映射到输出堆栈。
- en: Before we present our example, we need to introduce some terminology. We previously
    described the convolution operation in terms of applying a kernel to an input,
    both of which are 2D. We’ll continue to use the term *kernel* for this single,
    2D matrix. When implementing a convolutional layer, however, we’ll soon see that
    we need stacks of kernels, which are typically referred to in machine learning
    as *filters*. A filter is a stack of kernels. The filter, via its kernels, is
    applied over the input stack to produce the output stack. Since during training
    the model is learning kernels, it is fair to say that the model is also learning
    filters.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们展示示例之前，需要先介绍一些术语。我们之前用一个核对输入进行卷积操作，其中核和输入都是二维的。我们将继续使用*核*这一术语来指代这个单一的二维矩阵。然而，在实现卷积层时，我们很快会发现我们需要一堆核，这些通常在机器学习中被称为*滤波器*。滤波器是由多个核堆叠而成的。通过它的核，滤波器应用到输入堆栈上，产生输出堆栈。由于在训练过程中模型学习的是核，所以可以说模型也在学习滤波器。
- en: For our example, the input is a stack of two 5 × 5 arrays, the kernel size is
    3 × 3, and we want an output stack that is three deep. Why three? Because, as
    the designer of the CNN architecture, we believe that learning three outputs will
    help the network learn the task at hand. The convolution operation determines
    the width and height of each output array; we select the depth. We’ll use valid
    convolution, losing a border of thickness one on the output, meaning our input
    will drop two in width and height. Therefore, a 5 × 5 input convolved with a 3
    × 3 kernel will create a 3 × 3 output.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的示例，输入是一个由两个5 × 5数组构成的堆栈，核的大小是3 × 3，我们希望输出堆栈有三层。为什么是三层？因为作为CNN架构的设计者，我们认为学习三个输出将帮助网络更好地完成当前的任务。卷积操作决定了每个输出数组的宽度和高度；我们选择深度。我们将使用有效卷积，丢失输出边缘一像素的厚度，意味着输入的宽度和高度将分别减少2。因此，一个5
    × 5的输入经过一个3 × 3的核卷积后，会得到一个3 × 3的输出。
- en: That accounts for the change in dimension, but how do we go from a stack of
    two arrays to a stack of three? The key to mapping the 5 × 5 × 2 input to the
    desired 3 × 3 × 3 output is the set of kernels, the filter, learned during training.
    Let’s see how the filter gives us the mapping we want.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这解释了维度的变化，但我们如何从一个包含两个数组的堆栈变成一个包含三个数组的堆栈呢？将5 × 5 × 2的输入映射到所需的3 × 3 × 3的输出的关键在于训练过程中学习到的核集合，也就是滤波器。让我们看看这个滤波器是如何为我们提供所需的映射的。
- en: We’ll assume we already know the filters at this point, each of which is a 3
    × 3 × 2 stack of kernels. In general, if there are *M* arrays in the input stack
    and we want *N* arrays in the output stack using a kernel that is *K* × *K*, then
    we need a set of *N* filters, each one of which is a stack of *K* × *K* kernels
    *M* deep. Let’s explore why.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设此时已经知道滤波器，每个滤波器都是一个3 × 3 × 2的核堆栈。通常情况下，如果输入堆栈中有*M*个数组，并且我们希望输出堆栈有*N*个数组，使用一个*K*
    × *K*的核，那么我们需要一组*N*个滤波器，每个滤波器都是*M*深的*K* × *K*核的堆叠。让我们来探讨一下为什么。
- en: 'If we break up the stack so we can see each element clearly, our input stack
    looks like this:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将堆栈拆开，以便清楚地看到每个元素，我们的输入堆栈看起来是这样的：
- en: '![image](Images/293equ01.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/293equ01.jpg)'
- en: We have two 5 × 5 matrices labeled 0 and 1\. The values were selected at random.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有两个5 × 5的矩阵，标记为0和1。它们的值是随机选择的。
- en: To get an output stack of three, we need a set of three filters. The stack of
    kernels in each filter is two deep, to mirror the number of arrays in the input
    stack. The kernels themselves are 3 × 3, so we have three 3 × 3 × 2 filters, where
    we convolve each kernel in the filter with the corresponding input array. The
    three filters are
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 为了得到一个深度为三的输出堆栈，我们需要一组三个滤波器。每个滤波器中的核堆栈深度为二，以便与输入堆栈中的数组数量相匹配。核本身是3 × 3的，所以我们有三个3
    × 3 × 2的滤波器，每个核与对应的输入数组进行卷积。这三个滤波器是：
- en: '![image](Images/293equ02.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/293equ02.jpg)'
- en: where we’ve added 0 and 1 labels to show which kernels are applied to which
    input stack arrays. We also have a bias vector, as we did for the traditional
    neural network layers. This is a vector, one value for each kernel stack, that
    we add in at the end to help align the output of the convolutional layer to the
    data, just as we did for the traditional neural network layers. The bias adds
    one more degree of freedom to the layer—one more thing that can be learned to
    help the layer learn the most it can from the data. For our example, the bias
    vector is *b* = *{*1,0,2*}*, selected at random.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里添加了0和1标签，以显示哪些核应用于哪些输入堆栈数组。我们还添加了一个偏置向量，正如传统神经网络层一样。这个向量是每个核堆栈的一个值，我们在最后加上它，以帮助调整卷积层的输出与数据对齐，就像我们在传统神经网络层中所做的那样。偏置为层增加了一个自由度——它为学习提供了更多的东西，帮助层从数据中学习到尽可能多的信息。在我们的示例中，偏置向量是*b*
    = *{1,0,2}*，是随机选择的。
- en: To get the output stack, we convolve each kernel of each filter with the corresponding
    input array, sum the elements of the resulting output, and add the bias value.
    For filter *k*[0], we convolve the first input array with the first kernel to
    get
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 为了得到输出堆栈，我们将每个滤波器的每个核与相应的输入数组进行卷积，求得结果输出的元素和，并加上偏置值。对于滤波器*k*[0]，我们将第一个输入数组与第一个核卷积得到：
- en: '![image](Images/294equ01.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/294equ01.jpg)'
- en: 'Note we’re using * to mean the full convolution operation, which is fairly
    standard. We repeat this operation for the second kernel in *k*[0], applying it
    to the second array of the input:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用*来表示完整的卷积操作，这是一个相当标准的表示方法。我们对*k*[0]中的第二个核重复这一操作，将其应用到输入的第二个数组：
- en: '![image](Images/294equ02.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/294equ02.jpg)'
- en: 'Finally, we sum the two convolution outputs and add in the bias value:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将两个卷积输出相加，并加入偏置值：
- en: '![image](Images/294equ03.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/294equ03.jpg)'
- en: This gives us the first output array, the application of filter *k*[0] to the
    input stack.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们带来了第一个输出数组，即将滤波器*k*[0]应用于输入堆栈的结果。
- en: We repeat this process for filters *k*[1] and *k*[2] to get their outputs so
    that the final convolutional layer output for the given input is
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对滤波器*k*[1]和*k*[2]重复这个过程，以获得它们的输出，从而得到给定输入的最终卷积层输出。
- en: '![image](Images/294equ04.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/294equ04.jpg)'
- en: where we have written the stacked arrays side by side, a 3 × 3 × 3 output, as
    desired.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将堆叠的数组并排写出，得到一个3 × 3 × 3的输出，正如我们所期望的那样。
- en: Our convolutional layer example mapped a 5 × 5 × 2 input to a 3 × 3 × 3 output.
    If we naïvely used a fully connected layer instead, we would need a weight matrix
    that has 50 × 27 = 1350 weights that need to be learned. In contrast, the convolutional
    layer used only 3 × 3 × 2 weights per filter and three filters for a total of
    54 weights, excluding bias values. This is a significant reduction.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的卷积层示例将一个5 × 5 × 2的输入映射到一个3 × 3 × 3的输出。如果我们天真地使用一个全连接层，反而需要一个包含50 × 27 = 1350个权重的权重矩阵，这些权重需要被学习。相比之下，卷积层每个滤波器只用了3
    × 3 × 2个权重，并且使用了三个滤波器，总共只有54个权重（不包括偏置值）。这是一种显著的减少。
- en: Using a Convolutional Layer
  id: totrans-109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用卷积层
- en: The preceding example showed us how a convolutional layer works. Now let’s see
    the effect of one. Imagine that we’ve trained the network shown in [Figure 12-3](ch12.xhtml#ch12fig3),
    so we have the weights and biases we need to run unknown images through the network.
    (You’ll see how to train a CNN in Chapter Experiments with Keras and MNIST.)
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 上述示例向我们展示了卷积层是如何工作的。现在让我们看看一个卷积层的效果。假设我们已经训练了[图12-3](ch12.xhtml#ch12fig3)中显示的网络，因此我们已经有了运行未知图像所需的权重和偏置值。（你将在《Keras与MNIST实验》章节中看到如何训练CNN。）
- en: 'The first layer of the network in [Figure 12-3](ch12.xhtml#ch12fig3) is a convolutional
    layer that maps a 28 × 28 × 1 input, the single-channel grayscale digit image,
    to a 26 × 26 × 32 output using a filter with 32 3 × 3 kernels. Therefore, we know
    that the weights between the input image and output fit in an array that is 3
    × 3 × 1 × 32: 3 × 3 for the kernel size, 1 for the number of input channels, and
    32 for the number of kernels in the filter.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 网络的第一层在[图12-3](ch12.xhtml#ch12fig3)中是一个卷积层，它将一个28 × 28 × 1的输入——单通道灰度数字图像——映射到一个26
    × 26 × 32的输出，使用的是具有32个3 × 3核的滤波器。因此，我们知道，输入图像和输出之间的权重适合于一个3 × 3 × 1 × 32的数组：3
    × 3是核的大小，1是输入通道数，32是滤波器中核的数量。
- en: 'After training, what do the 32 3 × 3 kernels of the filter actually look like?
    We can extract them from the trained model and print them as a set of 32 3 × 3
    matrices. Here are the first two:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后，滤波器的32个3 × 3核实际长什么样？我们可以从训练好的模型中提取它们，并将它们作为32个3 × 3矩阵打印出来。以下是前两个：
- en: '![image](Images/295equ01.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/295equ01.jpg)'
- en: This is nice, but not particularly helpful for building intuition about what
    the kernels do.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这很不错，但对于构建关于核的作用的直觉帮助不大。
- en: We can also visualize the kernels of a filter by converting the matrices to
    images. To get the kernels as images, we first note that all the kernel values
    happen to fit in the range [*–*0.5,+0.5], so if we add 0.5 to each kernel value,
    we’ve mapped the range to [0,1]. After this, multiplication by 255 converts the
    kernel values to byte values, the same values a grayscale image uses. Additionally,
    a value of 0 is now 127, which is a middle gray value.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过将矩阵转换为图像来可视化滤波器的核。为了将核转换为图像，我们首先注意到所有的核值恰好都在 [*–*0.5,+0.5] 范围内，因此，如果我们将每个核值加上
    0.5，就将范围映射到 [0,1]。之后，乘以 255 将核值转换为字节值，这与灰度图像使用的值相同。此外，值为 0 时，变为 127，即中灰色值。
- en: After this conversion, the kernels can be shown as grayscale images, where negative
    kernel values are closer to black, and positive kernel values are closer to white.
    A final step is needed, however, because the mapped kernels are still only 3×3
    pixels. The last step is to upscale the 3 × 3 images to 64×64 pixels. We’ll upscale
    in two different ways. The first uses nearest-neighbor sampling to show the kernel
    in blocks. The second uses a Lanczos filter, which smooths the image, making it
    easier to see the orientation of the kernel. [Figure 12-4](ch12.xhtml#ch12fig4)
    shows the kernel images with the block versions on top and the smoothed versions
    on the bottom.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成此转换后，核可以作为灰度图像显示，其中负值核接近黑色，正值核接近白色。然而，还需要一个最后的步骤，因为映射后的核仍然仅是 3×3 像素。最后一步是将
    3 × 3 的图像放大到 64×64 像素。我们将以两种不同的方式进行放大。第一种方式使用最近邻采样来显示核的块状效果。第二种方式使用 Lanczos 滤波器，这种方法平滑图像，使得核的方向更加清晰。[图
    12-4](ch12.xhtml#ch12fig4) 显示了核图像，顶部是块状版本，底部是平滑后的版本。
- en: '![image](Images/12fig04.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/12fig04.jpg)'
- en: '*Figure 12-4: The 32 learned kernels of the first convolutional layer (top).
    Smoothed versions to show the orientations more clearly (bottom).*'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 12-4：第一卷积层的 32 个学习到的核（顶部）。平滑版本，以更清晰地显示方向（底部）。*'
- en: These images represent the 32 kernels learned by the first convolutional layer
    of the model in [Figure 12-3](ch12.xhtml#ch12fig3). There is just enough detail
    in the images to hint that the kernels are selecting for structure in specific
    directions, just like the kernel that produced the image on the lower right of
    [Figure 12-2](ch12.xhtml#ch12fig2), which emphasized diagonal structures.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这些图像表示了模型中[图 12-3](ch12.xhtml#ch12fig3)第一卷积层学习到的 32 个核。图像中包含了足够的细节，暗示这些核选择了特定方向上的结构，就像[图
    12-2](ch12.xhtml#ch12fig2)右下方图像中所示的核一样，它强调了对角线结构。
- en: Let’s turn our attention now to the effect of the kernels. What do the kernels
    do to an input MNIST image? We can run a sample MNIST image through the kernels
    by convolving each kernel with the sample, here a “3”, and following a process
    similar to the one that produced the preceding kernel images. The result is a
    set of 32 26 × 26 images, which we again upscale to 64 × 64 before displaying
    them. [Figure 12-5](ch12.xhtml#ch12fig5) shows the result.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将注意力转向核的效果。核对输入的 MNIST 图像有什么作用？我们可以通过对每个核与样本（这里是“3”）进行卷积，运行一个样本 MNIST 图像，并按照类似于生成前面核图像的过程进行。结果是
    32 个 26 × 26 的图像，我们再次将它们放大到 64 × 64 后进行显示。[图 12-5](ch12.xhtml#ch12fig5) 显示了结果。
- en: '![image](Images/12fig05.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/12fig05.jpg)'
- en: '*Figure 12-5: The 32 kernels applied to a sample MNIST input*'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 12-5：应用于样本 MNIST 输入的 32 个核*'
- en: The order of the kernels shown in [Figure 12-4](ch12.xhtml#ch12fig4) matches
    the images in [Figure 12-5](ch12.xhtml#ch12fig5). For example, the top-right image
    of [Figure 12-4](ch12.xhtml#ch12fig4) shows a kernel that is light on the upper
    left and dark on the lower right, meaning it will detect structures along the
    diagonal from lower left to upper right. The output from applying this kernel
    to the sample is the upper-right image of [Figure 12-5](ch12.xhtml#ch12fig5).
    We see that the kernel enhanced parts of the three that are primarily diagonal
    from the lower left to the upper right. Note, this example is easy to interpret
    because the input is a grayscale image with a single channel. This means that
    there is no summing of kernel outputs across channels as we previously saw for
    the more general operation.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 12-4](ch12.xhtml#ch12fig4)中显示的卷积核顺序与[图 12-5](ch12.xhtml#ch12fig5)中的图像相匹配。例如，[图
    12-4](ch12.xhtml#ch12fig4)右上方的图像显示了一个卷积核，左上角是浅色，右下角是深色，这意味着它将检测沿从左下到右上的对角线的结构。将这个卷积核应用到样本中的输出是[图
    12-5](ch12.xhtml#ch12fig5)右上角的图像。我们可以看到，这个卷积核增强了主要沿从左下到右上的对角线的三个部分。请注意，这个例子很容易理解，因为输入是一个单通道的灰度图像。这意味着我们之前在更通用的操作中看到的，跨通道对卷积核输出的求和并不存在。'
- en: 'Typically, the first convolutional layer of a CNN learns kernels that select
    for specific orientations, textures, or, if the input image is RGB, colors. For
    the grayscale MNIST images, orientation is most important. The kernels learned
    at higher convolutional layers in the CNN are also selecting for things, but the
    interpretation of *what* the kernel is selecting becomes more and more abstract
    and difficult to understand. It is worth noting that the kernels learned by a
    CNN’s first convolutional layer are very similar to the first layer of visual
    processing in the mammalian brain. This is the primary visual cortex or V1 layer
    that detects lines and edges. Additionally, always keep in mind that the set of
    convolutional and pooling layers are there to learn a new feature representation:
    a new representation of the input image. This new representation does a better
    job of separating classes so that the fully connected layers can more easily distinguish
    between them.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，CNN的第一个卷积层学习的卷积核选择特定的方向、纹理，或者如果输入图像是RGB格式，则是颜色。对于灰度MNIST图像，方向最为重要。在CNN的更高卷积层中学习的卷积核也会选择某些特征，但卷积核选择*什么*特征的解释变得越来越抽象，也更难以理解。值得注意的是，CNN第一个卷积层学习的卷积核与哺乳动物大脑中视觉处理的第一层非常相似。这一层是初级视觉皮层或V1层，用于检测线条和边缘。此外，请始终记住，卷积层和池化层的组合是为了学习一种新的特征表示：输入图像的新的表示方式。这种新的表示方式更好地分离了不同的类别，以便全连接层可以更容易地区分它们。
- en: Multiple Convolutional Layers
  id: totrans-125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多个卷积层
- en: Most CNNs have more than one convolutional layer. One reason for this is to
    build up features that are influenced by larger portions of the input as one goes
    deeper into the network. This introduces the ideas of *receptive field* and *effective
    receptive field*. The two concepts are similar and often confused. We can explain
    both by looking at [Figure 12-6](ch12.xhtml#ch12fig6).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数CNN有多个卷积层。这样做的一个原因是，随着网络层级的加深，逐渐构建出受输入图像更大部分影响的特征。这引出了*感受野*和*有效感受野*的概念。这两个概念相似且常常被混淆。我们可以通过查看[图
    12-6](ch12.xhtml#ch12fig6)来解释这两个概念。
- en: '![image](Images/12fig06.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/12fig06.jpg)'
- en: '*Figure 12-6: Receptive fields*'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 12-6：感受野*'
- en: The figure shows the *output* of two convolutional layers and the input to the
    model. We’re showing only the relevant parts of the output, using a 3 × 3 kernel.
    We’re also ignoring the depth of the filters since the receptive fields (defined
    next) are the same across the depth of the convolutional layer outputs.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 该图展示了两个卷积层的*输出*以及模型的输入。我们仅显示了输出的相关部分，使用的是3 × 3的卷积核。我们还忽略了滤波器的深度，因为感受野（定义如下）在卷积层输出的深度上是相同的。
- en: '[Figure 12-6](ch12.xhtml#ch12fig6) should be read right to left as the arrows
    indicate. This is the opposite direction to the flow of data through the network.
    Here, we are looking back to earlier layers to see what has influenced the output
    value at a higher layer. The squares are output values. The rightmost shaded square
    is one of the outputs of Conv[2]. This is our starting point for looking back
    to see what influences this value. The arrows point to the outputs of Conv[1]
    that influence the shaded value in Conv[2]. The value in Conv[2] then has a 3
    × 3 *receptive field* as it is directly influenced by the 3 × 3 shaded outputs
    of Conv[1]. This is how we’ll define *receptive field*: the set of outputs from
    the layer immediately before that directly influence the output of the current
    layer.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[图12-6](ch12.xhtml#ch12fig6)应从右至左阅读，如箭头所示。这与数据通过网络的流动方向相反。这里，我们回顾早期的层，查看它们是如何影响高层的输出值的。方框表示输出值。最右边的阴影方框是Conv[2]的输出之一。这是我们回顾并查找影响该值的起点。箭头指向Conv[1]的输出，这些输出影响Conv[2]中的阴影值。Conv[2]中的值具有一个3
    × 3的*感受野*，因为它直接受到Conv[1]中3 × 3阴影输出的影响。这就是我们定义*感受野*的方式：来自上一层的直接影响当前层输出的所有输出集合。'
- en: 'If we look at the set of input values that directly influence the 3 × 3 shaded
    region of Conv[1], we see a 5 × 5 region. This makes sense: each shaded output
    of Conv[1] has a receptive field that is a 3 × 3 region of the input. The receptive
    field is 3 × 3 because the kernels of Conv[1] are 3 × 3 kernels. They overlap
    so that the shaded 5 × 5 input region is what all the shaded Conv[1] outputs are
    influenced by.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们看一下直接影响Conv[1]的3 × 3阴影区域的输入值集合，我们会看到一个5 × 5区域。这是合理的：Conv[1]的每个阴影输出都有一个3
    × 3的感受野。感受野是3 × 3，因为Conv[1]的卷积核是3 × 3的。它们有重叠，因此阴影的5 × 5输入区域影响着所有Conv[1]的阴影输出。
- en: Look again at the rightmost shaded output value. If we trace back to the input
    all the values that can influence it, we see that the shaded 5 × 5 region of the
    input can affect its value. This region of the input is the *effective recep-
    tive field* for the rightmost shaded output of Conv[2]. This output value responds,
    ultimately, to what is happening in the input image in the leftmost shaded region.
    As the CNN gets deeper, with additional convolutional layers, we can see how the
    effective receptive field can change so that deeper convolutional layers are working
    with values ultimately derived from larger and larger portions of the input to
    the model.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 再次看看最右边的阴影输出值。如果我们追溯到输入，找到所有可能影响它的值，我们会看到输入的5 × 5区域能够影响该输出值。这个输入区域就是Conv[2]最右边阴影输出的*有效感受野*。最终，这个输出值响应的是输入图像最左边阴影区域发生的情况。随着CNN层数的加深，添加了更多的卷积层，我们可以看到有效感受野如何变化，从而使得更深的卷积层处理的是来自输入的更大部分的值。
- en: Initializing a Convolutional Layer
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 初始化卷积层
- en: In [Chapter 9](ch09.xhtml#ch09), we saw that the performance of a traditional
    neural network was strongly influenced by the type of random initialization used
    for the learned weights and biases. The same is true for CNNs. Recall that the
    weights of a convolutional layer are the values of the kernels. They are learned
    during backprop, just like the weights of a traditional neural network. We need
    an intelligent way to initialize these values when we set up the network. Fortunately,
    the best initialization approaches for a traditional neural network apply directly
    to convolutional layers as well. For example, Keras defaults to Glorot initialization,
    which, as we saw in [Chapter 9](ch09.xhtml#ch09), is sometimes called Xavier initialization
    in other toolkits.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第9章](ch09.xhtml#ch09)中，我们看到传统神经网络的性能受学习权重和偏差的随机初始化方式的强烈影响。卷积神经网络（CNN）也同样如此。回想一下，卷积层的权重是卷积核的值，它们在反向传播过程中被学习，就像传统神经网络的权重一样。在我们设置网络时，需要一种智能的方式来初始化这些值。幸运的是，传统神经网络的最佳初始化方法同样适用于卷积层。例如，Keras默认使用Glorot初始化，正如我们在[第9章](ch09.xhtml#ch09)中看到的，这在其他工具包中有时被称为Xavier初始化。
- en: Let’s move on now from convolutional layers to pooling layers. These are simpler
    but perform an important, if somewhat controversial, function.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们从卷积层转向池化层。这些层虽然较为简单，但它们执行着重要的（尽管有些争议的）功能。
- en: Pooling Layers
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 池化层
- en: Our favorite figure, [Figure 12-3](ch12.xhtml#ch12fig3), shows a pooling layer
    after the first two convolutional layers. This pooling layer takes an input stack
    of 24 × 24 × 64 and produces an output stack of 12 × 12 × 64\. The pooling part
    is marked as “2 × 2”. What’s going on here?
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最喜欢的图示，[图12-3](ch12.xhtml#ch12fig3)，展示了在前两个卷积层之后的池化层。这个池化层输入的是一个24 × 24 ×
    64的栈，输出的是一个12 × 12 × 64的栈。池化部分标记为“2 × 2”。这里到底发生了什么？
- en: The key is the “2 × 2”. This means, for each of the 64 24 × 24 inputs, we move
    a 2 × 2 sliding window over the input and perform an operation similar to convolution.
    Not explicitly called out in [Figure 12-3](ch12.xhtml#ch12fig3) is that the stride
    is also 2 so that the sliding 2 × 2 window jumps by two to avoid overlapping itself.
    This is typically the case, but doesn’t need to be. Since the pooling operation
    is per input in the stack, the output leaves the stack size unchanged. This is
    contrary to what a convolutional layer often does.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 关键在于“2 × 2”。这意味着，对于每个64个24 × 24的输入，我们将一个2 × 2的滑动窗口在输入上滑动，并执行类似于卷积的操作。[图12-3](ch12.xhtml#ch12fig3)中没有明确指出的是，步长也是2，以使得滑动窗口每次跳跃两个位置，从而避免重叠。这通常是这样，但并非必须如此。由于池化操作是对栈中的每个输入执行的，输出保持栈的大小不变。这与卷积层的常见做法相反。
- en: Let’s look at the pooling operation applied to a single input in the stack,
    a 24 × 24 matrix. [Figure 12-7](ch12.xhtml#ch12fig7) shows us what’s going on.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下应用于栈中单个输入的池化操作，这个输入是一个24 × 24的矩阵。[图12-7](ch12.xhtml#ch12fig7)展示了这里发生了什么。
- en: '![image](Images/12fig07.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/12fig07.jpg)'
- en: '*Figure 12-7: Applying 2 × 2 max pooling to an 8 × 8 input*'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '*图12-7：对一个8 × 8输入应用2 × 2最大池化*'
- en: The first 2 × 2 values are mapped to the first output value. Then we move over
    two and map the next 2 × 2 region to the output and so on until the entire input
    is mapped. The operation performed on each 2 × 2 region is up to the architect
    of the CNN. The most common operation is “select the largest value,” or *max pooling*,
    which is what we show in [Figure 12-7](ch12.xhtml#ch12fig7). This is also the
    operation the model in [Figure 12-3](ch12.xhtml#ch12fig3) is performing. Another
    fairly common pooling operation is to average the values.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 前两个2 × 2的值被映射到第一个输出值。然后我们向右移动两个位置，将下一个2 × 2区域映射到输出，依此类推，直到整个输入被映射。对每个2 × 2区域执行的操作由CNN的架构师决定。最常见的操作是“选择最大值”，也就是*最大池化*，这在[图12-7](ch12.xhtml#ch12fig7)中展示的就是这种操作。这也是[图12-3](ch12.xhtml#ch12fig3)中模型正在执行的操作。另一种相对常见的池化操作是对值进行平均。
- en: We can see from [Figure 12-7](ch12.xhtml#ch12fig7) that the 8 × 8 input matrix
    is mapped to a 4 × 4 output matrix. This explains why the output of the pooling
    layer in [Figure 12-3](ch12.xhtml#ch12fig3) is 12 × 12; each dimension is half
    the size of the input.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 从[图12-7](ch12.xhtml#ch12fig7)中我们可以看到，8 × 8的输入矩阵被映射到一个4 × 4的输出矩阵。这解释了为什么[图12-3](ch12.xhtml#ch12fig3)中的池化层输出是12
    × 12；每个维度的大小是输入的一半。
- en: The pooling operation is straightforward but throws information away. So why
    do it at all? The primary motivation for pooling is to reduce the number of values
    in the network. Typically, as depth increases, the number of filters used by convolutional
    layers increases, by design. We see this for even the simple network of [Figure
    12-3](ch12.xhtml#ch12fig3), where the first convolutional layer has 32 filters,
    while the second has 64\. Therefore, the second convolutional layer outputs 24
    × 24 × 64 = 36,864 values, but after 2 × 2 pooling, there are only 12 × 12 × 64
    = 9,216 values to work with, a 75 percent reduction. It’s important to note that
    we’re talking about the number of values present as we move data through the network,
    not the number of learned parameters in the layers. The second convolutional layer
    in [Figure 12-3](ch12.xhtml#ch12fig3) has 3 × 3 × 32 × 64 = 18,432 learned parameters
    (ignoring bias values), while the pooling layer has no learned parameters.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 池化操作很直接，但它会丢失信息。那么为什么还要做池化呢？池化的主要动机是减少网络中值的数量。通常，随着深度的增加，卷积层使用的滤波器数量也会增加，这是设计上的一种做法。即使是在[图12-3](ch12.xhtml#ch12fig3)这个简单的网络中，我们也可以看到，第一层卷积层有32个滤波器，而第二层有64个滤波器。因此，第二层卷积层输出的是24
    × 24 × 64 = 36,864个值，但经过2 × 2池化后，只有12 × 12 × 64 = 9,216个值，减少了75%。需要注意的是，我们这里讨论的是数据在网络中流动时的值的数量，而不是层中学习到的参数数量。[图12-3](ch12.xhtml#ch12fig3)中的第二个卷积层有3
    × 3 × 32 × 64 = 18,432个学习参数（忽略偏置值），而池化层没有学习参数。
- en: This reduction in the number of values in the output, which is our representation
    of the input, speeds up computation and acts as a regularizer to guard against
    overfitting. The regularization techniques and rationales of [Chapter 9](ch09.xhtml#ch09)
    are equally valid for CNNs. However, since pooling throws information away and
    selects proxies to represent entire regions of the representation (the convolutional
    layer outputs), it alters the spatial relationship between parts of the input.
    This loss of spatial relationships might be critical for some applications and
    has motivated people like Geoffrey Hinton to eliminate pooling by introducing
    other types of networks (search for “capsule networks”).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 输出值数量的减少，也就是我们对输入的表示，加速了计算，并充当了正则化器，以防止过拟合。[第9章](ch09.xhtml#ch09)中的正则化技术和原理对CNN同样适用。然而，由于池化会丢失信息，并选择代理来表示表示的整个区域（卷积层输出），它改变了输入部分之间的空间关系。这种空间关系的丧失对于某些应用可能至关重要，正是这个原因促使像Geoffrey
    Hinton这样的人通过引入其他类型的网络（搜索“胶囊网络”）来消除池化操作。
- en: 'Specifically, Hinton said the following regarding pooling layers in response
    to a question on Reddit asking for his most controversial opinion on machine learning:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，Hinton在Reddit上回答关于池化层的问题时，谈到了他对机器学习的最具争议的看法：
- en: The pooling operation used in convolutional neural networks is a big mistake
    and the fact that it works so well is a disaster. If the pools do not overlap,
    pooling loses valuable information about where things are. We need this information
    to detect precise relationships between the parts of an object.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络中使用的池化操作是一个重大错误，池化操作之所以效果良好，实际上是一场灾难。如果池化不重叠，池化将丢失有关物体位置的宝贵信息。我们需要这些信息来检测物体各部分之间的精确关系。
- en: He elaborates further in the answer, pointing out that allowing pooling operations
    to overlap does preserve some of the spatial relationships in a crude way. An
    overlapping pooling operation might be to use a 2 × 2 window as we used in [Figure
    12-7](ch12.xhtml#ch12fig7), but use a stride of 1 instead of 2.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 他在回答中进一步阐述，指出允许池化操作重叠的确以一种粗略的方式保留了一些空间关系。一个重叠的池化操作可能是使用2 × 2的窗口，就像我们在[图12-7](ch12.xhtml#ch12fig7)中使用的那样，但步长使用1而不是2。
- en: Concerns aside, pooling layers are an essential part of CNNs as presently implemented,
    but be careful when adding them to a model. Let’s move on now to the top layers
    of a CNN, the fully connected layers.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有这些顾虑，池化层仍然是目前CNN不可或缺的一部分，但在将它们添加到模型中时要小心。现在我们继续讨论CNN的顶层，即完全连接层。
- en: Fully Connected Layers
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 完全连接层
- en: In the second row of [Figure 12-3](ch12.xhtml#ch12fig3), all the layers starting
    with *Flatten* form the fully connected layer of the model. The figure uses Keras
    terminology; many people call the *Dense* layer the fully connected layer and
    assume there is a Flatten operation as part of it along with the activation (ReLU)
    and optional dropout before the softmax layer. Therefore, the model in [Figure
    12-3](ch12.xhtml#ch12fig3) has only one fully connected layer.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图12-3](ch12.xhtml#ch12fig3)的第二行中，所有以*Flatten*开头的层构成了模型的完全连接层。图中使用的是Keras术语；许多人称*Dense*层为完全连接层，并假设它包含一个Flatten操作，以及激活（ReLU）和在softmax层之前的可选Dropout操作。因此，[图12-3](ch12.xhtml#ch12fig3)中的模型只有一个完全连接层。
- en: 'We previously stated that the net effect of the convolutional and pooling layers
    is to change the representation of the input feature (the image, say) into one
    that makes it easier for a model to reason about. During training, we are asking
    the network to learn a different, often more compact, representation of the input
    to help the model perform better on unseen inputs. For the model in [Figure 12-3](ch12.xhtml#ch12fig3),
    all the layers up to and including the pooling layer (and the dropout layer after
    it for training) are there to learn a new representation of the input image. In
    this case, the fully connected layer is the model: it will take that new representation
    and ultimately make a classification based on it.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前已经说过，卷积层和池化层的总体效果是将输入特征（比如图像）的表示转化为一种更容易供模型推理的形式。在训练过程中，我们要求网络学习输入的另一种、通常是更紧凑的表示，以帮助模型在处理未见过的输入时表现得更好。对于[图12-3](ch12.xhtml#ch12fig3)中的模型，直到包括池化层（以及其后的Dropout层用于训练）的所有层，都是为了学习输入图像的新表示。在这种情况下，完全连接层就是模型：它将采用这种新表示，并最终基于它进行分类。
- en: 'Fully connected layers are just that, fully connected. The weights between
    the flattened final pooling layer of 9,216 elements for [Figure 12-3](ch12.xhtml#ch12fig3)
    (12 × 12 × 64 = 9,216) and the Dense layer of 128 elements are the same as if
    we were building a traditional neural network. This means that there are 9,216
    × 128 = 1,179,648 weights plus an additional 128 bias values that need to be learned
    during training. Therefore, of the 1,199,882 parameters (weights and biases) in
    the model of [Figure 12-3](ch12.xhtml#ch12fig3), 98.3 percent of them are in the
    transition between the final pooling layer and the fully connected layer. This
    illustrates an important point: fully connected layers are *expensive* in terms
    of parameters that need to be learned, just as they are for traditional neural
    networks. Ideally, if the feature learning layers, the convolutional and pooling
    layers, are doing their job well, we might expect to need only one or two fully
    connected layers.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 全连接层就是全连接的。对于图 [12-3](ch12.xhtml#ch12fig3) 的最终池化层（9,216 个元素，即 12 × 12 × 64 =
    9,216）与具有 128 个元素的 Dense 层之间的权重，与构建传统神经网络时一样。这意味着在训练期间需要学习 9,216 × 128 = 1,179,648
    个权重加上额外的 128 个偏置值。因此，在图 [12-3](ch12.xhtml#ch12fig3) 模型的 1,199,882 个参数（权重和偏置）中，98.3%
    在最终池化层和全连接层之间的过渡中。这说明了一个重要的观点：全连接层在需要学习的参数方面是*昂贵*的，就像它们对于传统神经网络一样。理想情况下，如果特征学习层，即卷积和池化层，能够很好地完成它们的工作，我们可能只需要一个或两个全连接层。
- en: 'Fully connected layers have another disadvantage, besides memory use, that
    can impact their utility. To see what this disadvantage is, consider the following
    scenario: you want to be able to locate digits in grayscale images. Assume for
    simplicity that the background is black. If you use the model of [Figure 12-3](ch12.xhtml#ch12fig3)
    trained on MNIST digits, you will have a model that is very good at identifying
    digits centered in 28×28 pixel images, but what if the input images are large
    and you do not know where the digits are in the image, let alone how many digits
    there are? Then things get a little more interesting. The model of [Figure 12-3](ch12.xhtml#ch12fig3)
    expects input images that are 28×28 pixels in size and only that size. In [Chapter
    13](ch13.xhtml#ch13), we will work through this problem in detail as an experiment,
    but for now, let’s discuss fully convolutional layers, a possible solution to
    this disadvantage of using fully connected layers in CNNs.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 全连接层除了内存使用之外，还有一个可能影响其效用的缺点。为了看清楚这个缺点是什么，请考虑以下情景：你希望能够在灰度图像中定位数字。简单起见，假设背景是黑色的。如果你使用在
    MNIST 数字上训练的图 [12-3](ch12.xhtml#ch12fig3) 模型，你将得到一个非常擅长识别在 28×28 像素图像中居中的数字的模型，但是如果输入图像很大，并且你不知道图像中数字的位置，更不用说有多少个数字了，那么情况就变得更有趣了。图
    [12-3](ch12.xhtml#ch12fig3) 的模型期望输入图像的尺寸为 28×28 像素，仅限于这个尺寸。在 [第 13 章](ch13.xhtml#ch13)
    中，我们将详细讨论这个问题作为一个实验，但现在，让我们讨论全卷积层，这是在 CNN 中使用全连接层时的一个可能解决方案的不利之处。
- en: Fully Convolutional Layers
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 完全卷积层
- en: In the last section, I said that the model of [Figure 12-3](ch12.xhtml#ch12fig3)
    expects input images that are 28×28 pixels in size and only that size. Let’s see
    why.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我说图 [12-3](ch12.xhtml#ch12fig3) 的模型期望输入图像的尺寸为 28×28 像素，仅限于这个尺寸。让我们看看为什么。
- en: There are many kinds of layers in this model. Some, like the ReLU and dropout
    layers, have no impact on the dimensionality of the data flowing through the network.
    The same cannot be said of the convolutional, pooling, and fully connected layers.
    Let’s look at these layers one by one to see how they are tied to the dimensionality
    of the input image.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型中有许多种类的层。像是 ReLU 和 dropout 层这样的层对通过网络流动的数据的维度没有影响。但是卷积、池化和全连接层则不同。让我们逐个看看这些层，看它们如何与输入图像的维度相关联。
- en: The convolutional layers implement convolutions. By definition, a convolution
    involves moving a fixed-size kernel over some input image (thinking purely 2D
    here). Nothing in that operation specifies the size of the input image. The output
    of the first convolutional layer in [Figure 12-3](ch12.xhtml#ch12fig3) is 26 ×
    26 × 32\. The 32 comes from the number of filters selected by the architecture.
    The 26 × 26 comes from using a 3 × 3 convolution kernel on a 28 × 28 input with
    no padding. If the input image were instead 64×64 pixels, the output of this layer
    would be 62 × 62 × 32, and we wouldn’t need to do anything to alter the architecture
    of the network. The convolutional layers of a CNN are agnostic to the spatial
    dimensions of their inputs.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层实现卷积。根据定义，卷积涉及将一个固定大小的卷积核滑动到某个输入图像上（这里纯粹考虑二维情况）。在这个操作中没有任何东西指定输入图像的大小。[图12-3](ch12.xhtml#ch12fig3)中的第一层卷积输出是26
    × 26 × 32。32来自架构选择的滤波器数量。26 × 26来自在28 × 28输入上使用3 × 3卷积核且没有填充的结果。如果输入图像是64×64像素，那么这一层的输出将是62
    × 62 × 32，而且我们无需对网络架构做任何改变。CNN的卷积层对输入的空间维度是无关的。
- en: 'The pooling layer in [Figure 12-3](ch12.xhtml#ch12fig3) takes a 24 × 24 × 64
    input and produces a 12 × 12 × 64 output. As we previously saw, the pooling operation
    is much like the convolution operation: it slides a fixed size window over the
    input, spatially, and produces an output; in this case, the output is half the
    dimensionality of the input while leaving the depth the same. Again, nothing in
    this operation fixes the spatial dimensions of the input stack. If the input stack
    were 32 × 32 × 64, the output of this max pooling operation would be 16 × 16 ×
    64 without a change needed to the architecture.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '[图12-3](ch12.xhtml#ch12fig3)中的池化层接受一个24 × 24 × 64的输入，并生成一个12 × 12 × 64的输出。正如我们之前看到的，池化操作与卷积操作非常相似：它在空间上滑动一个固定大小的窗口，并生成一个输出；在这个例子中，输出的维度是输入的一半，同时深度保持不变。同样，这个操作中没有任何东西固定输入栈的空间维度。如果输入栈是32
    × 32 × 64，那么这个最大池化操作的输出将是16 × 16 × 64，而不需要改变架构。'
- en: Finally, we have the fully connected layer that maps the 12 × 12 × 64 = 9,216
    pooling output to a 128 element fully connected (Dense) layer. As we saw in [Chapter
    8](ch08.xhtml#ch08), fully connected neural networks use matrices of weights between
    layers in their implementation. There are 9,216 elements in the output of the
    pooling layer and a fixed 128 in the dense layer, so we need a matrix that is
    9,216 × 128 elements. This size *is* fixed. If we use the network with a larger,
    say 32 × 32, input image, by the time we get through the pooling layer, the output
    size will be 14 × 14 × 64 = 12,544, which would require an existing 12,544 × 128
    weight matrix to map to the fully connected layer. Of course, this won’t work;
    we trained a network that uses a 9,216 × 128 matrix. The fully connected layers
    of a CNN fix the input size of the CNN. If we could get around this, we could
    apply inputs of any size to the CNN, assuming memory allows.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有了全连接层，它将12 × 12 × 64 = 9,216的池化输出映射到一个128元素的全连接（Dense）层。如我们在[第8章](ch08.xhtml#ch08)中看到的，全连接神经网络在其实现中使用了层之间的权重矩阵。池化层的输出中有9,216个元素，而全连接层中有固定的128个元素，因此我们需要一个9,216
    × 128的矩阵。这个大小*是*固定的。如果我们使用一个更大的输入图像，比如32 × 32，经过池化层后，输出大小将是14 × 14 × 64 = 12,544，这将需要一个现有的12,544
    × 128的权重矩阵来映射到全连接层。当然，这行不通；我们训练的网络使用的是9,216 × 128的矩阵。CNN的全连接层固定了CNN的输入大小。如果我们能够绕过这一点，那么我们就可以对CNN应用任何大小的输入，前提是内存允许。
- en: We could, naïvely, simply slide a 28 × 28 window over the larger input image,
    run each 28×28 pixel image through the model as we trained it, and output a larger
    map, where each pixel now has a probability of that digit being present. There
    are 10 digits, so we would have 10 output maps. This sliding window approach certainly
    works, but it’s very computationally expensive, as many simplistic implementations
    of algorithms often are.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以天真地做法是，简单地将一个28 × 28的窗口滑动到更大的输入图像上，将每个28×28像素的图像通过我们训练的模型进行处理，然后输出一个更大的映射，其中每个像素现在都有该数字存在的概率。数字有10个，因此我们将有10个输出映射。这个滑动窗口方法当然是有效的，但它非常计算密集，因为许多简单的算法实现通常都是如此。
- en: Fortunately for us, we can do better by converting the fully connected layer
    into an equivalent convolutional layer to make the model a *fully convolutional
    network*. In a fully convolutional network, there are no fully connected layers,
    and we’re not restricted to using a fixed input size. The relationship between
    input size and the output of the network when it is fully convolutional is something
    we will see in [Chapter 13](ch13.xhtml#ch13), but the essential operation is to
    look at the size of the last standard convolutional or pooling layer and replace
    the fully connected layer that follows with a convolutional layer using a kernel
    of the same size.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，通过将全连接层转换为等效的卷积层，我们可以将模型转换为*完全卷积网络*，从而获得更好的效果。在完全卷积网络中，没有全连接层，我们不再受限于使用固定的输入大小。当网络完全卷积时，输入大小与输出之间的关系将在[第13章](ch13.xhtml#ch13)中讨论，但基本操作是查看最后一个标准卷积或池化层的大小，并用相同大小的卷积核替换后续的全连接层。
- en: In [Figure 12-3](ch12.xhtml#ch12fig3), the output of the pooling layer is 12
    × 12 × 64\. Therefore, instead of the 128-element fully connected layer that we
    saw fixes our input size, we can mathematically get the same calculation by changing
    the fully connected layer into a 12 × 12 × 128 convolutional layer. Convolving
    a 12 × 12 kernel over a 12 × 12 input produces a single number. Therefore, the
    output of the 12 × 12 × 128 convolutional layer will be a 1 × 1 × 128 array, which
    is functionally the same as the 128 outputs of the fully connected layer that
    we originally used. Additionally, the convolution operation between a 12 × 12
    kernel and a 12 × 12 input is to simply multiply the kernel values by the input
    values, element by element, and sum them. This is what a fully connected layer
    does for each of its nodes.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图12-3](ch12.xhtml#ch12fig3)中，池化层的输出是12 × 12 × 64。因此，与我们之前看到的将输入大小固定的128元素全连接层不同，我们可以通过将全连接层转换为12
    × 12 × 128的卷积层来进行数学上的等效计算。将12 × 12的卷积核与12 × 12的输入进行卷积，最终会得到一个单一的数字。因此，12 × 12
    × 128的卷积层输出将是一个1 × 1 × 128的数组，在功能上与我们最初使用的128个全连接层输出相同。此外，12 × 12的卷积核与12 × 12的输入进行卷积的操作，实际上就是将卷积核的值与输入的值逐元素相乘并求和。这正是全连接层在每个节点上所做的工作。
- en: We do not save anything in terms of the number of parameters when using a convolutional
    layer this way. We can see this from [Figure 12-3](ch12.xhtml#ch12fig3). The 9,216
    elements of the pooling layer output times the 128 nodes of the fully connected
    layer means we have 9,216 × 128 = 1,179,648 weights + 128 bias terms needed for
    both the fully connected and fully convolutional layers. When moving to the 12
    × 12 × 128 convolutional layer, we have 12 × 12 × 64 × 128 = 1,179,648 weights
    to learn, the same as before. However, now we also have the freedom to change
    the input size, as the 12 × 12 × 128 convolutional layer will automatically convolve
    over any larger input, giving us outputs that represent the application of the
    network to 28 × 28 regions of the input with a stride determined by the specific
    architecture of the network.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 当以这种方式使用卷积层时，我们并没有在参数数量上节省任何东西。从[图12-3](ch12.xhtml#ch12fig3)中我们可以看到这一点。池化层输出的9,216个元素与全连接层的128个节点相乘，意味着我们需要9,216
    × 128 = 1,179,648个权重和128个偏置项，这些是全连接层和完全卷积层都需要的参数。转向12 × 12 × 128的卷积层时，我们需要学习12
    × 12 × 64 × 128 = 1,179,648个权重，和之前相同。然而，现在我们也可以自由地改变输入大小，因为12 × 12 × 128的卷积层将自动对更大的输入进行卷积，从而输出代表网络应用于输入28
    × 28区域的结果，步幅由网络的具体架构决定。
- en: 'Fully convolutional networks stem from the 2014 paper by Long, Shelhamer, and
    Darrell, “Fully Convolutional Networks for Semantic Segmentation,” which has been
    referenced over 19,000 times as of this writing. The phrase *semantic segmentation*
    refers to assigning a class label to each pixel of the input image. Currently,
    the go-to architecture for semantic segmentation is the U-Net (see “U-Net: Convolutional
    Networks for Biomedical Image Segmentation” by Ronneberger, Fischer, and Brox,
    2015) which has seen widespread success, especially in medical domains.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 完全卷积网络来源于2014年Long、Shelhamer和Darrell的论文《用于语义分割的完全卷积网络》（“Fully Convolutional
    Networks for Semantic Segmentation”），截至目前，这篇论文已被引用超过19,000次。术语*语义分割*指的是将一个类别标签分配给输入图像的每一个像素。目前，用于语义分割的常用架构是U-Net（参见Ronneberger、Fischer和Brox的2015年论文《U-Net：用于生物医学图像分割的卷积网络》），该架构在医学领域取得了广泛的成功。
- en: We’ve discussed the primary CNN layers, those found in [Figure 12-3](ch12.xhtml#ch12fig3).
    There are many more that we could cover, but they are generally beyond what we
    want to present at this level, with one exception, batch normalization, which
    we’ll experiment with in [Chapter 15](ch15.xhtml#ch15). New layer types are being
    added all the time in response to active research projects. However, in the end,
    the core includes the layers we have discussed in this chapter. Let’s move on
    now and see how a trained CNN processes unknown inputs.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了主要的CNN层，即[图12-3](ch12.xhtml#ch12fig3)中所示的层。虽然还有很多其他层我们可以介绍，但它们通常超出了我们在这个层次上所要呈现的内容，唯一的例外是批量归一化（batch
    normalization），我们将在[第15章](ch15.xhtml#ch15)中进行实验。新的层类型不断被加入，通常是响应于活跃的研究项目。然而，最终的核心层包含了我们在本章中讨论的层。现在让我们继续，看看一个训练好的CNN如何处理未知输入。
- en: Step by Step
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 步骤解析
- en: In the previous sections, we discussed the architecture and layers of our sample
    CNN, [Figure 12-3](ch12.xhtml#ch12fig3). In this section, we will illustrate the
    operation of the network to see how it responds to two new inputs, one a “4” and
    the other a “6”. We assume the network is fully trained; we’ll train for real
    it in [Chapter 13](ch13.xhtml#ch13).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们讨论了我们的示例CNN的架构和层次，见[图12-3](ch12.xhtml#ch12fig3)。在这一节中，我们将演示网络的操作，看看它如何响应两个新输入，一个是“4”，另一个是“6”。我们假设网络已经完全训练好了；我们将在[第13章](ch13.xhtml#ch13)中真正进行训练。
- en: The input image is passed through the model layer by layer
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 输入图像通过模型逐层传递
- en: input → conv[0] → conv[1] → pool → dense → softmax
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 输入 → 卷积[0] → 卷积[1] → 池化 → 全连接 → Softmax
- en: using the trained weights and biases to calculate outputs for each layer. We
    will refer to these as the *activations*. The output of the first convolutional
    layer is a stack of 32 26 × 26 images, the response of the input image to each
    of the 32 kernels. This stack then passes to the second convolutional layer to
    produce 64 24 × 24 outputs. Note, between the two convolutional layers is a ReLU
    operation that clips the output so that anything that would have been negative
    is now 0\. Doing this adds a nonlinearity to the data as it flows through the
    network. Without this nonlinearity, the net effect of the two convolutional layers
    is to act like a single convolutional layer. With the nonlinearity imposed by
    the ReLU, we enable the two convolutional layers to learn different things about
    the data.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 使用训练好的权重和偏置来计算每一层的输出。我们将这些称为*激活值*。第一卷积层的输出是32个26 × 26的图像堆叠，这是输入图像对每个32个卷积核的响应。这个堆叠随后传递到第二卷积层，生成64个24
    × 24的输出。注意，两层卷积层之间有一个ReLU操作，它会将输出中的负值裁剪为0。这样做为数据流经网络时增加了非线性。如果没有这个非线性，两层卷积层的净效果就相当于一个单独的卷积层。通过ReLU强加的非线性，我们使得这两层卷积层能够学习到数据的不同特征。
- en: The second ReLU operation makes the stack of 64 24 × 24 outputs 0 or positive.
    Next, a 2 × 2 max pooling operation reduces the 64 outputs to 12 × 12 in size.
    After this, a standard fully connected layer produces 128 output values as a vector
    from the 9,216 values in the stack of 12 × 12 activations. From this, a set of
    10 outputs, one for each digit, is calculated via a softmax. These are the output
    values of the network representing the network’s confidence as to which class
    label should be assigned to the input image.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个ReLU操作将64个24 × 24的输出转换为0或正值。接下来，2 × 2的最大池化操作将64个输出的大小减少到12 × 12。之后，一个标准的全连接层从12
    × 12激活值的9,216个值中生成128个输出值作为一个向量。随后，通过Softmax计算出一组10个输出值，每个数字对应一个输出。这些输出值代表了网络对输入图像应分配哪个类别标签的信心。
- en: 'We can illustrate the activations by displaying the output images: either 26
    × 26 for the first convolutional layer, 24 × 24 for the second convolutional layer,
    or 12 × 12 for the pooling layer. To show the activations from the fully connected
    layer, we can make an image of 128 bars, where the intensity of each bar represents
    the vector value. [Figure 12-8](ch12.xhtml#ch12fig8) shows the activations for
    our two sample digits.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过显示输出图像来说明激活值：第一卷积层为26 × 26，第二卷积层为24 × 24，池化层为12 × 12。为了展示全连接层的激活值，我们可以制作一个由128根条形图组成的图像，每根条形图的强度代表向量值。[图12-8](ch12.xhtml#ch12fig8)展示了我们两个样本数字的激活值。
- en: '![image](Images/12fig08.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/12fig08.jpg)'
- en: '*Figure 12-8: Model activations per layer. The output is inverted: darker implies
    stronger activation.*'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '*图12-8：每层模型激活值。输出是反转的：较暗表示激活值更强。*'
- en: Note that the images are inverted so that darker corresponds to stronger activation
    values. We are not showing the softmax outputs. These values are
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，图像已经反转，因此较暗的部分对应更强的激活值。我们没有显示softmax输出。这些值是
- en: '|  | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '|  | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 4 | 0.00 | 0.00 | 0.00 | 0.00 | 0.99 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 0.00 | 0.00 | 0.00 | 0.00 | 0.99 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |'
- en: '| 6 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.99 | 0.00 | 0.00 | 0.00 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.99 | 0.00 | 0.00 | 0.00 |'
- en: indicating that in both cases, the model is very confident of the class label
    that should be assigned and that it was, in fact, correct.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 表示在这两种情况下，模型对应该分配的类别标签非常有信心，且事实上它是正确的。
- en: Looking back at [Figure 12-8](ch12.xhtml#ch12fig8), we see that the output of
    the first convolutional layer is simply the response of the single input image
    (grayscale) and the kernels of the layer. This hearkens back to [Figure 12-2](ch12.xhtml#ch12fig2),
    where we saw that convolution could be used to highlight aspects of the input
    image. After the ReLU operation, the responses of the 64 filters of the second
    convolutional layer, each a stack of 32 kernels, seems to be picking out different
    portions or strokes in the input images. These can be thought of as a set of smaller
    components from which the input is constructed. The second ReLU and pooling operation
    preserve much of the structure of the second convolutional layer outputs, but
    reduce the size to one quarter what it was previously. Finally, the output of
    the fully connected layer shows the pattern derived from the input image, the
    new representation that we expect to be easier to classify than the raw image
    input.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾[图 12-8](ch12.xhtml#ch12fig8)，我们看到第一个卷积层的输出仅仅是单个输入图像（灰度图）和该层卷积核的响应。这让我们想起了[图
    12-2](ch12.xhtml#ch12fig2)，在那里我们看到卷积可以用来突出输入图像的某些特征。经过ReLU操作后，第二个卷积层的64个滤波器的响应，每个滤波器是32个卷积核的堆叠，似乎在从输入图像中选取不同的部分或笔划。这些可以看作是构成输入图像的小组件集。第二次ReLU和池化操作保留了第二卷积层输出的大部分结构，但将大小缩小至原来的四分之一。最后，全连接层的输出展示了从输入图像中派生的模式，这是我们期望比原始图像输入更容易分类的新表示。
- en: 'The dense layer outputs of [Figure 12-8](ch12.xhtml#ch12fig8) are different
    from each other. This begs the question: what do these outputs look like for several
    instances of four and six digits? Is there something in common that we can see,
    even in these values? We might expect that there is because we know this network
    has been trained and has achieved a very high accuracy of over 99 percent on the
    test set. Let’s take a look at running ten “4” and ten “6” images from the test
    set through the network and compare the dense layer activations. This gives us
    [Figure 12-9](ch12.xhtml#ch12fig9).'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 12-8](ch12.xhtml#ch12fig8)中的全连接层输出是彼此不同的。这引出了一个问题：对于四个和六个数字的多个实例，这些输出是什么样的？即使在这些值中，我们是否可以看到某些共同点？我们可能会期待看到共同点，因为我们知道这个网络已经过训练，并且在测试集上取得了超过99%的高准确率。让我们来看看将来自测试集的十个“4”和十个“6”图像输入网络，并比较全连接层的激活值。这给出了[图
    12-9](ch12.xhtml#ch12fig9)。'
- en: '![image](Images/12fig09.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/12fig09.jpg)'
- en: '*Figure 12-9: Dense layer activations for ten instances of 4 and 6\. The output
    is inverted: darker implies stronger activation.*'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 12-9：4和6的十个实例的全连接层激活值。输出被反转：较暗意味着更强的激活。*'
- en: On the left, we see the actual input to the model. On the right is the representation
    of the 128 outputs in the fully connected layer, the one that feeds into the softmax.
    Each digit has a particular pattern that is common to each one of the digits.
    However, there are also variations. The middle “4” has a very short stem, and
    we see that its representation in the fully connected layer is also different
    from all the other examples. Still, this digit was successfully called a “4” by
    the model with a certainty of 0.999936.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧是模型的实际输入。右侧是全连接层中128个输出的表示，这些输出将输入softmax层。每个数字都有一个特定的模式，这个模式在每个数字中都是共同的。然而，也有一些变化。中间的“4”有一个非常短的笔画，我们看到它在全连接层中的表示与其他所有示例不同。尽管如此，模型仍然成功地将其识别为“4”，且置信度为0.999936。
- en: '[Figure 12-9](ch12.xhtml#ch12fig9) provides evidence that the model learned
    what we wanted it to learn in terms of representation of the input. The softmax
    layer maps the 128 elements of the dense layer to 10, the output nodes from which
    the softmax probabilities are calculated. This is, in effect, a simple traditional
    neural network with no hidden layers. This simpler model succeeds in correctly
    labeling the images because the new representation of the inputs does a much better
    job of separating the classes so that even a simple model can make solid predictions.
    It also succeeds because the training process jointly optimizes both the weights
    of this top layer model and the weights of the lower layers that generate the
    input to the model at the same time, so they reinforce each other. Sometimes you
    will see this referred to in the literature at *end-to-end* training.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '[图12-9](ch12.xhtml#ch12fig9)提供了证据，证明模型在输入表示方面学到了我们希望它学到的东西。softmax层将稠密层的128个元素映射到10个输出节点，从这些输出节点计算softmax概率。实际上，这是一个没有隐藏层的简单传统神经网络。这个更简单的模型成功地正确标记了图像，因为输入的新表示在分离类别方面做得更好，甚至一个简单的模型也能做出可靠的预测。它之所以成功，还因为训练过程同时优化了这个顶层模型的权重和生成输入的低层模型的权重，从而相互强化。有时你会在文献中看到这个被称为*端到端*训练。'
- en: We can demonstrate the claim that the features are better separated by looking
    at a plot of the dense layer activations for the MNIST test data. Of course, we
    can’t look at the actual plot, as I have no idea how to visualize a plot in 128
    dimensions, but all is not lost. The machine learning community has created a
    powerful visualization tool called *t-SNE*, which, fortunately for us, is part
    of sklearn. This algorithm intelligently maps high-dimensional spaces to lower-dimensional
    spaces, including 2D. If we run a thousand randomly selected MNIST test images
    through the model and then run the resulting 128-dimension dense layer activations
    through t-SNE, we can produce a 2D plot where the separation between classes reflects
    the actual separation in the 128-dimensional space. [Figure 12-10](ch12.xhtml#ch12fig10)
    is the result.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过查看MNIST测试数据的稠密层激活的图来证明特征更好地分离这一说法。当然，我们不能查看实际的图形，因为我不知道如何可视化128维的图，但并非一切都失去了。机器学习社区创造了一个强大的可视化工具叫做*t-SNE*，幸运的是，它是sklearn的一部分。这个算法智能地将高维空间映射到低维空间，包括2D。如果我们将一千张随机选择的MNIST测试图像输入模型，然后将结果的128维稠密层激活值通过t-SNE处理，我们可以生成一个2D图，其中类别之间的分离反映了在128维空间中的实际分离。[图12-10](ch12.xhtml#ch12fig10)是结果。
- en: '![image](Images/12fig10.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/12fig10.jpg)'
- en: '*Figure 12-10: How well the model separates test samples by class (t-SNE plot)*'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '*图12-10：模型如何按类别分离测试样本（t-SNE图）*'
- en: In this plot, each class uses a different plot symbol. If the model did not
    correctly classify the sample, it is shown as a larger star. In this case, only
    a handful of samples were misclassified. The separation by class type is very
    evident; the model has learned a representation that makes it straightforward
    to decide on the correct class label in most cases. We can readily count 10 different
    blobs in the t-SNE plot.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个图中，每个类别使用不同的图形符号。如果模型没有正确分类样本，它会显示为更大的星号。在这种情况下，只有少数样本被错误分类。按类别类型的分离非常明显；模型已经学会了一个表示，使得在大多数情况下可以轻松决定正确的类别标签。我们可以很容易地在t-SNE图中数出10个不同的簇。
- en: Summary
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we introduced the major components of convolutional neural
    networks. These are workhorse networks for modern deep learning, especially for
    vision tasks because of their ability to learn from spatial relationships. We
    worked through a model to classify MNIST digits and detailed new processing layers,
    including convolutional layers and pooling layers. We then learned that the fully
    connected layers of a CNN are analogs of the traditional neural networks we learned
    about in earlier chapters.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了卷积神经网络的主要组成部分。由于能够从空间关系中学习，这些是现代深度学习的主力网络，特别是在视觉任务中。我们通过一个分类MNIST数字的模型，详细讲解了新的处理层，包括卷积层和池化层。然后，我们了解到CNN的全连接层是我们在前几章中学习的传统神经网络的类似物。
- en: Next, we saw how to modify the fully connected layers to enable operation on
    larger inputs. Finally, we looked at the activations generated by the network
    when a sample image was passed through and saw how the convolution and pooling
    layers worked together to produce a new representation of the input, one that
    helped to separate the classes in the feature space, thereby enabling high accuracy.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们学习了如何修改全连接层，以便能够处理更大的输入。最后，我们观察了当一个样本图像通过网络时，网络生成的激活情况，并看到了卷积层和池化层如何协同工作，产生输入的新的表示，这有助于在特征空间中分离类别，从而实现高准确率。
- en: In the next chapter, we’ll continue our look at CNNs, but instead of theory,
    we’ll work with actual examples to see how the various parameters of the network,
    and the hyperparameters used during training, affect model performance. This will
    help us build intuition about how to work with CNNs in the future.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将继续探讨卷积神经网络（CNN），但不同于理论部分，我们将通过实际例子来展示网络的各个参数，以及训练过程中使用的超参数，如何影响模型的性能。这将帮助我们建立未来使用CNN时的直觉。
