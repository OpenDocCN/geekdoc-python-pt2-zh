- en: '**12'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**12'
- en: INTRODUCTION TO CONVOLUTIONAL NEURAL NETWORKS**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络简介**
- en: '![image](Images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/common.jpg)'
- en: In this chapter, we’ll introduce a new and potent approach to dealing with multidimensional
    information. In particular, we’ll work through the theory and high-level operation
    of *convolutional neural networks* (CNNs), a cornerstone of modern deep learning.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍一种处理多维信息的新方法，特别是我们将深入探讨*卷积神经网络*（CNN）的理论和高层次操作，CNN是现代深度学习的基石。
- en: We’ll begin by presenting the motivations behind the development of CNNs. Convolutions
    are the heart of CNNs, so they’ll come next. We’ll discuss them in some detail,
    in particular how they’re used by the CNN. We’ll then introduce a basic CNN and
    work through its anatomy. We’ll use this basic CNN architecture for the remainder
    of the chapter. After we dissect a CNN, we’ll work through how convolutional layers
    work. Then come pooling layers. We’ll see what they do, what benefit they offer,
    and what price they exact in return. To round out our discussion of the fundamental
    components of a CNN, we’ll present the fully connected layers, which, in reality,
    are just the layers of a traditional, fully connected, feed-forward neural network
    like those of [Chapter 8](ch08.xhtml#ch08).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先介绍CNN发展的动机。卷积是CNN的核心，所以下面我们会介绍卷积的内容。我们将详细讨论卷积的使用方式，特别是CNN是如何使用卷积的。接着，我们将介绍一个基本的CNN，并分析其结构。我们将在本章的其余部分使用这个基本的CNN架构。在剖析完CNN之后，我们将探讨卷积层是如何工作的。接下来是池化层，我们将了解池化层的作用，它们带来的好处以及它们的代价。为了完善我们对CNN基本组成部分的讨论，我们将介绍完全连接层，实际上，它们只是传统的完全连接前馈神经网络中的层，就像[第8章](ch08.xhtml#ch08)中的那样。
- en: 'One topic will be conspicuously absent from this chapter: the mechanics of
    training a CNN. In part, we’ll gloss over training because it’s messy once convolutional
    layers are introduced, but primarily because we’ve already discussed backpropagation
    in [Chapter 9](ch09.xhtml#ch09), and we use the same algorithm to train a CNN.
    We calculate the weights and biases of all layers from the average loss over the
    training minibatch and use backprop to determine the derivatives we need to update
    the weights and biases for each stochastic gradient descent step.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将明显缺少一个话题：训练CNN的机制。部分原因是，卷积层的引入使得训练过程变得复杂，但主要是因为我们在[第9章](ch09.xhtml#ch09)中已经讨论过反向传播算法，我们使用相同的算法来训练CNN。我们通过计算训练小批量数据的平均损失来更新所有层的权重和偏置，并通过反向传播计算所需的导数，从而更新每一步随机梯度下降的权重和偏置。
- en: Why Convolutional Neural Networks?
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为什么选择卷积神经网络？
- en: CNNs have several advantages over traditional neural networks. First, the convolutional
    layers of a CNN require vastly fewer parameters than fully connected neural networks,
    as we’ll see later in the chapter. CNNs require fewer parameters because the convolution
    operation applies parameters in each layer to small subsets of the input instead
    of the entire input at once, as is done with a traditional neural network.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 相比传统的神经网络，CNN（卷积神经网络）有几个优势。首先，CNN的卷积层所需的参数远少于完全连接的神经网络，正如我们在本章稍后会看到的那样。CNN之所以需要更少的参数，是因为卷积操作在每一层中只对输入的一个小子集应用参数，而不是像传统神经网络那样对整个输入一次性应用参数。
- en: Second, CNNs introduce the idea of *spatial invariance*, the ability to detect
    a spatial relationship in the input regardless of where it appears. For example,
    if the input to a neural network is an image of a cat, a traditional neural network
    will take the image in as a single feature vector, meaning that if a cat appears
    in the upper-left corner of the image, the network will learn that cats can appear
    in the upper-left corners of the image but not that they can also appear in the
    lower-right corners (unless the training data contains examples with cats in the
    lower-right corners). For a CNN, however, the convolution operation can detect
    cats anywhere they appear.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，CNN引入了*空间不变性*的概念，即无论输入中某个特征出现在哪里，CNN都能检测到它的空间关系。例如，如果神经网络的输入是猫的图像，传统的神经网络会将图像作为一个单一的特征向量进行处理，这意味着如果猫出现在图像的左上角，网络就会学习到猫可以出现在左上角，但无法学习到猫也可以出现在右下角（除非训练数据中包含了右下角有猫的例子）。然而，对于CNN，卷积操作可以检测到猫出现的任何位置。
- en: While CNNs are usually used with two-dimensional inputs, they can also be used
    with one-dimensional inputs, like the feature vectors we have worked with up to
    now. However, the feature vectors we’ve worked with, like the iris measurements,
    don’t reflect any sort of spatial relationship as the parts of an image of a cat
    do. There’s nothing there for the convolution operation to take advantage of.
    This doesn’t mean that a CNN won’t work, but it does mean that it might not be
    the best sort of model to use. As always, we need to understand how various model
    types operate so we select the best model for the task at hand.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管CNN通常用于二维输入数据，它们也可以用于一维输入数据，比如我们到目前为止处理的特征向量。然而，我们处理的特征向量，如虹膜测量数据，并不反映像猫图像的各个部分之间的空间关系。这里没有任何可以让卷积操作利用的空间结构。这并不意味着CNN无法工作，但确实意味着它可能不是最佳的模型选择。正如往常一样，我们需要了解各种模型类型的工作原理，以便为手头的任务选择最合适的模型。
- en: '**Note** *Depending on who you ask, CNNs were either developed in 1980 by Fukushima
    to implement the Neocognitron model or in 1998 by LeCun et al. as presented in
    their famous paper “Gradient-Based Learning Applied to Document Recognition,”
    which, as of this writing, has been referenced over 21,000 times. My take is that
    both deserve credit, though LeCun used the phrase* convolutional neural network
    *or* convnet *as they are still sometimes called, and what is described in the
    paper is what we will work with in this book. The Neocognitron reflected some
    of the ideas in a CNN, but not CNNs themselves.*'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意** *根据你问的人不同，CNN的发明时间有所不同。有些人认为是福岛在1980年为了实现Neocognitron模型而开发的，另一些人认为是LeCun等人在1998年通过他们著名的论文“基于梯度的学习应用于文档识别”中提出的，这篇论文至今已经被引用超过21,000次。我个人认为，两者都应该获得一定的认可，尽管LeCun使用了卷积神经网络*（convolutional
    neural network）*或者* convnet *这一术语，正如它们现在有时被称呼的那样，论文中描述的内容就是我们在本书中将要使用的内容。Neocognitron反映了一些CNN的思想，但它并非CNN本身。*'
- en: Convolution
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 卷积
- en: '*Convolution* involves sliding one thing over another. For us, this means sliding
    a *kernel*, a small 2D array, over the input, which might be the input image to
    the CNN or the output of a lower convolutional layer. There is a formal mathematical
    definition of convolution, but it really won’t help us right now. Luckily, all
    our inputs are discrete, which means we can get away with a bit of a hand-waving.
    For simplicity, we’ll focus on only the two-dimensional case.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*卷积*涉及将一个对象滑动到另一个对象上。对于我们来说，这意味着将一个*卷积核*，一个小的二维数组，滑动到输入数据上，这个输入可能是CNN的输入图像，或者是某个低层卷积层的输出。卷积有正式的数学定义，但现在它对我们帮助不大。幸运的是，我们的所有输入都是离散的，这意味着我们可以稍微简化一些处理。为了简便起见，我们只关注二维的情况。'
- en: Scanning with the Kernel
  id: totrans-13
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用卷积核扫描
- en: The *kernel* is the thing we are asking the convolutional layer to learn during
    training. It’s a collection of small 2D arrays that we move over the input. Ultimately,
    the kernels become the weights of a convolutional layer in a CNN.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*卷积核*是我们在训练过程中要求卷积层学习的内容。它是由多个小的二维数组组成，我们将它滑动到输入数据上。最终，卷积核会成为CNN中卷积层的权重。'
- en: The essential operation of convolution is taking some small section of the input,
    the same size as the kernel, covering it with the kernel, performing some operation
    on the set of numbers to produce a single output number, and then repeating the
    process after moving the kernel to a new position in the input. Just how far the
    kernel is moved is known as the *stride*. Typically, the stride is 1, meaning
    the kernel slides over one element of the input.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积的基本操作是：选取输入数据中一个与卷积核大小相同的小区域，用卷积核覆盖这个区域，对这些数字执行某些操作，得到一个单一的输出数字，然后将卷积核移动到输入的新位置后，重复该过程。卷积核滑动的距离被称为*步幅*（stride）。通常情况下，步幅为1，意味着卷积核滑动一个输入元素。
- en: '[Figure 12-1](ch12.xhtml#ch12fig1) shows the effect of convolution on part
    of an MNIST digit image.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 12-1](ch12.xhtml#ch12fig1)展示了卷积对MNIST数字图像一部分的影响。'
- en: '![image](Images/12fig01.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/12fig01.jpg)'
- en: '*Figure 12-1: Convolving a kernel with an image*'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 12-1：卷积核与图像的卷积*'
- en: The image portion is on the left of [Figure 12-1](ch12.xhtml#ch12fig1), where
    you can see part of a handwritten 8\. The boxes correspond to pixel intensities,
    though for presentation purposes, we’ve expanded the original image so that many
    shades of gray are visible in each “pixel” box. The actual pixel values the convolution
    works with are given next, after the arrow.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图像部分位于[图 12-1](ch12.xhtml#ch12fig1)的左侧，你可以看到手写数字8的一部分。框框代表了像素的强度，不过为了展示目的，我们将原始图像进行了扩展，使得每个“像素”框内能看到更多不同的灰度值。卷积操作所处理的实际像素值将在箭头后给出。
- en: Here, the kernel is
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，卷积核是
- en: '![image](Images/285equ01.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/285equ01.jpg)'
- en: This is the set of numbers we’ll slide over the input pixels. This is a 3 ×
    3 matrix, so we need to cover a 3 × 3 region of the input image. The first 3 ×
    3 region, the upper-left corner, is
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们将滑动到输入像素上的数字集合。这个是一个3 × 3的矩阵，因此我们需要覆盖输入图像的3 × 3区域。第一个3 × 3区域，即左上角是
- en: '![image](Images/285equ02.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/285equ02.jpg)'
- en: 'We said that convolution performs an operation with the kernel and the covered
    region as the input. The operation is straightforward: multiply corresponding
    entries and sum them. Finding the first output value of the convolution begins
    with'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们说过，卷积通过与核以及覆盖区域作为输入来执行操作。操作很简单：将对应的项相乘并求和。卷积的第一个输出值的计算从以下开始：
- en: '![image](Images/286equ01.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/286equ01.jpg)'
- en: When the preceding elements are summed, this gives the output value as
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 当前面的元素求和时，得到的输出值是
- en: 0 + (–248) + 0 + (–145) + 759 + (–54) + 0 + (–253) + 0 = 59
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 0 + (–248) + 0 + (–145) + 759 + (–54) + 0 + (–253) + 0 = 59
- en: Okay, the output of the first convolution operation is 59\. What do we do with
    that number? The kernel is 3 × 3, an odd number along each side. This means that
    there is a middle element, the one with the 3 in it. The place in the output array
    where the middle number is gets replaced with the output value, the 59\. [Figure
    12-1](ch12.xhtml#ch12fig1) shows the full output of the convolution. Sure enough,
    the first element of the output is 59, located at the center of the kernel when
    the kernel is covering the upper-left corner.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，第一个卷积操作的输出是59。我们该如何处理这个数字呢？核是3 × 3，每一边都是奇数。这意味着它有一个中间元素，那个含有3的元素。在输出数组中，那个包含中间数字的位置会被输出值59替代。[图
    12-1](ch12.xhtml#ch12fig1)显示了卷积的完整输出。果然，当核覆盖左上角时，输出的第一个元素是59，位于核的中心。
- en: The remaining output values are calculated in precisely the same way but by
    moving the kernel over 1 pixel each time. When the end of a row is reached, the
    kernel moves back to the left side but down 1 pixel. In this way, it slides over
    the entire input image to produce the output shown in [Figure 12-1](ch12.xhtml#ch12fig1),
    just like the scan lines of an old analog television.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 剩余的输出值通过完全相同的方式计算，但每次将核移动1个像素。当一行的末尾被到达时，核会回到左侧，但下移1个像素。这样，它就像旧式模拟电视的扫描线一样，滑过整个输入图像，从而产生[图
    12-1](ch12.xhtml#ch12fig1)中显示的输出。
- en: The next output value is
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个输出值是
- en: '![image](Images/286equ02.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/286equ02.jpg)'
- en: which sums to *–* 212, as we see on the right side of [Figure 12-1](ch12.xhtml#ch12fig1).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 求和后得到*–* 212，正如我们在[图 12-1](ch12.xhtml#ch12fig1)右侧所看到的。
- en: Repeating the convolution operation produces the output shown in [Figure 12-1](ch12.xhtml#ch12fig1).
    Notice the empty boxes around the output. These values are empty because the middle
    of our 3 × 3 kernel does not cover the edge of the input array. Therefore, the
    output matrix of numbers is two smaller in each dimension than the input. If the
    kernel were 5 × 5, there would be a border 2 pixels wide instead of 1.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 重复卷积操作会产生[图 12-1](ch12.xhtml#ch12fig1)中显示的输出。注意输出周围的空框。这些值是空的，因为我们的3 × 3核的中间部分没有覆盖输入数组的边缘。因此，输出的数字矩阵在每个维度上都比输入小2。如果核是5
    × 5，那么边界将宽2个像素，而不是1个像素。
- en: Implementations of 2D convolution need to make a decision about these border
    pixels. There are options, and most toolkits support several of them. One is to
    simply ignore these pixels and make the output smaller than the input, as we’ve
    shown in [Figure 12-1](ch12.xhtml#ch12fig1). This approach is often known as *exact*
    or *valid* because we retain only values that are actually output by the operation.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 2D卷积的实现需要对这些边界像素做出决策。有几种选项，大多数工具包都支持其中的一些。一种方法是简单地忽略这些像素，使输出比输入小，正如我们在[图 12-1](ch12.xhtml#ch12fig1)中所展示的那样。这种方法通常被称为*精确*或*有效*，因为我们仅保留那些实际由操作输出的值。
- en: Another approach is to imagine that a border of 0 values surrounds the input
    image. The border is as thick as is needed so that the kernel fits with its middle
    value matching the upper-left pixel of the input. For our example in [Figure 12-1](ch12.xhtml#ch12fig1),
    this means a border of 1 pixel because the kernel is 3 × 3 and there is one element
    on either side of the kernel’s center value. If the kernel were 5 × 5, the border
    would be 2 pixels since there are two values on either side of the kernel center.
    This is known as *zero-padding* and gives an output that is the same size as the
    input. Instead of convolving a 28×28 pixel MNIST digit image with a 3 × 3 kernel
    and getting a 26×26 pixel output as shown in [Figure 12-1](ch12.xhtml#ch12fig1),
    we get an output that is also 28×28 pixels.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是想象输入图像四周围绕着一个0值的边框。这个边框的厚度根据需要来调整，以便使卷积核的中心值与输入图像的左上角像素匹配。对于[图12-1](ch12.xhtml#ch12fig1)中的示例，这意味着边框宽度为1个像素，因为卷积核是3×3，并且卷积核中心的两侧各有一个元素。如果卷积核是5×5，则边框宽度为2个像素，因为卷积核中心的两侧各有两个值。这被称为*零填充*，并且输出的大小与输入相同。这样，卷积一个28×28像素的MNIST数字图像与一个3×3卷积核，得到的输出仍然是28×28像素，而不是如[图12-1](ch12.xhtml#ch12fig1)所示的26×26像素输出。
- en: If we zero pad the example image in [Figure 12-1](ch12.xhtml#ch12fig1), we can
    fill in the first empty output square like so
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们对[图12-1](ch12.xhtml#ch12fig1)中的示例图像进行零填充，我们可以像这样填充第一个空的输出方格。
- en: '![image](Images/287equ01.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/287equ01.jpg)'
- en: which sums to *–* 213\. This means that the upper-left corner of the output
    matrix in [Figure 12-1](ch12.xhtml#ch12fig1), which currently has an empty box,
    could be replaced by *–* 213\. Similarly, the rest of the empty boxes would have
    values, and the output of the convolution operation would be 28×28 pixels.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 其总和为*–* 213。这意味着在[图12-1](ch12.xhtml#ch12fig1)中的输出矩阵的左上角，当前是一个空框，可以替换为*–* 213。同样，其他空框也会有值，卷积操作的输出将是28×28像素。
- en: Convolution for Image Processing
  id: totrans-39
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 图像处理中的卷积
- en: Convolution, when used in a neural network, is sometimes viewed as magical,
    a special operation that lets convolutional neural networks do the wonderful things
    that they can do. This is more or less true, but the convolution operation is
    certainly not anything new. Even if we ignore mathematics entirely and think only
    of the discrete convolution of 2D images, we see that image scientists were using
    convolution for image processing decades before convolution was applied to machine
    learning.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积在神经网络中使用时，有时被视为一种神奇的操作，允许卷积神经网络做出它们所能做的奇妙事情。这个说法或多或少是正确的，但卷积操作绝对不是新鲜事物。即使我们完全忽略数学，只考虑二维图像的离散卷积，我们也会看到，图像科学家们在卷积被应用于机器学习之前的几十年，就已经在图像处理领域使用卷积了。
- en: The convolution operation allows for all manner of image processing. For example,
    consider the images shown in [Figure 12-2](ch12.xhtml#ch12fig2).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积操作可以实现各种各样的图像处理。例如，考虑[图12-2](ch12.xhtml#ch12fig2)中展示的图像。
- en: '![image](Images/12fig02.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/12fig02.jpg)'
- en: '*Figure 12-2: 5 × 5 convolution kernels applied to an image*'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*图12-2：应用于图像的5×5卷积核*'
- en: The original moon image is on the upper left. The other three images are the
    output from convolving the moon image with different 5 × 5 kernels. Moving clockwise
    from the upper right, the kernels either emphasize edges, diagonal structures
    (upper left to lower right), or blur the input image. All of this is accomplished
    by changing the values in the kernel, but the convolution operation remains the
    same.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 原始月亮图像位于左上角。其他三幅图像是通过将月亮图像与不同的5×5卷积核进行卷积得到的输出。从右上角开始，顺时针方向，卷积核分别突出边缘、对角线结构（从左上到右下）或模糊输入图像。所有这些都通过改变卷积核中的值来实现，但卷积操作本身保持不变。
- en: From a machine learning perspective, the power of a convolutional approach comes
    partially from the savings in terms of parameters. If a model can learn a set
    of kernels, that is a smaller set of numbers to learn than the weights for a fully
    connected model. This is a good thing on its own. The fact that a convolution
    can pull out other information about an image, such as its slowly changing components
    (the blur of [Figure 12-2](ch12.xhtml#ch12fig2)), its rapidly changing components
    (the edges of [Figure 12-2](ch12.xhtml#ch12fig2)), or even components along a
    specific direction (the diagonals of [Figure 12-2](ch12.xhtml#ch12fig2)), means
    that the model gains insight as to what is in the input. And, since we move the
    kernel over the image, we’re not dependent upon *where* in the image these structures
    occur.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 从机器学习的角度来看，卷积方法的优势部分来自于参数的节省。如果一个模型可以学习一组卷积核，那么学习这些卷积核所需的数字集合比全连接模型所需的权重少。这本身就是一件好事。卷积可以提取图像中的其他信息，例如图像的缓慢变化部分（[图12-2](ch12.xhtml#ch12fig2)的模糊），快速变化的部分（[图12-2](ch12.xhtml#ch12fig2)的边缘），甚至沿特定方向的部分（[图12-2](ch12.xhtml#ch12fig2)的对角线），这意味着模型能够获得有关输入的更多信息。而且，由于我们将卷积核在图像上滑动，我们并不依赖于这些结构在图像中的具体位置。
- en: Anatomy of a Convolutional Neural Network
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 卷积神经网络的解剖学
- en: Medical students learn about anatomy by dissecting a cadaver to see the parts
    and how they relate to each other. In similar, though less challenging, fashion,
    we’ll start with the body of a CNN, an illustration of its basic architecture,
    and then pull it apart to learn what each component is and what it does.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 医学生通过解剖尸体来学习解剖学，观察各部分及其相互关系。同样，尽管挑战性较小，我们将从CNN的结构开始，展示其基本架构，然后逐步拆解它，学习每个组件的功能及其作用。
- en: '[Figure 12-3](ch12.xhtml#ch12fig3) shows us our body. This is the default example
    CNN used by the Keras toolkit to train a model that classifies MNIST digits. We’ll
    use it as our standard for the remainder of this chapter.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[图12-3](ch12.xhtml#ch12fig3)展示了我们的身体。这是Keras工具包用于训练分类MNIST数字模型的默认示例CNN。我们将在本章的其余部分使用它作为标准。'
- en: '![image](Images/12fig03.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/12fig03.jpg)'
- en: '*Figure 12-3: The architecture of a basic convolutional neural network*'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '*图12-3：基本卷积神经网络的架构*'
- en: How do we interpret this figure? Like a traditional neural network, a CNN has
    an input and an output. In this case, the input is the digit image on the upper
    left. The network then flows left to right, following the arrows. At the end of
    the top row, the network continues on the following row. Note, we’ve duplicated
    the layer at the end of the top row and placed it at the beginning of the next
    row for presentation purposes.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何解读这个图？像传统神经网络一样，卷积神经网络（CNN）也有输入和输出。在这个例子中，输入是左上角的数字图像。网络从左到右流动，沿着箭头的方向。到达顶部行的末尾时，网络继续沿着下一行进行。请注意，我们已将顶部行末的层复制并放置到下一行的开始部分，便于展示。
- en: The flow continues along the bottom row, again left to right, until the output
    is reached. The output here is a softmax layer to give us the likelihoods of each
    of the possible digits, just as we saw for the traditional neural networks of
    [Chapter 10](ch10.xhtml#ch10).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 流程继续沿着底行，从左到右，直到达到输出。这里的输出是一个softmax层，用于给出每个可能数字的概率，就像我们在[第10章](ch10.xhtml#ch10)的传统神经网络中看到的那样。
- en: Different Types of Layers
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 层的不同类型
- en: 'Between each arrow is a layer of the network. The first thing we notice is
    that, unlike a traditional neural network, a CNN has many kinds of layers. Let’s
    list them here. We’ll discuss each in turn:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 每个箭头之间是网络的一个层。我们首先注意到，与传统神经网络不同，卷积神经网络有许多种类的层。我们在这里列出它们，并将逐一讨论每一种：
- en: Convolutional (*Conv*)
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积（*Conv*）
- en: ReLU
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ReLU
- en: Pooling (*Pool*)
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 池化（*Pool*）
- en: Dropout
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dropout
- en: Flatten
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 展平
- en: Dense
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dense
- en: We should note that we’re using the Keras names for the layers. For instance,
    Keras uses *Dense* for what many other toolkits call *fully connected* or even
    *InnerProduct* layers.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该注意到，我们使用的是Keras命名的层。例如，Keras使用*Dense*来表示许多其他工具包称之为*全连接*或*内积*层。
- en: Several of these layers should already be familiar. We know a ReLU layer implements
    a rectified linear unit that takes each of its inputs and asks if it is greater
    than or less than 0\. If the input is less than 0, the output is 0; otherwise,
    the output is the input. We can express this mathematically as
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一些层应该已经很熟悉。我们知道，ReLU层实现了一个修正线性单元，它会检查每个输入是否大于或小于0。如果输入小于0，输出为0；否则，输出为输入值。我们可以用数学公式表达为：
- en: ReLU(*x*) = max(0, *x*)
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU(*x*) = max(0, *x*)
- en: where the *max* function returns the largest of its two arguments.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，*max*函数返回其两个参数中的最大值。
- en: Likewise, we mentioned dropout in [Chapter 9](ch09.xhtml#ch09). Dropout selects
    a percentage of its outputs at random during training and sets them to 0\. This
    provides a powerful form of regularization to help the network learn meaningful
    representations of the input data. There are two dropout layers in our basic CNN.
    The first uses a probability of 25 percent, meaning during any minibatch pass
    while training, some 25 percent of the outputs will be set to 0\. The second dropout
    layer uses a probability of 50 percent.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们在[第9章](ch09.xhtml#ch09)中提到了dropout。Dropout在训练过程中随机选择一定比例的输出，并将其设置为0。这为网络提供了一种强大的正则化形式，帮助网络学习输入数据的有意义表示。我们的基本CNN中有两个dropout层，第一个dropout层的概率为25%，这意味着在每次训练的小批量传递过程中，大约25%的输出会被设置为0。第二个dropout层的概率为50%。
- en: The *Flatten* and *Dense* layers are old friends, though we know them by another
    name and not as independent entities. Our traditional feedforward neural network
    uses fully connected layers to process a one-dimensional vector. Here, Flatten
    and Dense work together to implement a fully connected layer. The Flatten layer
    takes its input—usually a four-dimensional array (we’ll see why later)—and turns
    it into a vector. It does something similar to what we did to construct the vector
    form of the MNIST dataset, where we put the pixels of each row end-to-end to unravel
    the two-dimensional image. The Dense layer implements a traditional neural network
    layer, where each input value is mapped to each node of the Dense layer. Typically,
    the output of the Dense layer is passed to another Dense layer or a softmax layer
    to let the network make predictions.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '*Flatten*和*Dense*层是老朋友，尽管我们通常以另一种名称认识它们，而不是作为独立的实体。我们传统的前馈神经网络使用全连接层处理一维向量。在这里，Flatten和Dense一起实现一个全连接层。Flatten层将其输入（通常是一个四维数组，我们稍后会解释原因）转换为一个向量。它的工作类似于我们构建MNIST数据集的向量形式时所做的事情，即将每一行的像素首尾相接，展开二维图像。Dense层实现传统的神经网络层，其中每个输入值都映射到Dense层的每个节点。通常，Dense层的输出会传递给另一个Dense层或softmax层，以便网络进行预测。'
- en: Internally, many layers of a CNN expect four-dimensional arrays as inputs and
    produce four-dimensional arrays as outputs. The first dimension is the number
    of inputs in the minibatch. So, if we have a minibatch of 24, the first dimension
    of the 4D array will be 24.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在内部，许多CNN层期望四维数组作为输入，并生成四维数组作为输出。第一维是小批量中的输入数量。因此，如果我们有一个大小为24的小批量，那么四维数组的第一维将是24。
- en: The second and third dimensions are called the *height* and *width*. If the
    input to a layer is the input to the model (say, an image), then these dimensions
    are truly the height and width dimensions of the image. If the input is really
    the output of some other layer, say a (yet to be described) convolutional layer,
    the *height* and *width* refer to the output from applying a convolutional kernel
    to some input. For example, the output in [Figure 12-1](ch12.xhtml#ch12fig1) has
    height and width of 26.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 第二维和第三维被称为*高度*和*宽度*。如果层的输入是模型的输入（例如，一张图像），那么这两个维度实际上就是图像的高度和宽度。如果输入实际上是其他层的输出，例如一个（稍后描述的）卷积层的输出，则*高度*和*宽度*指的是应用卷积核于某个输入后得到的输出。例如，[图12-1](ch12.xhtml#ch12fig1)中的输出的高度和宽度为26。
- en: The last dimension is the number of channels, if an input image; or the number
    of *feature maps*, if the output of a convolutional or pooling layer. The number
    of channels in an image is simply the number of bands, where a grayscale image
    has a single band and a color image typically has three bands, one each for red,
    green, and blue. Some color images also have an alpha channel used to specify
    how transparent a pixel is, but these are typically dropped before passing the
    image through a CNN.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一维是通道数，如果是输入图像的话；如果是卷积层或池化层的输出，则是*特征图*的数量。图像中的通道数实际上就是带的数量，灰度图像有一个带，彩色图像通常有三个带，分别对应红色、绿色和蓝色。有些彩色图像还有一个alpha通道，用于指定像素的透明度，但这些通常会在将图像传递给CNN之前被丢弃。
- en: The output in [Figure 12-1](ch12.xhtml#ch12fig1) is called a *feature map* because
    it is the response from convolving a kernel over an input. As we saw in [Figure
    12-2](ch12.xhtml#ch12fig2), convolving a kernel over an image can pull out features
    in the image, so the outputs of the kernels used by a convolutional layer are
    called *feature maps*.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[图12-1](ch12.xhtml#ch12fig1)中的输出被称为*特征图*，因为它是将卷积核在输入上进行卷积操作的响应。正如我们在[图12-2](ch12.xhtml#ch12fig2)中看到的那样，通过对图像进行卷积操作，可以提取出图像中的特征，因此卷积层使用的卷积核的输出称为*特征图*。'
- en: 'This leaves two layers to investigate: *Convolutional* and *Pooling*. These
    layers are new.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这留下了两个层次需要研究：*卷积*层和*池化*层。这些层是新的。
- en: In our basic CNN, the convolutions operate on sets of two-dimensional inputs
    where by *set* I mean a stack of two-dimensional arrays, where the third dimension
    is the number of channels or feature maps. This means that unlike every other
    model we’ve looked at in this book, the input here really is the full image, not
    a vector created from the image. In terms of CNNs, however, the convolutions need
    not operate on only two-dimensional inputs. Three-dimensional convolutions exist,
    as do one-dimensional, though both are seldom used compared to two-dimensional
    convolutions.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的基本CNN中，卷积操作作用于二维输入的集合，其中*集合*指的是一个二维数组的堆叠，第三维是通道数或特征图的数量。这意味着与我们在本书中看到的其他模型不同，这里的输入是真正的完整图像，而不是从图像创建的向量。然而，在CNN中，卷积不一定仅作用于二维输入。也存在三维卷积和一维卷积，尽管这两种卷积相比于二维卷积使用得较少。
- en: A pooling layer is used to reduce the spatial dimension of its input by combining
    input values according to some rule. The most common rule is *max*, where the
    largest value in the small block moved over the input is kept; the other values
    are discarded. Again, we’ll cover pooling layers at length in this chapter.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 池化层用于通过按照某些规则合并输入值来减少输入的空间维度。最常见的规则是*最大池化*，即在输入的一个小块中保留最大值，其他值则被丢弃。我们将在本章中详细讨论池化层。
- en: Many other layer types can be used by modern networks, and many of these are
    directly supported in Keras already, though it’s possible to add your own layers.
    This flexibility is one reason Keras often quickly supports new deep learning
    developments. As with a traditional neural network, for a layer to have weights
    that can be learned, the layer needs to be differentiable in a mathematical sense
    so that the chain rule can continue, and the partial derivatives can be calculated
    to learn how to adjust the weights during gradient descent. If the previous sentence
    is not clear, it’s time to review the backprop section of [Chapter 9](ch09.xhtml#ch09).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现代网络可以使用许多其他类型的层，且这些层中的许多在Keras中已经得到直接支持，尽管也可以添加自定义层。这种灵活性是Keras能够快速支持新深度学习发展的原因之一。与传统的神经网络一样，为了让某一层具有可以学习的权重，该层在数学意义上需要是可微分的，以便链式法则可以继续应用，部分导数可以计算出来，从而学习如何在梯度下降过程中调整权重。如果前一句话不清楚，是时候回顾一下[第9章](ch09.xhtml#ch09)的反向传播部分了。
- en: Passing Data Through the CNN
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据通过CNN的传递
- en: Let’s look again at [Figure 12-3](ch12.xhtml#ch12fig3). A lot is happening here
    beyond just the order and names of the layers. Many layers have numbers in italics
    running along the bottom. These numbers represent the dimensions of the output
    of the layer, the height, width, and number of feature maps. If the layer has
    only a single number, it outputs a vector with that many elements.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再看看[图12-3](ch12.xhtml#ch12fig3)。这里不仅仅是层的顺序和名称在变化。许多层的底部有斜体数字，这些数字表示该层输出的维度，包括高度、宽度和特征图的数量。如果该层只有一个数字，它输出一个包含该数量元素的向量。
- en: The input to the CNN is a 28 × 28 × 1 image. The output of a convolutional layer
    is a set of feature maps. Thus the output of the first convolutional layer is
    26 × 26 × 32, meaning there are 32 feature maps, each a 26 × 26 image calculated
    from the single 28 × 28 × 1 input image. Similarly, the output of the second convolutional
    layer is 24 × 24 × 64, a set of 64 feature maps derived from the 26 × 26 × 32
    input, which was itself the output of the first convolutional layer.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: CNN的输入是28 × 28 × 1的图像。卷积层的输出是一组特征图。因此，第一层卷积层的输出是26 × 26 × 32，这意味着有32个特征图，每个特征图是从单个28
    × 28 × 1的输入图像计算得出的26 × 26的图像。同样，第二层卷积层的输出是24 × 24 × 64，这是一组由26 × 26 × 32的输入计算得出的64个特征图，而这个输入本身是第一层卷积层的输出。
- en: We see that the pooling layer at the end of the first row takes its 24 × 24
    × 64 input and reduces it to 12 × 12 × 64\. The “max” label tells us what the
    pooling is doing; it takes a 2 × 2 region of the input and returns the largest
    value. Since the input is 2 × 2 and it returns only one value, this reduces each
    24 × 24 input to a 12 × 12 output. This process is applied to each feature map
    so that the output is 12 × 12 × 64.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，第一行末尾的池化层将其 24 × 24 × 64 的输入减少到 12 × 12 × 64。标签“max”告诉我们池化层的操作；它会对输入的 2
    × 2 区域进行操作，并返回最大的值。由于输入是 2 × 2，且池化层只返回一个值，这将每个 24 × 24 的输入减少为 12 × 12 的输出。这个过程应用于每个特征图，因此输出为
    12 × 12 × 64。
- en: Looking at the bottom row of [Figure 12-3](ch12.xhtml#ch12fig3) shows us that
    the Flatten layer takes the 12 × 12 × 64 output of the pooling layer and turns
    it into a vector of 9,216 elements. Why 9,216? Because 12 × 12 × 64 = 9,216\.
    Next, the Dense layer has 128 nodes, and, finally, our output softmax has 10 nodes
    because there are 10 classes, the digits 0 through 9.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下 [图 12-3](ch12.xhtml#ch12fig3) 的底部行，我们可以看到 Flatten 层将池化层的 12 × 12 × 64 输出转换成了一个包含
    9,216 个元素的向量。为什么是 9,216？因为 12 × 12 × 64 = 9,216。接下来，Dense 层有 128 个节点，最后我们的输出 softmax
    层有 10 个节点，因为有 10 个类别，数字 0 到 9。
- en: In [Figure 12-3](ch12.xhtml#ch12fig3), the ReLU and Dropout layers have no numbers
    below them. These layers do not alter the shape of their inputs. They simply perform
    some operation on each of the elements regardless of the shape.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [图 12-3](ch12.xhtml#ch12fig3) 中，ReLU 和 Dropout 层下面没有数字。这些层不会改变输入的形状。它们仅对每个元素执行某些操作，无论其形状如何。
- en: 'The convolutional layers of our basic CNN have other numbers associated with
    them: “3 × 3” and “32” or “64”. The 3 × 3 tells us the size of the convolutional
    kernel, and the 32 or 64 tells us the number of feature maps.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的基本 CNN 的卷积层还有其他相关的数字：“3 × 3”和“32”或“64”。3 × 3 告诉我们卷积核的大小，而 32 或 64 告诉我们特征图的数量。
- en: We already alluded to the 2 × 2 part of the pooling layer. This represents the
    size of the pooling kernel, which, much like a convolutional kernel, slides over
    the input, feature map by feature map (or channel by channel), to reduce the size
    of the input. Working with a 2 × 2 pooling kernel implies that, typically, the
    output will be one-half the size of the input in each of the row and column dimensions.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经提到了池化层中的 2 × 2 部分。这代表了池化核的大小，池化核就像卷积核一样，在输入的每个特征图（或通道）上滑动，减少输入的大小。使用 2 ×
    2 池化核意味着，通常情况下，输出在行和列维度上将是输入的一半大小。
- en: '[Figure 12-3](ch12.xhtml#ch12fig3) has familiar parts, but the presentation
    is new to us, and we have these mysterious new layers to think about, like convolutional
    and pooling layers, so we are sure to be somewhat nebulous in our understanding
    right now. That is perfectly fine. We have new ideas and some visual indications
    of how they link together to make a CNN. For now, this is all we need. The remainder
    of this chapter will, I hope, be a series of “aha!” moments for you as you think
    back to this figure and its parts. When you understand what each is doing, you’ll
    start to see why they are where they are in the processing chain, leading from
    image input to output softmax predictions.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 12-3](ch12.xhtml#ch12fig3) 有一些熟悉的部分，但展示的方式对我们来说是新的，我们还需要思考一些神秘的新层，例如卷积层和池化层，因此我们现在对这些内容的理解可能会有些模糊。这个完全没问题。我们已经有了一些新想法，并通过一些视觉指示看到了它们如何连接在一起形成一个
    CNN。现在，这些就是我们所需要的。接下来的章节将会带给你一系列的“啊哈”时刻，当你回顾这张图及其各部分时，你会明白每个部分的作用，进而理解它们在图像输入到输出
    softmax 预测的处理链中所处的位置。'
- en: Convolutional Layers
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 卷积层
- en: If our discussion of convolution ended with the preceding sections, we’d understand
    the essential operation but still be in the dark about exactly *how* a convolutional
    layer in a CNN works. Bearing this in mind, let’s look at how the convolution
    idea generalizes across the inputs and outputs of a CNN’s convolutional layer.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们关于卷积的讨论仅仅停留在前面的部分，我们会理解卷积的基本操作，但仍然无法确切理解卷积层在 CNN 中是如何工作的。考虑到这一点，让我们来看看卷积的概念是如何在
    CNN 的卷积层的输入和输出中泛化的。
- en: How a Convolution Layer Works
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 卷积层是如何工作的
- en: The input and output of a convolutional layer can both be thought of as stacks
    of 2D arrays (or matrices). The operation of the convolutional layer is best illustrated
    with a simple example showing how to map the input stack of arrays to the output
    stack.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层的输入和输出都可以被看作是 2D 数组（或矩阵）的堆叠。卷积层的操作通过一个简单的例子最能说明，展示如何将输入数组堆栈映射到输出数组堆栈。
- en: Before we present our example, we need to introduce some terminology. We previously
    described the convolution operation in terms of applying a kernel to an input,
    both of which are 2D. We’ll continue to use the term *kernel* for this single,
    2D matrix. When implementing a convolutional layer, however, we’ll soon see that
    we need stacks of kernels, which are typically referred to in machine learning
    as *filters*. A filter is a stack of kernels. The filter, via its kernels, is
    applied over the input stack to produce the output stack. Since during training
    the model is learning kernels, it is fair to say that the model is also learning
    filters.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们展示示例之前，需要介绍一些术语。我们之前描述了卷积操作，即将一个内核应用于输入，二者都是二维的。我们将继续使用*内核*这一术语来指代这个单一的二维矩阵。然而，在实现卷积层时，我们很快就会看到，我们需要一堆内核，这些内核在机器学习中通常被称为*滤波器*。滤波器是内核的堆叠。滤波器通过它的内核作用于输入堆栈，从而产生输出堆栈。由于在训练过程中模型学习的是内核，因此可以说模型也在学习滤波器。
- en: For our example, the input is a stack of two 5 × 5 arrays, the kernel size is
    3 × 3, and we want an output stack that is three deep. Why three? Because, as
    the designer of the CNN architecture, we believe that learning three outputs will
    help the network learn the task at hand. The convolution operation determines
    the width and height of each output array; we select the depth. We’ll use valid
    convolution, losing a border of thickness one on the output, meaning our input
    will drop two in width and height. Therefore, a 5 × 5 input convolved with a 3
    × 3 kernel will create a 3 × 3 output.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，输入是两个 5 × 5 的数组堆栈，内核大小为 3 × 3，我们希望得到一个深度为三的输出堆栈。为什么是三？因为作为 CNN 架构的设计者，我们认为学习三个输出将有助于网络学习当前任务。卷积操作决定了每个输出数组的宽度和高度；我们选择深度。我们将使用有效卷积，输出会丢失一像素的边框，这意味着输入的宽度和高度都会减少
    2。因此，一个 5 × 5 的输入与一个 3 × 3 的内核卷积后将产生一个 3 × 3 的输出。
- en: That accounts for the change in dimension, but how do we go from a stack of
    two arrays to a stack of three? The key to mapping the 5 × 5 × 2 input to the
    desired 3 × 3 × 3 output is the set of kernels, the filter, learned during training.
    Let’s see how the filter gives us the mapping we want.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这解释了尺寸变化，但我们如何将两个数组的堆栈转换为三个数组的堆栈呢？将 5 × 5 × 2 的输入映射到所需的 3 × 3 × 3 输出的关键是训练过程中学习到的一组内核，即滤波器。让我们看看滤波器是如何提供我们想要的映射的。
- en: We’ll assume we already know the filters at this point, each of which is a 3
    × 3 × 2 stack of kernels. In general, if there are *M* arrays in the input stack
    and we want *N* arrays in the output stack using a kernel that is *K* × *K*, then
    we need a set of *N* filters, each one of which is a stack of *K* × *K* kernels
    *M* deep. Let’s explore why.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设此时已经知道滤波器，每个滤波器是一个 3 × 3 × 2 的内核堆叠。一般来说，如果输入堆栈中有 *M* 个数组，并且我们希望输出堆栈中有 *N*
    个数组，且使用的内核是 *K* × *K*，那么我们需要一组 *N* 个滤波器，每个滤波器都是一个 *K* × *K* 的内核堆栈，深度为 *M*。让我们来探讨一下原因。
- en: 'If we break up the stack so we can see each element clearly, our input stack
    looks like this:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们把堆栈分开，这样可以清晰地看到每个元素，那么我们的输入堆栈看起来是这样的：
- en: '![image](Images/293equ01.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/293equ01.jpg)'
- en: We have two 5 × 5 matrices labeled 0 and 1\. The values were selected at random.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有两个 5 × 5 的矩阵，分别标记为 0 和 1。值是随机选择的。
- en: To get an output stack of three, we need a set of three filters. The stack of
    kernels in each filter is two deep, to mirror the number of arrays in the input
    stack. The kernels themselves are 3 × 3, so we have three 3 × 3 × 2 filters, where
    we convolve each kernel in the filter with the corresponding input array. The
    three filters are
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 为了得到一个深度为三的输出堆栈，我们需要一组三个滤波器。每个滤波器中的内核堆栈深度为二，以与输入堆栈中的数组数量相对应。内核本身是 3 × 3 的，所以我们有三个
    3 × 3 × 2 的滤波器，其中每个滤波器的内核与相应的输入数组进行卷积。这三个滤波器是：
- en: '![image](Images/293equ02.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/293equ02.jpg)'
- en: where we’ve added 0 and 1 labels to show which kernels are applied to which
    input stack arrays. We also have a bias vector, as we did for the traditional
    neural network layers. This is a vector, one value for each kernel stack, that
    we add in at the end to help align the output of the convolutional layer to the
    data, just as we did for the traditional neural network layers. The bias adds
    one more degree of freedom to the layer—one more thing that can be learned to
    help the layer learn the most it can from the data. For our example, the bias
    vector is *b* = *{*1,0,2*}*, selected at random.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里添加了0和1标签，以显示哪些卷积核应用于哪些输入堆栈数组。我们还拥有一个偏置向量，正如我们在传统神经网络层中所做的那样。这是一个向量，每个卷积核堆栈都有一个值，我们在最后加上它，帮助将卷积层的输出与数据对齐，就像我们在传统神经网络层中所做的那样。偏置为层添加了一个额外的自由度——它是一个可以学习的项，帮助层从数据中学习到更多东西。在我们的示例中，偏置向量是
    *b* = *{*1,0,2*}*，是随机选择的。
- en: To get the output stack, we convolve each kernel of each filter with the corresponding
    input array, sum the elements of the resulting output, and add the bias value.
    For filter *k*[0], we convolve the first input array with the first kernel to
    get
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得输出堆栈，我们将每个卷积核与相应的输入数组进行卷积，求出输出的元素之和，并加上偏置值。对于卷积核 *k*[0]，我们将第一个输入数组与第一个卷积核进行卷积，得到：
- en: '![image](Images/294equ01.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/294equ01.jpg)'
- en: 'Note we’re using * to mean the full convolution operation, which is fairly
    standard. We repeat this operation for the second kernel in *k*[0], applying it
    to the second array of the input:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用 * 来表示完整的卷积操作，这是相当标准的。我们对 *k*[0] 中的第二个卷积核重复这个操作，将其应用于输入的第二个数组：
- en: '![image](Images/294equ02.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/294equ02.jpg)'
- en: 'Finally, we sum the two convolution outputs and add in the bias value:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将两个卷积输出相加，并加上偏置值：
- en: '![image](Images/294equ03.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/294equ03.jpg)'
- en: This gives us the first output array, the application of filter *k*[0] to the
    input stack.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我们第一个输出数组，即卷积核 *k*[0] 对输入堆栈的应用。
- en: We repeat this process for filters *k*[1] and *k*[2] to get their outputs so
    that the final convolutional layer output for the given input is
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对卷积核 *k*[1] 和 *k*[2] 执行相同的过程，以获得它们的输出，因此给定输入的最终卷积层输出为：
- en: '![image](Images/294equ04.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/294equ04.jpg)'
- en: where we have written the stacked arrays side by side, a 3 × 3 × 3 output, as
    desired.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将堆叠的数组并排写出，得到一个3 × 3 × 3的输出，正如所希望的那样。
- en: Our convolutional layer example mapped a 5 × 5 × 2 input to a 3 × 3 × 3 output.
    If we naïvely used a fully connected layer instead, we would need a weight matrix
    that has 50 × 27 = 1350 weights that need to be learned. In contrast, the convolutional
    layer used only 3 × 3 × 2 weights per filter and three filters for a total of
    54 weights, excluding bias values. This is a significant reduction.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的卷积层示例将一个5 × 5 × 2的输入映射到3 × 3 × 3的输出。如果我们天真地使用全连接层替代，则需要一个包含50 × 27 = 1350个权重的权重矩阵，这些权重需要学习。相比之下，卷积层每个卷积核只使用了3
    × 3 × 2个权重，三个卷积核总共需要54个权重（不包括偏置值）。这是一项显著的减少。
- en: Using a Convolutional Layer
  id: totrans-109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用卷积层
- en: The preceding example showed us how a convolutional layer works. Now let’s see
    the effect of one. Imagine that we’ve trained the network shown in [Figure 12-3](ch12.xhtml#ch12fig3),
    so we have the weights and biases we need to run unknown images through the network.
    (You’ll see how to train a CNN in Chapter Experiments with Keras and MNIST.)
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的示例向我们展示了卷积层是如何工作的。现在让我们看看它的效果。假设我们已经训练了[图12-3](ch12.xhtml#ch12fig3)中显示的网络，因此我们拥有运行未知图像时所需的权重和偏置。（你将在《使用Keras和MNIST的实验》一章中看到如何训练CNN。）
- en: 'The first layer of the network in [Figure 12-3](ch12.xhtml#ch12fig3) is a convolutional
    layer that maps a 28 × 28 × 1 input, the single-channel grayscale digit image,
    to a 26 × 26 × 32 output using a filter with 32 3 × 3 kernels. Therefore, we know
    that the weights between the input image and output fit in an array that is 3
    × 3 × 1 × 32: 3 × 3 for the kernel size, 1 for the number of input channels, and
    32 for the number of kernels in the filter.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[图12-3](ch12.xhtml#ch12fig3)中的网络第一层是一个卷积层，它将28 × 28 × 1的输入（单通道灰度数字图像）映射到26
    × 26 × 32的输出，使用的是32个3 × 3的卷积核。因此，我们知道输入图像和输出之间的权重可以存储在一个3 × 3 × 1 × 32的数组中：3 ×
    3是卷积核的大小，1是输入通道数，32是卷积核的数量。'
- en: 'After training, what do the 32 3 × 3 kernels of the filter actually look like?
    We can extract them from the trained model and print them as a set of 32 3 × 3
    matrices. Here are the first two:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，卷积核的32个3 × 3的实际样子是什么样的？我们可以从训练好的模型中提取它们，并以一组32个3 × 3的矩阵形式打印出来。以下是前两个：
- en: '![image](Images/295equ01.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/295equ01.jpg)'
- en: This is nice, but not particularly helpful for building intuition about what
    the kernels do.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这很好，但对于理解卷积核的作用并没有特别大的帮助。
- en: We can also visualize the kernels of a filter by converting the matrices to
    images. To get the kernels as images, we first note that all the kernel values
    happen to fit in the range [*–*0.5,+0.5], so if we add 0.5 to each kernel value,
    we’ve mapped the range to [0,1]. After this, multiplication by 255 converts the
    kernel values to byte values, the same values a grayscale image uses. Additionally,
    a value of 0 is now 127, which is a middle gray value.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过将矩阵转换为图像来可视化卷积核。为了将卷积核转换为图像，我们首先注意到所有卷积核值都恰好适合在范围 [*–*0.5, +0.5] 之间，因此，如果我们给每个卷积核值加上
    0.5，就把范围映射到了 [0,1]。之后，乘以 255 将卷积核值转换为字节值，这与灰度图像使用的值相同。此外，值为 0 的地方现在变成了 127，这是一个中灰色值。
- en: After this conversion, the kernels can be shown as grayscale images, where negative
    kernel values are closer to black, and positive kernel values are closer to white.
    A final step is needed, however, because the mapped kernels are still only 3×3
    pixels. The last step is to upscale the 3 × 3 images to 64×64 pixels. We’ll upscale
    in two different ways. The first uses nearest-neighbor sampling to show the kernel
    in blocks. The second uses a Lanczos filter, which smooths the image, making it
    easier to see the orientation of the kernel. [Figure 12-4](ch12.xhtml#ch12fig4)
    shows the kernel images with the block versions on top and the smoothed versions
    on the bottom.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 转换后，卷积核可以作为灰度图像显示，其中负的卷积核值接近黑色，正的卷积核值接近白色。然而，还需要最后一步，因为映射后的卷积核仍然只有 3×3 像素。最后一步是将
    3 × 3 的图像放大到 64×64 像素。我们将以两种不同的方式进行放大。第一种方法使用最近邻插值显示卷积核的块状图像。第二种方法使用 Lanczos 滤波器，这可以平滑图像，使得卷积核的方向更加清晰。[图
    12-4](ch12.xhtml#ch12fig4)展示了卷积核图像，顶部是块状版本，底部是平滑版本。
- en: '![image](Images/12fig04.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/12fig04.jpg)'
- en: '*Figure 12-4: The 32 learned kernels of the first convolutional layer (top).
    Smoothed versions to show the orientations more clearly (bottom).*'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 12-4：第一卷积层的 32 个学习到的卷积核（上）。平滑版本清晰显示卷积核的方向（下）。*'
- en: These images represent the 32 kernels learned by the first convolutional layer
    of the model in [Figure 12-3](ch12.xhtml#ch12fig3). There is just enough detail
    in the images to hint that the kernels are selecting for structure in specific
    directions, just like the kernel that produced the image on the lower right of
    [Figure 12-2](ch12.xhtml#ch12fig2), which emphasized diagonal structures.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这些图像代表了模型第一卷积层学习到的 32 个卷积核，见 [图 12-3](ch12.xhtml#ch12fig3)。这些图像包含足够的细节，暗示着卷积核正在选择特定方向上的结构，就像
    [图 12-2](ch12.xhtml#ch12fig2) 右下角的卷积核生成的图像一样，强调了对角结构。
- en: Let’s turn our attention now to the effect of the kernels. What do the kernels
    do to an input MNIST image? We can run a sample MNIST image through the kernels
    by convolving each kernel with the sample, here a “3”, and following a process
    similar to the one that produced the preceding kernel images. The result is a
    set of 32 26 × 26 images, which we again upscale to 64 × 64 before displaying
    them. [Figure 12-5](ch12.xhtml#ch12fig5) shows the result.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来关注卷积核的效果。卷积核对输入的 MNIST 图像做了什么？我们可以通过将每个卷积核与示例图像（这里是一个“3”）进行卷积，并遵循类似于生成前面卷积核图像的过程，来对一个示例
    MNIST 图像进行处理。结果是一组 32 个 26 × 26 的图像，我们再次将它们放大到 64 × 64 后再显示出来。[图 12-5](ch12.xhtml#ch12fig5)展示了结果。
- en: '![image](Images/12fig05.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/12fig05.jpg)'
- en: '*Figure 12-5: The 32 kernels applied to a sample MNIST input*'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 12-5：应用于示例 MNIST 输入的 32 个卷积核*'
- en: The order of the kernels shown in [Figure 12-4](ch12.xhtml#ch12fig4) matches
    the images in [Figure 12-5](ch12.xhtml#ch12fig5). For example, the top-right image
    of [Figure 12-4](ch12.xhtml#ch12fig4) shows a kernel that is light on the upper
    left and dark on the lower right, meaning it will detect structures along the
    diagonal from lower left to upper right. The output from applying this kernel
    to the sample is the upper-right image of [Figure 12-5](ch12.xhtml#ch12fig5).
    We see that the kernel enhanced parts of the three that are primarily diagonal
    from the lower left to the upper right. Note, this example is easy to interpret
    because the input is a grayscale image with a single channel. This means that
    there is no summing of kernel outputs across channels as we previously saw for
    the more general operation.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 12-4](ch12.xhtml#ch12fig4)中显示的卷积核顺序与[图 12-5](ch12.xhtml#ch12fig5)中的图像相匹配。例如，[图
    12-4](ch12.xhtml#ch12fig4)右上角的图像显示了一个左上方亮、右下方暗的卷积核，意味着它将检测从左下到右上的对角线结构。将此卷积核应用于样本后的输出是[图
    12-5](ch12.xhtml#ch12fig5)中的右上角图像。我们看到，卷积核增强了那些主要从左下到右上的对角线部分。请注意，这个例子容易解释，因为输入是一个单通道的灰度图像。这意味着我们没有像之前更一般的操作那样跨通道求和卷积核输出。
- en: 'Typically, the first convolutional layer of a CNN learns kernels that select
    for specific orientations, textures, or, if the input image is RGB, colors. For
    the grayscale MNIST images, orientation is most important. The kernels learned
    at higher convolutional layers in the CNN are also selecting for things, but the
    interpretation of *what* the kernel is selecting becomes more and more abstract
    and difficult to understand. It is worth noting that the kernels learned by a
    CNN’s first convolutional layer are very similar to the first layer of visual
    processing in the mammalian brain. This is the primary visual cortex or V1 layer
    that detects lines and edges. Additionally, always keep in mind that the set of
    convolutional and pooling layers are there to learn a new feature representation:
    a new representation of the input image. This new representation does a better
    job of separating classes so that the fully connected layers can more easily distinguish
    between them.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，CNN的第一个卷积层学习的卷积核会选择特定的方向、纹理，或者如果输入图像是RGB的，则选择颜色。对于灰度MNIST图像，方向最为重要。CNN中更高层的卷积层学习的卷积核也在选择某些特征，但卷积核选择的*具体内容*变得越来越抽象，难以理解。值得注意的是，CNN第一个卷积层学习到的卷积核与哺乳动物大脑中视觉处理的第一层非常相似。这就是初级视觉皮层或V1层，负责检测线条和边缘。此外，请始终记住，卷积层和池化层的集合旨在学习一个新的特征表示：输入图像的全新表示。这种新表示能更好地分离不同类别，从而使得全连接层能够更容易地区分它们。
- en: Multiple Convolutional Layers
  id: totrans-125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多个卷积层
- en: Most CNNs have more than one convolutional layer. One reason for this is to
    build up features that are influenced by larger portions of the input as one goes
    deeper into the network. This introduces the ideas of *receptive field* and *effective
    receptive field*. The two concepts are similar and often confused. We can explain
    both by looking at [Figure 12-6](ch12.xhtml#ch12fig6).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数卷积神经网络（CNN）都有多个卷积层。其原因之一是随着网络的深入，能够构建出受输入更大部分影响的特征。这引出了*感受野*和*有效感受野*的概念。这两个概念相似且常常被混淆。我们可以通过查看[图
    12-6](ch12.xhtml#ch12fig6)来解释这两者。
- en: '![image](Images/12fig06.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/12fig06.jpg)'
- en: '*Figure 12-6: Receptive fields*'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 12-6：感受野*'
- en: The figure shows the *output* of two convolutional layers and the input to the
    model. We’re showing only the relevant parts of the output, using a 3 × 3 kernel.
    We’re also ignoring the depth of the filters since the receptive fields (defined
    next) are the same across the depth of the convolutional layer outputs.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 该图显示了两个卷积层的*输出*和模型的输入。我们只展示了输出的相关部分，使用了一个 3 × 3 的卷积核。我们也忽略了滤波器的深度，因为感受野（接下来定义）在卷积层的输出深度上是相同的。
- en: '[Figure 12-6](ch12.xhtml#ch12fig6) should be read right to left as the arrows
    indicate. This is the opposite direction to the flow of data through the network.
    Here, we are looking back to earlier layers to see what has influenced the output
    value at a higher layer. The squares are output values. The rightmost shaded square
    is one of the outputs of Conv[2]. This is our starting point for looking back
    to see what influences this value. The arrows point to the outputs of Conv[1]
    that influence the shaded value in Conv[2]. The value in Conv[2] then has a 3
    × 3 *receptive field* as it is directly influenced by the 3 × 3 shaded outputs
    of Conv[1]. This is how we’ll define *receptive field*: the set of outputs from
    the layer immediately before that directly influence the output of the current
    layer.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[图12-6](ch12.xhtml#ch12fig6)应按照箭头所示从右向左阅读。这是与数据流经网络的方向相反的。这里，我们回头看更早的层，查看是什么影响了更高层的输出值。方框是输出值。最右边的阴影方框是Conv[2]的输出之一。这是我们回溯以查看影响此值的因素的起点。箭头指向影响Conv[2]中阴影值的Conv[1]输出。然后，Conv[2]中的值有一个3
    × 3的*感受野*，因为它直接受Conv[1]中3 × 3阴影输出的影响。这就是我们定义*感受野*的方式：即直接影响当前层输出的前一层输出集合。'
- en: 'If we look at the set of input values that directly influence the 3 × 3 shaded
    region of Conv[1], we see a 5 × 5 region. This makes sense: each shaded output
    of Conv[1] has a receptive field that is a 3 × 3 region of the input. The receptive
    field is 3 × 3 because the kernels of Conv[1] are 3 × 3 kernels. They overlap
    so that the shaded 5 × 5 input region is what all the shaded Conv[1] outputs are
    influenced by.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看直接影响Conv[1]的3 × 3阴影区域的输入值集合，我们会看到一个5 × 5的区域。这是有道理的：Conv[1]的每个阴影输出都有一个3
    × 3的感受野。感受野为3 × 3，因为Conv[1]的卷积核是3 × 3的卷积核。它们重叠在一起，以便阴影的5 × 5输入区域就是所有阴影Conv[1]输出所受影响的区域。
- en: Look again at the rightmost shaded output value. If we trace back to the input
    all the values that can influence it, we see that the shaded 5 × 5 region of the
    input can affect its value. This region of the input is the *effective recep-
    tive field* for the rightmost shaded output of Conv[2]. This output value responds,
    ultimately, to what is happening in the input image in the leftmost shaded region.
    As the CNN gets deeper, with additional convolutional layers, we can see how the
    effective receptive field can change so that deeper convolutional layers are working
    with values ultimately derived from larger and larger portions of the input to
    the model.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 再次看看最右边的阴影输出值。如果我们追溯到输入，所有能够影响它的值，我们会发现输入中的这个阴影的5 × 5区域能够影响它的值。这个输入区域是Conv[2]最右边阴影输出的*有效感受野*。这个输出值最终响应的是输入图像中最左边阴影区域的内容。随着CNN的加深，增加了卷积层，我们可以看到有效感受野如何变化，以便更深的卷积层处理的值最终来自越来越大范围的输入数据。
- en: Initializing a Convolutional Layer
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 初始化卷积层
- en: In [Chapter 9](ch09.xhtml#ch09), we saw that the performance of a traditional
    neural network was strongly influenced by the type of random initialization used
    for the learned weights and biases. The same is true for CNNs. Recall that the
    weights of a convolutional layer are the values of the kernels. They are learned
    during backprop, just like the weights of a traditional neural network. We need
    an intelligent way to initialize these values when we set up the network. Fortunately,
    the best initialization approaches for a traditional neural network apply directly
    to convolutional layers as well. For example, Keras defaults to Glorot initialization,
    which, as we saw in [Chapter 9](ch09.xhtml#ch09), is sometimes called Xavier initialization
    in other toolkits.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第9章](ch09.xhtml#ch09)中，我们看到传统神经网络的性能受到所使用的随机初始化方法（用于学习的权重和偏置）的强烈影响。对于CNN来说，情况也是如此。回想一下，卷积层的权重就是卷积核的值。这些值在反向传播过程中被学习，就像传统神经网络的权重一样。当我们设置网络时，我们需要一种智能的方式来初始化这些值。幸运的是，传统神经网络的最佳初始化方法同样适用于卷积层。例如，Keras默认使用Glorot初始化，就像我们在[第9章](ch09.xhtml#ch09)中看到的那样，在其他工具包中有时称其为Xavier初始化。
- en: Let’s move on now from convolutional layers to pooling layers. These are simpler
    but perform an important, if somewhat controversial, function.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们从卷积层转到池化层。这些层较为简单，但它们执行着一个重要的、虽然有点有争议的功能。
- en: Pooling Layers
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 池化层
- en: Our favorite figure, [Figure 12-3](ch12.xhtml#ch12fig3), shows a pooling layer
    after the first two convolutional layers. This pooling layer takes an input stack
    of 24 × 24 × 64 and produces an output stack of 12 × 12 × 64\. The pooling part
    is marked as “2 × 2”. What’s going on here?
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最喜欢的图，[图12-3](ch12.xhtml#ch12fig3)，展示了在前两层卷积后有一个池化层。这个池化层将24 × 24 × 64的输入堆栈转换为12
    × 12 × 64的输出堆栈。池化部分标记为“2 × 2”。这里到底发生了什么？
- en: The key is the “2 × 2”. This means, for each of the 64 24 × 24 inputs, we move
    a 2 × 2 sliding window over the input and perform an operation similar to convolution.
    Not explicitly called out in [Figure 12-3](ch12.xhtml#ch12fig3) is that the stride
    is also 2 so that the sliding 2 × 2 window jumps by two to avoid overlapping itself.
    This is typically the case, but doesn’t need to be. Since the pooling operation
    is per input in the stack, the output leaves the stack size unchanged. This is
    contrary to what a convolutional layer often does.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 关键在于“2 × 2”。这意味着，对于每个64个24 × 24的输入，我们将一个2 × 2的滑动窗口移动在输入上，并执行类似卷积的操作。在[图12-3](ch12.xhtml#ch12fig3)中没有明确指出的是，步幅也是2，以便滑动的2
    × 2窗口每次移动两步，避免重叠。这通常是这样的，但不一定非得如此。由于池化操作是针对堆栈中的每个输入执行的，因此输出的堆栈大小保持不变。这与卷积层的做法通常不同。
- en: Let’s look at the pooling operation applied to a single input in the stack,
    a 24 × 24 matrix. [Figure 12-7](ch12.xhtml#ch12fig7) shows us what’s going on.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看池化操作如何应用于堆栈中的单个输入，一个24 × 24的矩阵。[图12-7](ch12.xhtml#ch12fig7)向我们展示了发生了什么。
- en: '![image](Images/12fig07.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/12fig07.jpg)'
- en: '*Figure 12-7: Applying 2 × 2 max pooling to an 8 × 8 input*'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '*图12-7：对8 × 8输入应用2 × 2最大池化*'
- en: The first 2 × 2 values are mapped to the first output value. Then we move over
    two and map the next 2 × 2 region to the output and so on until the entire input
    is mapped. The operation performed on each 2 × 2 region is up to the architect
    of the CNN. The most common operation is “select the largest value,” or *max pooling*,
    which is what we show in [Figure 12-7](ch12.xhtml#ch12fig7). This is also the
    operation the model in [Figure 12-3](ch12.xhtml#ch12fig3) is performing. Another
    fairly common pooling operation is to average the values.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个2 × 2的值映射到第一个输出值。然后我们向右移动两个单位，将下一个2 × 2的区域映射到输出，以此类推，直到整个输入都被映射。每个2 × 2区域上执行的操作由CNN的设计者决定。最常见的操作是“选择最大值”，即*最大池化*，这就是我们在[图12-7](ch12.xhtml#ch12fig7)中展示的操作。这也是[图12-3](ch12.xhtml#ch12fig3)中的模型所执行的操作。另一个相当常见的池化操作是对值进行平均。
- en: We can see from [Figure 12-7](ch12.xhtml#ch12fig7) that the 8 × 8 input matrix
    is mapped to a 4 × 4 output matrix. This explains why the output of the pooling
    layer in [Figure 12-3](ch12.xhtml#ch12fig3) is 12 × 12; each dimension is half
    the size of the input.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从[图12-7](ch12.xhtml#ch12fig7)中可以看到，8 × 8的输入矩阵被映射为4 × 4的输出矩阵。这就解释了为什么[图12-3](ch12.xhtml#ch12fig3)中池化层的输出是12
    × 12；每个维度的大小是输入的一半。
- en: The pooling operation is straightforward but throws information away. So why
    do it at all? The primary motivation for pooling is to reduce the number of values
    in the network. Typically, as depth increases, the number of filters used by convolutional
    layers increases, by design. We see this for even the simple network of [Figure
    12-3](ch12.xhtml#ch12fig3), where the first convolutional layer has 32 filters,
    while the second has 64\. Therefore, the second convolutional layer outputs 24
    × 24 × 64 = 36,864 values, but after 2 × 2 pooling, there are only 12 × 12 × 64
    = 9,216 values to work with, a 75 percent reduction. It’s important to note that
    we’re talking about the number of values present as we move data through the network,
    not the number of learned parameters in the layers. The second convolutional layer
    in [Figure 12-3](ch12.xhtml#ch12fig3) has 3 × 3 × 32 × 64 = 18,432 learned parameters
    (ignoring bias values), while the pooling layer has no learned parameters.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 池化操作简单，但会丢失信息。那么为什么要进行池化呢？池化的主要动机是减少网络中的值的数量。通常，随着深度的增加，卷积层使用的滤波器数量会增加，这是设计上的要求。即使是[图12-3](ch12.xhtml#ch12fig3)中的简单网络，我们也能看到这一点，其中第一层卷积有32个滤波器，而第二层有64个滤波器。因此，第二层卷积输出的值是24
    × 24 × 64 = 36,864个，但经过2 × 2池化后，剩下的值只有12 × 12 × 64 = 9,216个，减少了75%。需要注意的是，我们谈论的是数据通过网络时存在的值的数量，而不是层中学习的参数数量。[图12-3](ch12.xhtml#ch12fig3)中的第二层卷积有3
    × 3 × 32 × 64 = 18,432个学习参数（忽略偏置值），而池化层没有学习参数。
- en: This reduction in the number of values in the output, which is our representation
    of the input, speeds up computation and acts as a regularizer to guard against
    overfitting. The regularization techniques and rationales of [Chapter 9](ch09.xhtml#ch09)
    are equally valid for CNNs. However, since pooling throws information away and
    selects proxies to represent entire regions of the representation (the convolutional
    layer outputs), it alters the spatial relationship between parts of the input.
    This loss of spatial relationships might be critical for some applications and
    has motivated people like Geoffrey Hinton to eliminate pooling by introducing
    other types of networks (search for “capsule networks”).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这种输出值数量的减少，即我们对输入的表示，能够加速计算，并且作为正则化手段，帮助防止过拟合。[第9章](ch09.xhtml#ch09)中的正则化技术和理论同样适用于CNN。然而，由于池化会丢弃信息，并选择代理来表示整个区域（卷积层的输出），它改变了输入部分之间的空间关系。空间关系的丧失可能对某些应用至关重要，这也促使像Geoffrey
    Hinton这样的人通过引入其他类型的网络来消除池化（可以搜索“胶囊网络”）。
- en: 'Specifically, Hinton said the following regarding pooling layers in response
    to a question on Reddit asking for his most controversial opinion on machine learning:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，Hinton在Reddit上回应关于池化层的提问时表示了以下观点，这是针对一个询问他关于机器学习最具争议观点的问题的回答：
- en: The pooling operation used in convolutional neural networks is a big mistake
    and the fact that it works so well is a disaster. If the pools do not overlap,
    pooling loses valuable information about where things are. We need this information
    to detect precise relationships between the parts of an object.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络中使用的池化操作是一个大错误，尽管它效果显著，但这实际上是灾难性的。如果池化区域没有重叠，池化就会丢失关于物体位置的宝贵信息。我们需要这些信息来检测物体各部分之间的精确关系。
- en: He elaborates further in the answer, pointing out that allowing pooling operations
    to overlap does preserve some of the spatial relationships in a crude way. An
    overlapping pooling operation might be to use a 2 × 2 window as we used in [Figure
    12-7](ch12.xhtml#ch12fig7), but use a stride of 1 instead of 2.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 他在回答中进一步阐述道，允许池化操作重叠在粗略程度上确实可以保留一些空间关系。一个重叠的池化操作可能是使用2 × 2窗口，就像我们在[图12-7](ch12.xhtml#ch12fig7)中使用的那样，但步长设为1而非2。
- en: Concerns aside, pooling layers are an essential part of CNNs as presently implemented,
    but be careful when adding them to a model. Let’s move on now to the top layers
    of a CNN, the fully connected layers.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 摆脱忧虑，池化层是目前卷积神经网络（CNN）中不可或缺的一部分，但在将它们添加到模型时要小心。现在我们来讲解CNN的顶部层，即全连接层。
- en: Fully Connected Layers
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 全连接层
- en: In the second row of [Figure 12-3](ch12.xhtml#ch12fig3), all the layers starting
    with *Flatten* form the fully connected layer of the model. The figure uses Keras
    terminology; many people call the *Dense* layer the fully connected layer and
    assume there is a Flatten operation as part of it along with the activation (ReLU)
    and optional dropout before the softmax layer. Therefore, the model in [Figure
    12-3](ch12.xhtml#ch12fig3) has only one fully connected layer.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图12-3](ch12.xhtml#ch12fig3)的第二行中，从*Flatten*开始的所有层组成了模型的全连接层。该图使用了Keras术语；许多人将*Dense*层称为全连接层，并假设它包含Flatten操作、激活函数（ReLU）以及在softmax层之前的可选dropout。因此，[图12-3](ch12.xhtml#ch12fig3)中的模型只有一个全连接层。
- en: 'We previously stated that the net effect of the convolutional and pooling layers
    is to change the representation of the input feature (the image, say) into one
    that makes it easier for a model to reason about. During training, we are asking
    the network to learn a different, often more compact, representation of the input
    to help the model perform better on unseen inputs. For the model in [Figure 12-3](ch12.xhtml#ch12fig3),
    all the layers up to and including the pooling layer (and the dropout layer after
    it for training) are there to learn a new representation of the input image. In
    this case, the fully connected layer is the model: it will take that new representation
    and ultimately make a classification based on it.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到过，卷积层和池化层的总体作用是将输入特征（例如图像）的表示转换成一种更便于模型推理的形式。在训练过程中，我们要求网络学习输入的另一种、更紧凑的表示，以帮助模型在面对未知输入时表现更好。对于[图12-3](ch12.xhtml#ch12fig3)中的模型，所有层次，直到包括池化层（以及池化层后的dropout层用于训练），都在学习输入图像的新表示。在这种情况下，全连接层即是模型：它将利用这个新表示，最终做出基于该表示的分类。
- en: 'Fully connected layers are just that, fully connected. The weights between
    the flattened final pooling layer of 9,216 elements for [Figure 12-3](ch12.xhtml#ch12fig3)
    (12 × 12 × 64 = 9,216) and the Dense layer of 128 elements are the same as if
    we were building a traditional neural network. This means that there are 9,216
    × 128 = 1,179,648 weights plus an additional 128 bias values that need to be learned
    during training. Therefore, of the 1,199,882 parameters (weights and biases) in
    the model of [Figure 12-3](ch12.xhtml#ch12fig3), 98.3 percent of them are in the
    transition between the final pooling layer and the fully connected layer. This
    illustrates an important point: fully connected layers are *expensive* in terms
    of parameters that need to be learned, just as they are for traditional neural
    networks. Ideally, if the feature learning layers, the convolutional and pooling
    layers, are doing their job well, we might expect to need only one or two fully
    connected layers.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 完全连接层顾名思义，就是完全连接的。[图 12-3](ch12.xhtml#ch12fig3)中，来自最终池化层（9,216个元素）的扁平化权重与128个元素的Dense层之间的连接，就像我们在构建传统神经网络时一样。这意味着有9,216
    × 128 = 1,179,648个权重，以及额外的128个偏置值需要在训练过程中学习。因此，在[图 12-3](ch12.xhtml#ch12fig3)模型中的1,199,882个参数（权重和偏置）中，98.3%的参数都存在于最终池化层和完全连接层之间的过渡部分。这说明了一个重要的观点：完全连接层在需要学习的参数方面是*昂贵的*，就像传统神经网络一样。理想情况下，如果特征学习层（卷积层和池化层）做得很好，我们可能只需要一两个完全连接层。
- en: 'Fully connected layers have another disadvantage, besides memory use, that
    can impact their utility. To see what this disadvantage is, consider the following
    scenario: you want to be able to locate digits in grayscale images. Assume for
    simplicity that the background is black. If you use the model of [Figure 12-3](ch12.xhtml#ch12fig3)
    trained on MNIST digits, you will have a model that is very good at identifying
    digits centered in 28×28 pixel images, but what if the input images are large
    and you do not know where the digits are in the image, let alone how many digits
    there are? Then things get a little more interesting. The model of [Figure 12-3](ch12.xhtml#ch12fig3)
    expects input images that are 28×28 pixels in size and only that size. In [Chapter
    13](ch13.xhtml#ch13), we will work through this problem in detail as an experiment,
    but for now, let’s discuss fully convolutional layers, a possible solution to
    this disadvantage of using fully connected layers in CNNs.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 除了内存使用外，完全连接层还有一个缺点，这会影响它们的实用性。为了理解这个缺点是什么，考虑以下场景：你希望能够在灰度图像中定位数字。假设为了简化，背景是黑色的。如果你使用的是在
    MNIST 数字数据集上训练的[图 12-3](ch12.xhtml#ch12fig3)模型，那么你会得到一个非常擅长识别居中在28×28像素图像中的数字的模型，但如果输入的图像很大，而且你不知道图像中数字的位置，更别提有多少个数字了呢？那么事情就变得有趣了。[图
    12-3](ch12.xhtml#ch12fig3)模型期望输入图像的大小为28×28像素，且仅限这种大小。在[第13章](ch13.xhtml#ch13)中，我们将详细探讨这个问题并进行实验，但现在，让我们讨论完全卷积层，这是一种可能解决在
    CNN 中使用完全连接层所带来的缺点的方法。
- en: Fully Convolutional Layers
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 全卷积层
- en: In the last section, I said that the model of [Figure 12-3](ch12.xhtml#ch12fig3)
    expects input images that are 28×28 pixels in size and only that size. Let’s see
    why.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我提到[图 12-3](ch12.xhtml#ch12fig3)模型期望输入图像的大小为28×28像素，并且仅限这种大小。现在让我们看看为什么。
- en: There are many kinds of layers in this model. Some, like the ReLU and dropout
    layers, have no impact on the dimensionality of the data flowing through the network.
    The same cannot be said of the convolutional, pooling, and fully connected layers.
    Let’s look at these layers one by one to see how they are tied to the dimensionality
    of the input image.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型中有许多种不同的层。有些层，如ReLU和dropout层，对流经网络的数据的维度没有影响。而卷积层、池化层和完全连接层则不能这么说。让我们逐一看一下这些层，看看它们是如何与输入图像的维度相关联的。
- en: The convolutional layers implement convolutions. By definition, a convolution
    involves moving a fixed-size kernel over some input image (thinking purely 2D
    here). Nothing in that operation specifies the size of the input image. The output
    of the first convolutional layer in [Figure 12-3](ch12.xhtml#ch12fig3) is 26 ×
    26 × 32\. The 32 comes from the number of filters selected by the architecture.
    The 26 × 26 comes from using a 3 × 3 convolution kernel on a 28 × 28 input with
    no padding. If the input image were instead 64×64 pixels, the output of this layer
    would be 62 × 62 × 32, and we wouldn’t need to do anything to alter the architecture
    of the network. The convolutional layers of a CNN are agnostic to the spatial
    dimensions of their inputs.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层实现了卷积操作。根据定义，卷积操作涉及将一个固定大小的卷积核在输入图像上滑动（这里只考虑二维情况）。这个操作中没有任何部分指定输入图像的大小。[图12-3](ch12.xhtml#ch12fig3)中第一个卷积层的输出是26
    × 26 × 32。32来自架构选择的过滤器数量。26 × 26则来自使用一个3 × 3卷积核对28 × 28的输入图像进行卷积，没有填充。如果输入图像是64×64像素，则该层的输出将是62
    × 62 × 32，且不需要改变网络架构。CNN的卷积层对其输入的空间维度是无关的。
- en: 'The pooling layer in [Figure 12-3](ch12.xhtml#ch12fig3) takes a 24 × 24 × 64
    input and produces a 12 × 12 × 64 output. As we previously saw, the pooling operation
    is much like the convolution operation: it slides a fixed size window over the
    input, spatially, and produces an output; in this case, the output is half the
    dimensionality of the input while leaving the depth the same. Again, nothing in
    this operation fixes the spatial dimensions of the input stack. If the input stack
    were 32 × 32 × 64, the output of this max pooling operation would be 16 × 16 ×
    64 without a change needed to the architecture.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '[图12-3](ch12.xhtml#ch12fig3)中的池化层接收一个24 × 24 × 64的输入并产生一个12 × 12 × 64的输出。正如我们之前所看到的，池化操作很像卷积操作：它在空间上滑动一个固定大小的窗口，并产生一个输出；在这种情况下，输出的空间维度是输入的一半，而深度保持不变。同样，这个操作没有固定输入堆栈的空间维度。如果输入堆栈是32
    × 32 × 64，那么这个最大池化操作的输出将是16 × 16 × 64，而不需要改变架构。'
- en: Finally, we have the fully connected layer that maps the 12 × 12 × 64 = 9,216
    pooling output to a 128 element fully connected (Dense) layer. As we saw in [Chapter
    8](ch08.xhtml#ch08), fully connected neural networks use matrices of weights between
    layers in their implementation. There are 9,216 elements in the output of the
    pooling layer and a fixed 128 in the dense layer, so we need a matrix that is
    9,216 × 128 elements. This size *is* fixed. If we use the network with a larger,
    say 32 × 32, input image, by the time we get through the pooling layer, the output
    size will be 14 × 14 × 64 = 12,544, which would require an existing 12,544 × 128
    weight matrix to map to the fully connected layer. Of course, this won’t work;
    we trained a network that uses a 9,216 × 128 matrix. The fully connected layers
    of a CNN fix the input size of the CNN. If we could get around this, we could
    apply inputs of any size to the CNN, assuming memory allows.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有一个全连接层，它将12 × 12 × 64 = 9,216的池化输出映射到一个128元素的全连接（Dense）层。正如我们在[第8章](ch08.xhtml#ch08)中看到的，全连接神经网络在实现中使用层间的权重矩阵。池化层的输出有9,216个元素，而全连接层的固定大小为128，因此我们需要一个9,216
    × 128的矩阵。这个大小*是*固定的。如果我们使用更大尺寸的输入图像，比如32 × 32，那么在池化层处理完后，输出大小将是14 × 14 × 64 =
    12,544，这就需要一个现有的12,544 × 128的权重矩阵来映射到全连接层。当然，这样是行不通的；我们训练的是一个使用9,216 × 128矩阵的网络。CNN的全连接层固定了CNN的输入大小。如果我们能绕过这个限制，我们就能将任何大小的输入应用于CNN，只要内存允许。
- en: We could, naïvely, simply slide a 28 × 28 window over the larger input image,
    run each 28×28 pixel image through the model as we trained it, and output a larger
    map, where each pixel now has a probability of that digit being present. There
    are 10 digits, so we would have 10 output maps. This sliding window approach certainly
    works, but it’s very computationally expensive, as many simplistic implementations
    of algorithms often are.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以简单地将一个28 × 28的窗口在更大的输入图像上滑动，按照我们训练模型的方式，逐个将每个28×28像素的图像传入模型，并输出一个更大的图像，其中每个像素现在都有一个数字出现的概率。由于有10个数字，我们将得到10个输出图。这个滑动窗口方法当然可行，但它非常计算密集，因为许多简单的算法实现通常都是如此。
- en: Fortunately for us, we can do better by converting the fully connected layer
    into an equivalent convolutional layer to make the model a *fully convolutional
    network*. In a fully convolutional network, there are no fully connected layers,
    and we’re not restricted to using a fixed input size. The relationship between
    input size and the output of the network when it is fully convolutional is something
    we will see in [Chapter 13](ch13.xhtml#ch13), but the essential operation is to
    look at the size of the last standard convolutional or pooling layer and replace
    the fully connected layer that follows with a convolutional layer using a kernel
    of the same size.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，通过将全连接层转换为等效的卷积层，我们可以做得更好，从而使模型成为*全卷积网络*。在全卷积网络中，没有全连接层，我们也不受限于使用固定的输入大小。输入大小和网络输出之间的关系将在[第13章](ch13.xhtml#ch13)中讨论，但本质操作是查看最后一个标准卷积或池化层的大小，并用相同大小的卷积核替换后续的全连接层。
- en: In [Figure 12-3](ch12.xhtml#ch12fig3), the output of the pooling layer is 12
    × 12 × 64\. Therefore, instead of the 128-element fully connected layer that we
    saw fixes our input size, we can mathematically get the same calculation by changing
    the fully connected layer into a 12 × 12 × 128 convolutional layer. Convolving
    a 12 × 12 kernel over a 12 × 12 input produces a single number. Therefore, the
    output of the 12 × 12 × 128 convolutional layer will be a 1 × 1 × 128 array, which
    is functionally the same as the 128 outputs of the fully connected layer that
    we originally used. Additionally, the convolution operation between a 12 × 12
    kernel and a 12 × 12 input is to simply multiply the kernel values by the input
    values, element by element, and sum them. This is what a fully connected layer
    does for each of its nodes.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图12-3](ch12.xhtml#ch12fig3)中，池化层的输出是12 × 12 × 64。因此，我们可以通过将全连接层更改为12 × 12
    × 128的卷积层，数学上得到与之前一样的计算。对12 × 12的卷积核与12 × 12的输入进行卷积，会生成一个单一的数字。因此，12 × 12 × 128卷积层的输出将是一个1
    × 1 × 128的数组，其功能上与我们原本使用的128个全连接层输出相同。此外，12 × 12的卷积核与12 × 12输入之间的卷积操作，就是将卷积核的值逐元素地与输入值相乘并求和。这正是全连接层为每个节点所做的操作。
- en: We do not save anything in terms of the number of parameters when using a convolutional
    layer this way. We can see this from [Figure 12-3](ch12.xhtml#ch12fig3). The 9,216
    elements of the pooling layer output times the 128 nodes of the fully connected
    layer means we have 9,216 × 128 = 1,179,648 weights + 128 bias terms needed for
    both the fully connected and fully convolutional layers. When moving to the 12
    × 12 × 128 convolutional layer, we have 12 × 12 × 64 × 128 = 1,179,648 weights
    to learn, the same as before. However, now we also have the freedom to change
    the input size, as the 12 × 12 × 128 convolutional layer will automatically convolve
    over any larger input, giving us outputs that represent the application of the
    network to 28 × 28 regions of the input with a stride determined by the specific
    architecture of the network.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种方式的卷积层并不会在参数数量上节省任何东西。我们可以从[图12-3](ch12.xhtml#ch12fig3)中看到这一点。池化层输出的9,216个元素与全连接层的128个节点相乘，意味着我们需要9,216
    × 128 = 1,179,648个权重和128个偏置项，这对于全连接层和全卷积层都是一样的。当我们转到12 × 12 × 128的卷积层时，我们需要学习的权重数为12
    × 12 × 64 × 128 = 1,179,648，与之前一样。然而，现在我们也有了更改输入大小的自由，因为12 × 12 × 128的卷积层将自动对任何更大的输入进行卷积，输出将表示网络对输入的28
    × 28区域应用，步幅由网络的具体架构决定。
- en: 'Fully convolutional networks stem from the 2014 paper by Long, Shelhamer, and
    Darrell, “Fully Convolutional Networks for Semantic Segmentation,” which has been
    referenced over 19,000 times as of this writing. The phrase *semantic segmentation*
    refers to assigning a class label to each pixel of the input image. Currently,
    the go-to architecture for semantic segmentation is the U-Net (see “U-Net: Convolutional
    Networks for Biomedical Image Segmentation” by Ronneberger, Fischer, and Brox,
    2015) which has seen widespread success, especially in medical domains.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '全卷积网络源自2014年Long、Shelhamer和Darrell的论文《全卷积网络用于语义分割》（Fully Convolutional Networks
    for Semantic Segmentation），截至撰写时该论文已被引用超过19,000次。*语义分割*一词指的是为输入图像的每个像素分配一个类别标签。目前，语义分割的主流架构是U-Net（参见Ronneberger、Fischer和Brox的2015年论文《U-Net:
    用于生物医学图像分割的卷积网络》），该架构在医学领域尤其取得了广泛的成功。'
- en: We’ve discussed the primary CNN layers, those found in [Figure 12-3](ch12.xhtml#ch12fig3).
    There are many more that we could cover, but they are generally beyond what we
    want to present at this level, with one exception, batch normalization, which
    we’ll experiment with in [Chapter 15](ch15.xhtml#ch15). New layer types are being
    added all the time in response to active research projects. However, in the end,
    the core includes the layers we have discussed in this chapter. Let’s move on
    now and see how a trained CNN processes unknown inputs.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了主要的CNN层，这些层见于[图 12-3](ch12.xhtml#ch12fig3)。虽然还有许多其他层我们可以介绍，但它们通常超出了我们在此阶段的讨论范围，唯一的例外是批量归一化，我们将在[第15章](ch15.xhtml#ch15)中进行实验。新的层类型正在不断增加，以应对活跃的研究项目。然而，最终，核心层还是我们在本章中讨论的这些层。接下来，我们将看到一个训练好的CNN如何处理未知的输入。
- en: Step by Step
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 步骤详解
- en: In the previous sections, we discussed the architecture and layers of our sample
    CNN, [Figure 12-3](ch12.xhtml#ch12fig3). In this section, we will illustrate the
    operation of the network to see how it responds to two new inputs, one a “4” and
    the other a “6”. We assume the network is fully trained; we’ll train for real
    it in [Chapter 13](ch13.xhtml#ch13).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们讨论了我们的样本CNN的架构和层，[图 12-3](ch12.xhtml#ch12fig3)中展示了这些内容。在本节中，我们将展示网络的操作，看看它如何对两个新输入作出响应，一个是“4”，另一个是“6”。我们假设网络已经完全训练好；我们将在[第13章](ch13.xhtml#ch13)中进行真实训练。
- en: The input image is passed through the model layer by layer
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 输入图像通过模型一层一层地传递
- en: input → conv[0] → conv[1] → pool → dense → softmax
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 输入 → 卷积[0] → 卷积[1] → 池化 → 全连接 → softmax
- en: using the trained weights and biases to calculate outputs for each layer. We
    will refer to these as the *activations*. The output of the first convolutional
    layer is a stack of 32 26 × 26 images, the response of the input image to each
    of the 32 kernels. This stack then passes to the second convolutional layer to
    produce 64 24 × 24 outputs. Note, between the two convolutional layers is a ReLU
    operation that clips the output so that anything that would have been negative
    is now 0\. Doing this adds a nonlinearity to the data as it flows through the
    network. Without this nonlinearity, the net effect of the two convolutional layers
    is to act like a single convolutional layer. With the nonlinearity imposed by
    the ReLU, we enable the two convolutional layers to learn different things about
    the data.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 使用训练好的权重和偏置来计算每一层的输出。我们称这些为*激活值*。第一个卷积层的输出是32个26 × 26的图像堆叠，表示输入图像对每个32个卷积核的响应。这个堆叠接着传递到第二个卷积层，生成64个24
    × 24的输出。请注意，在两个卷积层之间有一个ReLU操作，它会将输出中任何本应为负的值剪裁为0。这样做为数据在网络中流动时增加了非线性。如果没有这个非线性，两个卷积层的综合效果就相当于一个单一的卷积层。通过ReLU施加的非线性，我们使得两个卷积层能够学习到数据中的不同特征。
- en: The second ReLU operation makes the stack of 64 24 × 24 outputs 0 or positive.
    Next, a 2 × 2 max pooling operation reduces the 64 outputs to 12 × 12 in size.
    After this, a standard fully connected layer produces 128 output values as a vector
    from the 9,216 values in the stack of 12 × 12 activations. From this, a set of
    10 outputs, one for each digit, is calculated via a softmax. These are the output
    values of the network representing the network’s confidence as to which class
    label should be assigned to the input image.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 第二次ReLU操作使得64个24 × 24输出要么为0，要么为正数。接着，2 × 2的最大池化操作将64个输出的大小减少到12 × 12。之后，一个标准的全连接层将从12
    × 12激活堆叠中的9,216个值中生成128个输出值作为向量。从中，通过softmax计算得到10个输出值，每个数字对应一个输出。这些是网络的输出值，表示网络对应该为输入图像分配哪个类别标签的信心。
- en: 'We can illustrate the activations by displaying the output images: either 26
    × 26 for the first convolutional layer, 24 × 24 for the second convolutional layer,
    or 12 × 12 for the pooling layer. To show the activations from the fully connected
    layer, we can make an image of 128 bars, where the intensity of each bar represents
    the vector value. [Figure 12-8](ch12.xhtml#ch12fig8) shows the activations for
    our two sample digits.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过显示输出图像来说明激活情况：第一个卷积层为26 × 26，第二个卷积层为24 × 24，池化层为12 × 12。为了展示全连接层的激活情况，我们可以制作128条条形图，其中每条条形的强度代表向量值。[图
    12-8](ch12.xhtml#ch12fig8)展示了我们两个样本数字的激活情况。
- en: '![image](Images/12fig08.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/12fig08.jpg)'
- en: '*Figure 12-8: Model activations per layer. The output is inverted: darker implies
    stronger activation.*'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 12-8：每一层的模型激活情况。输出是倒置的：越深的颜色意味着越强的激活。*'
- en: Note that the images are inverted so that darker corresponds to stronger activation
    values. We are not showing the softmax outputs. These values are
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这些图像已经被反转，因此较暗的部分对应于更强的激活值。我们没有展示softmax的输出。这些值是
- en: '|  | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '|  | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 4 | 0.00 | 0.00 | 0.00 | 0.00 | 0.99 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 0.00 | 0.00 | 0.00 | 0.00 | 0.99 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |'
- en: '| 6 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.99 | 0.00 | 0.00 | 0.00 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.99 | 0.00 | 0.00 | 0.00 |'
- en: indicating that in both cases, the model is very confident of the class label
    that should be assigned and that it was, in fact, correct.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明，在这两种情况下，模型都非常有信心应该分配给哪个类别标签，并且它实际上是正确的。
- en: Looking back at [Figure 12-8](ch12.xhtml#ch12fig8), we see that the output of
    the first convolutional layer is simply the response of the single input image
    (grayscale) and the kernels of the layer. This hearkens back to [Figure 12-2](ch12.xhtml#ch12fig2),
    where we saw that convolution could be used to highlight aspects of the input
    image. After the ReLU operation, the responses of the 64 filters of the second
    convolutional layer, each a stack of 32 kernels, seems to be picking out different
    portions or strokes in the input images. These can be thought of as a set of smaller
    components from which the input is constructed. The second ReLU and pooling operation
    preserve much of the structure of the second convolutional layer outputs, but
    reduce the size to one quarter what it was previously. Finally, the output of
    the fully connected layer shows the pattern derived from the input image, the
    new representation that we expect to be easier to classify than the raw image
    input.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾[图12-8](ch12.xhtml#ch12fig8)，我们可以看到第一个卷积层的输出实际上是单张输入图像（灰度图）与该层卷积核的响应。这让我们想起了[图12-2](ch12.xhtml#ch12fig2)，在其中我们看到卷积可以用来突出输入图像的不同特征。在ReLU操作之后，第二个卷积层的64个滤波器的响应，每个滤波器都是32个卷积核的堆叠，似乎在从输入图像中选出不同的部分或笔画。这些可以被看作是构成输入图像的较小组件。第二次ReLU操作和池化操作保持了第二个卷积层输出的大部分结构，但将其大小缩小到之前的四分之一。最后，全连接层的输出显示了从输入图像中提取出的模式，这一新的表示形式预计比原始输入图像更容易分类。
- en: 'The dense layer outputs of [Figure 12-8](ch12.xhtml#ch12fig8) are different
    from each other. This begs the question: what do these outputs look like for several
    instances of four and six digits? Is there something in common that we can see,
    even in these values? We might expect that there is because we know this network
    has been trained and has achieved a very high accuracy of over 99 percent on the
    test set. Let’s take a look at running ten “4” and ten “6” images from the test
    set through the network and compare the dense layer activations. This gives us
    [Figure 12-9](ch12.xhtml#ch12fig9).'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '[图12-8](ch12.xhtml#ch12fig8)中的全连接层输出彼此不同。这引出了一个问题：对于多个“四”和“六”的实例，这些输出是什么样子的？我们是否可以看到一些共同点，即使是在这些值中？我们可能会预期会有所发现，因为我们知道这个网络已经经过训练，并且在测试集上的准确率已经超过了99%。让我们来看看通过网络处理十张“四”和十张“六”图像的情况，并比较全连接层的激活情况。这给出了[图12-9](ch12.xhtml#ch12fig9)。'
- en: '![image](Images/12fig09.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/12fig09.jpg)'
- en: '*Figure 12-9: Dense layer activations for ten instances of 4 and 6\. The output
    is inverted: darker implies stronger activation.*'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '*图12-9：四和六的十个实例的全连接层激活。输出被反转：较暗表示更强的激活。*'
- en: On the left, we see the actual input to the model. On the right is the representation
    of the 128 outputs in the fully connected layer, the one that feeds into the softmax.
    Each digit has a particular pattern that is common to each one of the digits.
    However, there are also variations. The middle “4” has a very short stem, and
    we see that its representation in the fully connected layer is also different
    from all the other examples. Still, this digit was successfully called a “4” by
    the model with a certainty of 0.999936.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧是模型的实际输入。右侧是全连接层128个输出的表示，这些输出最终输入到softmax层。每个数字都有一个特定的模式，该模式在每个数字中都是共同的。然而，也存在一些变化。中间的“四”有一个非常短的笔画，我们可以看到它在全连接层中的表示也与其他所有例子不同。尽管如此，模型依然成功地将其识别为“四”，且置信度为0.999936。
- en: '[Figure 12-9](ch12.xhtml#ch12fig9) provides evidence that the model learned
    what we wanted it to learn in terms of representation of the input. The softmax
    layer maps the 128 elements of the dense layer to 10, the output nodes from which
    the softmax probabilities are calculated. This is, in effect, a simple traditional
    neural network with no hidden layers. This simpler model succeeds in correctly
    labeling the images because the new representation of the inputs does a much better
    job of separating the classes so that even a simple model can make solid predictions.
    It also succeeds because the training process jointly optimizes both the weights
    of this top layer model and the weights of the lower layers that generate the
    input to the model at the same time, so they reinforce each other. Sometimes you
    will see this referred to in the literature at *end-to-end* training.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '[图12-9](ch12.xhtml#ch12fig9)提供了证据，证明模型学到了我们希望它学习的输入表示。Softmax层将密集层的128个元素映射到10个输出节点，进而计算softmax概率。这实际上是一个没有隐藏层的简单传统神经网络。这个简单的模型能够正确标注图像，是因为输入的新表示在分离各类方面做得更好，甚至一个简单的模型也能做出可靠的预测。它成功的另一个原因是，训练过程同时优化了这个顶层模型的权重和生成输入到模型的低层权重，因此它们相互强化。有时，文献中会将这种方法称为*端到端*训练。'
- en: We can demonstrate the claim that the features are better separated by looking
    at a plot of the dense layer activations for the MNIST test data. Of course, we
    can’t look at the actual plot, as I have no idea how to visualize a plot in 128
    dimensions, but all is not lost. The machine learning community has created a
    powerful visualization tool called *t-SNE*, which, fortunately for us, is part
    of sklearn. This algorithm intelligently maps high-dimensional spaces to lower-dimensional
    spaces, including 2D. If we run a thousand randomly selected MNIST test images
    through the model and then run the resulting 128-dimension dense layer activations
    through t-SNE, we can produce a 2D plot where the separation between classes reflects
    the actual separation in the 128-dimensional space. [Figure 12-10](ch12.xhtml#ch12fig10)
    is the result.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过查看MNIST测试数据的密集层激活图来证明特征分离更好的这一说法。当然，我们无法看到实际的图形，因为我不知道如何可视化一个128维的图形，但并非全然无望。机器学习社区创造了一个强大的可视化工具，叫做*t-SNE*，幸运的是它是sklearn的一部分。这个算法智能地将高维空间映射到低维空间，包括2D。如果我们将1000个随机选择的MNIST测试图像通过模型，然后将生成的128维密集层激活通过t-SNE处理，就可以生成一个2D图形，在该图形中，类与类之间的分离反映了128维空间中的实际分离。[图12-10](ch12.xhtml#ch12fig10)即为结果。
- en: '![image](Images/12fig10.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/12fig10.jpg)'
- en: '*Figure 12-10: How well the model separates test samples by class (t-SNE plot)*'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '*图12-10：模型如何按类别分离测试样本（t-SNE图）*'
- en: In this plot, each class uses a different plot symbol. If the model did not
    correctly classify the sample, it is shown as a larger star. In this case, only
    a handful of samples were misclassified. The separation by class type is very
    evident; the model has learned a representation that makes it straightforward
    to decide on the correct class label in most cases. We can readily count 10 different
    blobs in the t-SNE plot.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个图中，每个类别使用不同的图形符号。如果模型未能正确分类样本，则以更大的星号表示。在这种情况下，只有少数样本被错误分类。类别之间的分离非常明显；模型已经学到了一个表示方式，使得在大多数情况下，正确的类别标签容易判断。我们可以在t-SNE图中清楚地数出10个不同的簇。
- en: Summary
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we introduced the major components of convolutional neural
    networks. These are workhorse networks for modern deep learning, especially for
    vision tasks because of their ability to learn from spatial relationships. We
    worked through a model to classify MNIST digits and detailed new processing layers,
    including convolutional layers and pooling layers. We then learned that the fully
    connected layers of a CNN are analogs of the traditional neural networks we learned
    about in earlier chapters.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了卷积神经网络的主要组成部分。这些是现代深度学习的主力网络，尤其是在视觉任务中，因为它们能够从空间关系中学习。我们通过了一个模型来分类MNIST数字，并详细介绍了新的处理层，包括卷积层和池化层。接着，我们了解到CNN的全连接层是我们在前几章中学习的传统神经网络的类比。
- en: Next, we saw how to modify the fully connected layers to enable operation on
    larger inputs. Finally, we looked at the activations generated by the network
    when a sample image was passed through and saw how the convolution and pooling
    layers worked together to produce a new representation of the input, one that
    helped to separate the classes in the feature space, thereby enabling high accuracy.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们看到了如何修改全连接层以便能够处理更大的输入。最后，我们观察了当一张样本图像通过网络时，网络生成的激活函数，并看到了卷积层和池化层如何协同工作，产生输入的新表示，从而帮助在特征空间中分离不同类别，实现了高准确度。
- en: In the next chapter, we’ll continue our look at CNNs, but instead of theory,
    we’ll work with actual examples to see how the various parameters of the network,
    and the hyperparameters used during training, affect model performance. This will
    help us build intuition about how to work with CNNs in the future.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将继续探讨卷积神经网络（CNN），但不同于理论，我们将通过实际例子来观察网络的各种参数以及训练过程中使用的超参数如何影响模型的表现。这将帮助我们建立未来使用CNN时的直觉。
