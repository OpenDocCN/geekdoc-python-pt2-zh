- en: '**7'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**7'
- en: EXPERIMENTS WITH CLASSICAL MODELS**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 经典模型实验**
- en: '![image](Images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/common.jpg)'
- en: In [Chapter 6](ch06.xhtml#ch06), we introduced several classical machine learning
    models. Let’s now take the datasets we built in [Chapter 5](ch05.xhtml#ch05) and
    use them with these models to see how well they perform. We’ll use sklearn to
    create the models and then we’ll compare them by looking at how well they do on
    the held-out test sets.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第6章](ch06.xhtml#ch06)中，我们介绍了几种经典的机器学习模型。现在让我们使用在[第5章](ch05.xhtml#ch05)中构建的数据集，并将其与这些模型一起使用，看看它们的表现如何。我们将使用sklearn创建模型，然后通过查看它们在保留的测试集上的表现来进行比较。
- en: 'This will give us a good overview of how to work with sklearn and help us build
    intuition about how the different models perform relative to one another. We’ll
    use three datasets: the iris dataset, both original and augmented; the breast
    cancer dataset; and the vector form of the MNIST handwritten digits dataset.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我们一个很好的概述，帮助我们了解如何使用sklearn，并建立对不同模型相对表现的直觉。我们将使用三个数据集：鸢尾花数据集（包括原始数据和增强数据）；乳腺癌数据集；以及MNIST手写数字数据集的向量形式。
- en: Experiments with the Iris Dataset
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 鸢尾花数据集实验
- en: 'We’ll start with the iris dataset. This data set has four continuous features—the
    measurements of the sepal length, sepal width, petal length, and petal width—and
    three classes—different iris species. There are 150 samples, 50 each from the
    three classes. In [Chapter 5](ch05.xhtml#ch05), we applied PCA augmentation to
    the dataset, so we actually have two versions we can work with: the original 150
    samples and the 1200 augmented training samples. Both can use the same test set.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从鸢尾花数据集开始。该数据集有四个连续特征——萼片长度、萼片宽度、花瓣长度和花瓣宽度——以及三个类别——不同的鸢尾花种类。共有150个样本，每个类别50个。在[第5章](ch05.xhtml#ch05)中，我们对数据集应用了PCA增强，因此我们实际上有两个版本可以使用：原始的150个样本和增强后的1200个训练样本。两者都可以使用相同的测试集。
- en: We’ll use sklearn to implement versions of the Nearest Centroid, *k*-NN, Naïve
    Bayes, Decision Tree, Random Forest, and SVM models we outlined in [Chapter 6](ch06.xhtml#ch06).
    We’ll quickly see how powerful and elegant the sklearn toolkit is since our tests
    are virtually all identical across the models. The only thing that changes is
    the particular class we instantiate.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用sklearn实现我们在[第6章](ch06.xhtml#ch06)中概述的最近质心、*k*-NN、朴素贝叶斯、决策树、随机森林和SVM模型的版本。我们将很快看到sklearn工具包是多么强大和优雅，因为我们的测试几乎在所有模型中都完全相同。唯一变化的是我们实例化的具体类。
- en: Testing the Classical Models
  id: totrans-8
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 经典模型测试
- en: The code for our initial tests is in [Listing 7-1](ch07.xhtml#ch7lis1).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最初测试的代码在[清单7-1](ch07.xhtml#ch7lis1)中。
- en: import numpy as np
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: from sklearn.neighbors import NearestCentroid
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.neighbors import NearestCentroid
- en: from sklearn.neighbors import KNeighborsClassifier
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.neighbors import KNeighborsClassifier
- en: from sklearn.naive_bayes import GaussianNB, MultinomialNB
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.naive_bayes import GaussianNB, MultinomialNB
- en: from sklearn.tree import DecisionTreeClassifier
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.tree import DecisionTreeClassifier
- en: from sklearn.ensemble import RandomForestClassifier
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.ensemble import RandomForestClassifier
- en: from sklearn.svm import SVC
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.svm import SVC
- en: '❶ def run(x_train, y_train, x_test, y_test, clf):'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '❶ def run(x_train, y_train, x_test, y_test, clf):'
- en: clf.fit(x_train, y_train)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: clf.fit(x_train, y_train)
- en: print("    predictions  :", clf.predict(x_test))
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: print("    预测结果  :", clf.predict(x_test))
- en: print("    actual labels:", y_test)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: print("    实际标签:", y_test)
- en: print("    score = %0.4f" % clf.score(x_test, y_test))
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: print("    得分 = %0.4f" % clf.score(x_test, y_test))
- en: print()
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: print()
- en: 'def main():'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 'def main():'
- en: ❷ x = np.load("../data/iris/iris_features.npy")
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ x = np.load("../data/iris/iris_features.npy")
- en: y = np.load("../data/iris/iris_labels.npy")
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: y = np.load("../data/iris/iris_labels.npy")
- en: N = 120
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: N = 120
- en: x_train = x[:N]; x_test = x[N:]
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: x_train = x[:N]; x_test = x[N:]
- en: y_train = y[:N]; y_test = y[N:]
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: y_train = y[:N]; y_test = y[N:]
- en: ❸ xa_train=np.load("../data/iris/iris_train_features_augmented.npy")
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ xa_train=np.load("../data/iris/iris_train_features_augmented.npy")
- en: ya_train=np.load("../data/iris/iris_train_labels_augmented.npy")
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ya_train=np.load("../data/iris/iris_train_labels_augmented.npy")
- en: xa_test =np.load("../data/iris/iris_test_features_augmented.npy")
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: xa_test =np.load("../data/iris/iris_test_features_augmented.npy")
- en: ya_test =np.load("../data/iris/iris_test_labels_augmented.npy")
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ya_test =np.load("../data/iris/iris_test_labels_augmented.npy")
- en: print("Nearest Centroid:")
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: print("最近质心：")
- en: ❹ run(x_train, y_train, x_test, y_test, NearestCentroid())
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ run(x_train, y_train, x_test, y_test, NearestCentroid())
- en: print("k-NN classifier (k=3):")
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: print("k-NN分类器（k=3）：")
- en: run(x_train, y_train, x_test, y_test,
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test,
- en: KNeighborsClassifier(n_neighbors=3))
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: KNeighborsClassifier(n_neighbors=3))
- en: print("Naive Bayes classifier (Gaussian):")
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: print("朴素贝叶斯分类器（高斯）：")
- en: ❺ run(x_train, y_train, x_test, y_test, GaussianNB())
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ run(x_train, y_train, x_test, y_test, GaussianNB())
- en: print("Naive Bayes classifier (Multinomial):")
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: print("朴素贝叶斯分类器（多项式）：")
- en: run(x_train, y_train, x_test, y_test, MultinomialNB())
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test, MultinomialNB())
- en: ❻ print("Decision Tree classifier:")
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ print("决策树分类器：")
- en: run(x_train, y_train, x_test, y_test, DecisionTreeClassifier())
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test, DecisionTreeClassifier())
- en: print("Random Forest classifier (estimators=5):")
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: print("随机森林分类器（估计器=5）：")
- en: run(xa_train, ya_train, xa_test, ya_test,
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: run(xa_train, ya_train, xa_test, ya_test,
- en: RandomForestClassifier(n_estimators=5))
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: RandomForestClassifier(n_estimators=5))
- en: ❼ print("SVM (linear, C=1.0):")
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ print("SVM（线性，C=1.0）：")
- en: run(xa_train, ya_train, xa_test, ya_test, SVC(kernel="linear", C=1.0))
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: run(xa_train, ya_train, xa_test, ya_test, SVC(kernel="linear", C=1.0))
- en: print("SVM (RBF, C=1.0, gamma=0.25):")
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: print("SVM（RBF，C=1.0，gamma=0.25）：")
- en: run(xa_train, ya_train, xa_test, ya_test,
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: run(xa_train, ya_train, xa_test, ya_test,
- en: SVC(kernel="rbf", C=1.0, gamma=0.25))
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: SVC(kernel="rbf", C=1.0, gamma=0.25))
- en: print("SVM (RBF, C=1.0, gamma=0.001, augmented)")
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: print("SVM（RBF，C=1.0，gamma=0.001，增强版）")
- en: run(xa_train, ya_train, xa_test, ya_test,
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: run(xa_train, ya_train, xa_test, ya_test,
- en: SVC(kernel="rbf", C=1.0, gamma=0.001))
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: SVC(kernel="rbf", C=1.0, gamma=0.001))
- en: ❽ print("SVM (RBF, C=1.0, gamma=0.001, original)")
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ print("SVM（RBF，C=1.0，gamma=0.001，原始）")
- en: run(x_train, y_train, x_test, y_test,
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test,
- en: SVC(kernel="rbf", C=1.0, gamma=0.001))
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: SVC(kernel="rbf", C=1.0, gamma=0.001))
- en: '*Listing 7-1: Classic models using the iris dataset. See* iris_experiments.py.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '*Listing 7-1: 使用鸢尾花数据集的经典模型。请参见* iris_experiments.py。'
- en: 'First, we import the necessary classes and modules. Notice that each of the
    classes represents a single type of model (classifier). For the Naïve Bayes classifier,
    we’re using two versions: the Gaussian version, `GaussianNB`, because the features
    are continuous values, and `MultinomialNB` for the discrete case to illustrate
    the effect of choosing a model that’s inappropriate for the dataset we’re working
    with. Because sklearn has a uniform interface for its classifiers, we can simplify
    things by using the same function to train and test any particular classifier.
    That function is `run` ❶. We pass in the training features (`x_train`) and labels
    (`y_train`) along with the test features and labels (`x_test`, `y_test`). We also
    pass in the particular classifier object (`clf`).'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入必要的类和模块。请注意，每个类代表一种特定的模型（分类器）。对于朴素贝叶斯分类器，我们使用了两个版本：高斯版本`GaussianNB`，因为特征是连续值，以及`MultinomialNB`，用于离散情况，以说明选择不适合数据集的模型的影响。由于
    sklearn 为其分类器提供了统一的接口，我们可以通过使用相同的函数来训练和测试任何特定的分类器，从而简化流程。这个函数是`run` ❶。我们传入训练特征（`x_train`）和标签（`y_train`）以及测试特征和标签（`x_test`、`y_test`）。我们还传入特定的分类器对象（`clf`）。
- en: The first thing we do inside `run` is fit the model to the data by calling `fit`
    with the training data samples and labels. This is the training step. After the
    model is trained, we can test how well it does by calling the `predict` method
    with the held-out test data. This method returns the predicted class label for
    each sample in the test data. We held back 30 samples from the original 150 so
    `predict` will return a vector of 30 class label assignments, which we print.
    Next, we print the actual test labels so we can compare them visually with the
    predictions. Finally, we use the `score` method to apply the classifier to the
    test data (`x_test`) using the known test labels (`y_test`) to calculate the overall
    accuracy.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `run` 中，首先通过调用 `fit` 方法，将模型拟合到数据上，传入训练数据样本和标签，这是训练步骤。训练完成后，我们可以通过调用 `predict`
    方法并传入保留的测试数据来测试模型的表现。该方法返回测试数据中每个样本的预测类别标签。我们从原始的 150 个样本中保留了 30 个样本，因此 `predict`
    将返回一个包含 30 个类别标签的向量，我们会打印出来。接下来，我们打印出实际的测试标签，以便与预测结果进行视觉对比。最后，我们使用 `score` 方法，将分类器应用到测试数据（`x_test`）上，使用已知的测试标签（`y_test`）来计算总体准确率。
- en: The accuracy is returned as a fraction between 0 and 1\. If every test sample
    were given the wrong label, the accuracy would be 0\. Even random guessing will
    do better than that, so a return value of 0 is a sign that something is amiss.
    Since there are three classes in the iris dataset, we’d expect a classifier that
    guesses the class at random to be right about one-third of the time and return
    a value close to 0.3333\. The actual score is calculated as
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率作为 0 到 1 之间的分数返回。如果每个测试样本都被标记为错误，准确率将为 0。即使是随机猜测，也能比这做得更好，因此返回值为 0 是一个表明出了问题的信号。由于鸢尾花数据集中有三个类别，我们预计一个随机猜测类别的分类器大约有三分之一的概率猜对，返回值接近
    0.3333。实际得分计算方法如下：
- en: score = *N[c]*/(*N[c]* + *N[w]*)
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: score = *N[c]* / (*N[c]* + *N[w]*)
- en: where *N*[*c*] is the number of test samples for which the predicted class is
    correct; that is, it matches the class label in `y_test`. *N*[*w*] is the number
    of test samples where the predicted class does not match the actual class label.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *N*[*c*] 是预测类别正确的测试样本数；也就是说，它与`y_test`中的类别标签匹配。*N*[*w*] 是预测类别与实际类别标签不匹配的测试样本数。
- en: Now that we have a way to train and test each classifier, all we need to do
    is load the datasets and run a series of experiments by creating different classifier
    objects and passing them to `run`. Back inside of `main`, we begin by loading
    the original iris dataset and separating it into train and test cases ❷. We also
    load the augmented iris dataset that we created in [Chapter 5](ch05.xhtml#ch05)
    ❸. By design, the two test sets are identical, so regardless of which training
    set we use, the test set will be the same. This simplifies our comparisons.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了训练和测试每个分类器的方法，接下来只需要加载数据集并通过创建不同的分类器对象并将其传递给`run`来运行一系列实验。在`main`函数内部，我们首先加载原始的鸢尾花数据集，并将其分为训练集和测试集
    ❷。我们还加载了在[第5章](ch05.xhtml#ch05)中创建的增强版鸢尾花数据集 ❸。这两个测试集在设计上是相同的，因此无论我们使用哪个训练集，测试集都会是一样的。这简化了我们的比较。
- en: 'We then define and execute the Nearest Centroid classifier ❹. The output is
    shown here:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接着定义并执行了最近质心分类器 ❹。输出结果如下所示：
- en: '[PRE0]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We’ve removed spaces to make a visual comparison between the predicted and actual
    class labels easier. If there’s an error, the corresponding value, 0–2, will not
    match between the two lines. The score is also shown. In this case, it’s 1.0,
    which tells us that the classifier was perfect in its predictions on the held-out
    test set. This isn’t surprising; the iris dataset is a simple one. Because the
    iris dataset was randomized when created in [Chapter 5](ch05.xhtml#ch05), you
    might get a different overall score. However, unless your randomization was particularly
    unfortunate, you should have a high test score.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们移除了空格，以便更直观地对比预测的类别标签和实际的类别标签。如果存在错误，相应的值（0-2）将在两行之间不匹配。得分也会显示。在这种情况下，得分为
    1.0，这告诉我们分类器在保留的测试集上的预测是完美的。这并不令人惊讶；鸢尾花数据集是一个简单的例子。由于鸢尾花数据集在[第5章](ch05.xhtml#ch05)中被随机化，你可能会得到不同的整体得分。然而，除非你的随机化非常不幸，否则你应该能够得到一个较高的测试得分。
- en: Based on what we learned in [Chapter 6](ch06.xhtml#ch06), we should expect that
    if the Nearest Centroid classifier is perfect on the test data, then all the other
    more sophisticated models will likewise be perfect. This is generally the case
    here, but as we’ll see, careless selection of model type or model hyperparameter
    values will result in inferior performance even from a more sophisticated model.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 基于我们在[第6章](ch06.xhtml#ch06)中学到的内容，我们应该预期，如果最近质心分类器在测试数据上是完美的，那么所有其他更复杂的模型也将是完美的。一般来说，这种情况是成立的，但正如我们将看到的，粗心选择模型类型或模型超参数值将导致即使是更复杂的模型也表现不佳。
- en: 'Look again at [Listing 7-1](ch07.xhtml#ch7lis1), where we train a Gaussian
    Naïve Bayes classifier by passing an instance of `GaussianNB` to `run` ❺. This
    classifier is also perfect and returns a score of 1.0\. This is the correct way
    to use continuous values with a Naïve Bayes classifier. What happens if we instead
    use the discrete case even though we have continuous features? This is the `MultinomialNB`
    classifier, which assumes the features are selected from a discrete set of possible
    values. For the iris dataset, we can get away with defining such a classifier
    because the feature values are non-negative. However, because the features are
    not discrete, this model is not perfect and returns the following:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 再次查看[列表 7-1](ch07.xhtml#ch7lis1)，我们通过将`GaussianNB`的实例传递给`run`来训练高斯朴素贝叶斯分类器 ❺。这个分类器也是完美的，返回得分为1.0。
    这就是使用连续值与朴素贝叶斯分类器的正确方法。如果我们尽管有连续特征，但仍使用离散情况，会发生什么呢？这就是`MultinomialNB`分类器，它假设特征是从离散的可能值集合中选择的。对于鸢尾花数据集，由于特征值是非负的，我们可以使用这种分类器。然而，由于这些特征并非离散的，这个模型并不完美，返回了以下结果：
- en: '[PRE1]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Here we see that the classifier is only 86.7 percent accurate on our test samples.
    If we need discrete counts for the probabilities, why did this approach work at
    all in this case? The answer is evident in the sklearn source code for the `MultinomialNB`
    classifier. The method that counts feature frequencies per class uses `np.dot`
    so that even if the feature values are continuous, the output will be a valid
    number, though not an integer. Still, mistakes were made, so we shouldn’t be happy.
    We should instead be careful to select the proper classifier type for the actual
    data we’re working with.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，分类器在我们的测试样本上的准确率只有86.7%。如果我们需要概率的离散计数，为什么这种方法在这种情况下还能有效呢？答案在`MultinomialNB`分类器的sklearn源代码中显而易见。该方法通过`np.dot`计算每个类别的特征频率，因此即使特征值是连续的，输出仍然是一个有效的数字，尽管不是整数。尽管如此，还是出现了错误，所以我们不应感到满足。相反，我们应该小心地为我们正在处理的实际数据选择合适的分类器类型。
- en: The next model we train in [Listing 7-1](ch07.xhtml#ch7lis1) is a Decision Tree
    ❻. This classifier is perfect on this dataset, as is the Random Forest trained
    next. Note, the Random Forest is using five estimators, meaning five random trees
    are created and trained; voting between the individual outputs determines the
    final class label. Note also that the Random Forest is trained on the augmented
    iris dataset, `xa_train`, because of the limited number of training samples in
    the unaugmented dataset.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[清单 7-1](ch07.xhtml#ch7lis1)中训练的下一个模型是决策树❻。这个分类器在该数据集上表现得非常好，接下来训练的随机森林也是如此。注意，随机森林使用了五个估计器，这意味着创建并训练了五棵随机树；个别输出之间的投票决定了最终的类别标签。还要注意，随机森林是在扩增后的鸢尾花数据集`xa_train`上训练的，因为未扩增数据集中的训练样本数量有限。
- en: 'We then train several SVM classifiers ❼, also on the augmented dataset. Recall
    that SVMs have two parameters we control: the margin constant, `C`, and `gamma`
    used by the Gaussian kernel.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们在扩增后的数据集上训练了几个支持向量机(SVM)分类器❼。回想一下，SVM有两个我们可以控制的参数：边际常数`C`和高斯核使用的`gamma`。
- en: The first is a linear SVM, meaning we need a value for the margin constant (`C`).
    We define `C` to be 1.0, the default value for sklearn. This classifier is perfect
    on the test data, as is the following classifier using the Gaussian kernel, for
    which we also set *γ* to 0.25\. The `SVC` class defaults to `auto` for `gamma`,
    which sets *γ* to 1/*n*, where *n* is the number of features. For the iris dataset,
    *n* = 4 so *γ* = 0.25.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个是线性SVM，这意味着我们需要为边际常数(`C`)指定一个值。我们将`C`定义为1.0，这是sklearn的默认值。这个分类器在测试数据上表现完美，接下来的分类器使用高斯核，我们也将*γ*设置为0.25。`SVC`类默认`gamma`为`auto`，这会将*γ*设置为1/*n*，其中*n*是特征的数量。对于鸢尾花数据集，*n*
    = 4，因此*γ* = 0.25。
- en: 'Next, we train a model with very small *γ*. The classifier is still perfect
    on the test data. Lastly, we train the same type of SVM, but instead of the augmented
    training data, we use the original training data ❽. This classifier is not perfect:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用非常小的*γ*训练一个模型。这个分类器在测试数据上仍然完美。最后，我们训练了相同类型的SVM，但这次我们使用的是原始训练数据❽，而不是增强后的训练数据。这个分类器并不完美：
- en: '[PRE2]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In fact, it’s rather dismal. It never predicts class 1 and is right only 56.7
    percent of the time. This shows that data augmentation is valuable as it turned
    a lousy classifier into a good one—at least, good as far as we can know from the
    small test set we are using!
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，结果相当糟糕。它从未预测出类别1，而且只有56.7%的时间是正确的。这表明数据增强非常有价值，因为它将一个糟糕的分类器转变成了一个不错的分类器——至少，从我们使用的小测试集来看是这样的！
- en: Implementing a Nearest Centroid Classifier
  id: totrans-78
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实现最近质心分类器
- en: What if we were stranded on a deserted island and didn’t have access to sklearn?
    Could we still quickly build a suitable classifier for the iris dataset? The answer
    is “yes,” as [Listing 7-2](ch07.xhtml#ch7lis2) shows. This code implements a quick-and-dirty
    Nearest Centroid classifier for the iris dataset.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们被困在荒岛上，无法访问sklearn，怎么办？我们还能快速构建一个适合鸢尾花数据集的分类器吗？答案是“可以”，正如[清单 7-2](ch07.xhtml#ch7lis2)所示。这段代码实现了一个快速且简陋的最近质心分类器，用于鸢尾花数据集。
- en: import numpy as np
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: '❶ def centroids(x,y):'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '❶ def centroids(x,y):'
- en: c0 = x[np.where(y==0)].mean(axis=0)
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: c0 = x[np.where(y==0)].mean(axis=0)
- en: c1 = x[np.where(y==1)].mean(axis=0)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: c1 = x[np.where(y==1)].mean(axis=0)
- en: c2 = x[np.where(y==2)].mean(axis=0)
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: c2 = x[np.where(y==2)].mean(axis=0)
- en: return [c0,c1,c2]
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: return [c0,c1,c2]
- en: '❷ def predict(c0,c1,c2,x):'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '❷ def predict(c0,c1,c2,x):'
- en: p = np.zeros(x.shape[0], dtype="uint8")
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: p = np.zeros(x.shape[0], dtype="uint8")
- en: 'for i in range(x.shape[0]):'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i in range(x.shape[0]):'
- en: d = [((c0-x[i])**2).sum(),
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: d = [((c0-x[i])**2).sum(),
- en: ((c1-x[i])**2).sum(),
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ((c1-x[i])**2).sum(),
- en: ((c2-x[i])**2).sum()]
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ((c2-x[i])**2).sum()]
- en: p[i] = np.argmin(d)
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: p[i] = np.argmin(d)
- en: return p
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: return p
- en: 'def main():'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 'def main():'
- en: ❸ x = np.load("../data/iris/iris_features.npy")
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ x = np.load("../data/iris/iris_features.npy")
- en: y = np.load("../data/iris/iris_labels.npy")
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: y = np.load("../data/iris/iris_labels.npy")
- en: N = 120
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: N = 120
- en: x_train = x[:N]; x_test = x[N:]
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: x_train = x[:N]; x_test = x[N:]
- en: y_train = y[:N]; y_test = y[N:]
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: y_train = y[:N]; y_test = y[N:]
- en: c0, c1, c2 = centroids(x_train, y_train)
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: c0, c1, c2 = centroids(x_train, y_train)
- en: p = predict(c0,c1,c2, x_test)
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: p = predict(c0, c1, c2, x_test)
- en: nc = len(np.where(p == y_test)[0])
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: nc = len(np.where(p == y_test)[0])
- en: nw = len(np.where(p != y_test)[0])
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: nw = len(np.where(p != y_test)[0])
- en: acc = float(nc) / (float(nc)+float(nw))
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: acc = float(nc) / (float(nc) + float(nw))
- en: print("predicted:", p)
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: print("预测值：", p)
- en: print("actual   :", y_test)
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: print("实际值：", y_test)
- en: print("test accuracy = %0.4f" % acc)
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: print("测试准确率 = %0.4f" % acc)
- en: '*Listing 7-2: A quick-and-dirty Nearest Centroid classifier for the iris dataset.
    See* iris_centroids.py.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 7-2：一个快速简便的最近中心分类器，用于鸢尾花数据集。见* iris_centroids.py。'
- en: We load the iris data and separate it into train and test sets as before ❸.
    The `centroids` function returns the centroids of the three classes ❶. We can
    easily calculate these by finding the per feature means of each training sample
    of the desired class. This is all it takes to train this model. If we compare
    the returned centroids with those in the preceding trained `NearestCentroid` classifier
    (see the `centroids_` member variable), we get precisely the same values.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们加载鸢尾花数据集，并像之前一样将其分为训练集和测试集❸。`centroids`函数返回三个类别的中心点❶。我们可以通过计算每个训练样本的特征均值来轻松得到这些中心点。这就是训练这个模型所需的全部内容。如果我们将返回的中心点与之前训练的`NearestCentroid`分类器中的中心点（请参见`centroids_`成员变量）进行比较，我们会得到完全相同的值。
- en: Using the classifier is straightforward, as `predict` shows ❷. First, we define
    the vector of predictions, one per test sample (`x`). The loop defines `d`, a
    vector of Euclidean distances from the current test sample, `x[i]`, to the three
    class centroids. The index of the smallest distance in `d` is the predicted class
    label (`p[i]`).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 使用分类器非常直接，如`predict`所示❷。首先，我们定义预测向量，每个测试样本一个（`x`）。循环定义了`d`，它是当前测试样本`x[i]`到三个类别中心点的欧几里得距离向量。`d`中最小距离的索引就是预测的类别标签（`p[i]`）。
- en: Let’s unpack `d` a bit more. We set `d` to a list of three values, the distances
    from the centroids to the current test sample. The expression
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进一步解析一下`d`。我们将`d`设置为一个包含三个值的列表，表示从每个中心点到当前测试样本的距离。这个表达式
- en: '[PRE3]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: is a bit dense. The phrase `c0-x[i]` returns a vector of four numbers—four because
    we have four features. These are the differences between the centroid of class
    0 and the test sample feature value. This quantity is squared, which squares each
    of the four values. This squared vector is summed, element by element, to return
    the distance measure.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这一部分有点密集。`c0-x[i]`这个短语返回一个包含四个数字的向量——四个数字是因为我们有四个特征。这些数字是类别0的中心点与测试样本特征值之间的差异。这个差异会被平方，即将四个值中的每一个平方。这个平方后的向量按元素逐一相加，得到距离度量。
- en: Strictly speaking, we’re missing a final step. The actual distance between `c0`
    and `x[i]` is the square root of this value. Since we’re simply looking for the
    smallest distance to each of the centroids, we don’t need to calculate the square
    root. The smallest value will still be the smallest value, whether we take the
    square root of all the values or not. Running this code produces the same output
    as we saw previously for the Nearest Centroid classifier, which is encouraging.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 严格来说，我们还缺少最后一步。`c0`和`x[i]`之间的实际距离是这个值的平方根。由于我们只是寻找到每个类别中心点的最小距离，所以我们不需要计算平方根。无论我们是否计算所有值的平方根，最小的值仍然是最小值。运行这段代码会产生与我们之前为最近中心分类器看到的相同的输出，这是令人鼓舞的。
- en: The iris dataset is extremely simple, so we shouldn’t be surprised by the excellent
    performance of our models even though we saw that careless selection of model
    type and hyperparameters will cause us trouble. Let’s now look at a larger dataset
    with more features, one that was not meant as a toy.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 鸢尾花数据集非常简单，因此即使我们看到不小心选择模型类型和超参数会给我们带来麻烦，我们也不应该对模型的优异表现感到惊讶。现在让我们来看一个更大的数据集，它有更多特征，并且不是为了作为玩具而设计的。
- en: Experiments with the Breast Cancer Dataset
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 乳腺癌数据集实验
- en: 'The two-class breast cancer dataset we developed in [Chapter 5](ch05.xhtml#ch05)
    has 569 samples, each with 30 features, all measurements from a histology slide.
    There are 212 malignant cases (class 1) and 357 benign cases (class 0). Let’s
    train our classic models on this dataset and see what sort of results we get.
    As all the features are continuous, let’s use the normalized version of the dataset.
    Recall that a normalized dataset is one where, per feature in the feature vector,
    each value has the mean for that feature subtracted and then is divided by the
    standard deviation:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第5章](ch05.xhtml#ch05)中开发的双类乳腺癌数据集包含569个样本，每个样本有30个特征，所有测量均来自组织学切片。该数据集有212个恶性病例（类1）和357个良性病例（类0）。我们将在此数据集上训练经典模型，并查看得到的结果。由于所有特征都是连续的，我们将使用数据集的标准化版本。回想一下，标准化数据集是指对于每个特征向量中的特征，先减去该特征的均值，再除以该特征的标准差：
- en: '![image](Images/135equ01.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/135equ01.jpg)'
- en: Normalization of the dataset maps all the features into the same overall range
    so that the value of one feature is similar to the value of another. This helps
    many model types and is a typical data preprocessing step, as we discussed in
    [Chapter 4](ch04.xhtml#ch04).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的标准化将所有特征映射到相同的总体范围内，使得一个特征的值与另一个特征的值相似。这有助于许多模型类型，并且是典型的数据预处理步骤，如我们在[第4章](ch04.xhtml#ch04)中讨论的那样。
- en: Two Initial Test Runs
  id: totrans-120
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 两次初步测试
- en: First, we’ll do a quick run with a single test split, as we did in the previous
    section. The code is in [Listing 7-3](ch07.xhtml#ch7lis3) and mimics the code
    we described previously, where we pass in the model instance, train it, and then
    score it using the testing data.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将像上一节那样快速运行一次单一的测试拆分。代码见[列表 7-3](ch07.xhtml#ch7lis3)，它模仿了我们之前描述的代码，在其中我们传入模型实例，训练它，然后使用测试数据对其进行评分。
- en: import numpy as np
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: from sklearn.neighbors import NearestCentroid
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.neighbors import NearestCentroid
- en: from sklearn.neighbors import KNeighborsClassifier
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.neighbors import KNeighborsClassifier
- en: from sklearn.naive_bayes import GaussianNB, MultinomialNB
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.naive_bayes import GaussianNB, MultinomialNB
- en: from sklearn.tree import DecisionTreeClassifier
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.tree import DecisionTreeClassifier
- en: from sklearn.ensemble import RandomForestClassifier
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.ensemble import RandomForestClassifier
- en: from sklearn.svm import SVC
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.svm import SVC
- en: 'def run(x_train, y_train, x_test, y_test, clf):'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 'def run(x_train, y_train, x_test, y_test, clf):'
- en: clf.fit(x_train, y_train)
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: clf.fit(x_train, y_train)
- en: print("    score = %0.4f" % clf.score(x_test, y_test))
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: print("    得分 = %0.4f" % clf.score(x_test, y_test))
- en: print()
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: print()
- en: 'def main():'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 'def main():'
- en: x = np.load("../data/breast/bc_features_standard.npy")
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: x = np.load("../data/breast/bc_features_standard.npy")
- en: y = np.load("../data/breast/bc_labels.npy")
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: y = np.load("../data/breast/bc_labels.npy")
- en: ❶ N = 455
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ N = 455
- en: x_train = x[:N];  x_test = x[N:]
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: x_train = x[:N];  x_test = x[N:]
- en: y_train = y[:N];  y_test = y[N:]
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: y_train = y[:N];  y_test = y[N:]
- en: print("Nearest Centroid:")
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: print("最近中心:")
- en: run(x_train, y_train, x_test, y_test, NearestCentroid())
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test, NearestCentroid())
- en: print("k-NN classifier (k=3):")
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: print("k-NN 分类器 (k=3):")
- en: run(x_train, y_train, x_test, y_test,
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test,
- en: KNeighborsClassifier(n_neighbors=3))
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: KNeighborsClassifier(n_neighbors=3))
- en: print("k-NN classifier (k=7):")
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: print("k-NN 分类器 (k=7):")
- en: run(x_train, y_train, x_test, y_test,
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test,
- en: KNeighborsClassifier(n_neighbors=7))
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: KNeighborsClassifier(n_neighbors=7))
- en: print("Naive Bayes classifier (Gaussian):")
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: print("朴素贝叶斯分类器 (高斯):")
- en: run(x_train, y_train, x_test, y_test, GaussianNB())
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test, GaussianNB())
- en: print("Decision Tree classifier:")
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: print("决策树分类器:")
- en: run(x_train, y_train, x_test, y_test, DecisionTreeClassifier())
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test, DecisionTreeClassifier())
- en: print("Random Forest classifier (estimators=5):")
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: print("随机森林分类器 (估计器=5):")
- en: run(x_train, y_train, x_test, y_test,
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test,
- en: RandomForestClassifier(n_estimators=5))
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: RandomForestClassifier(n_estimators=5))
- en: print("Random Forest classifier (estimators=50):")
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: print("随机森林分类器 (估计器=50):")
- en: run(x_train, y_train, x_test, y_test,
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test,
- en: RandomForestClassifier(n_estimators=50))
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: RandomForestClassifier(n_estimators=50))
- en: print("SVM (linear, C=1.0):")
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: print("支持向量机 (线性, C=1.0):")
- en: run(x_train, y_train, x_test, y_test, SVC(kernel="linear", C=1.0))
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test, SVC(kernel="linear", C=1.0))
- en: print("SVM (RBF, C=1.0, gamma=0.03333):")
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: print("支持向量机 (RBF, C=1.0, gamma=0.03333):")
- en: run(x_train, y_train, x_test, y_test,
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test,
- en: SVC(kernel="rbf", C=1.0, gamma=0.03333))
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: SVC(kernel="rbf", C=1.0, gamma=0.03333))
- en: '*Listing 7-3: Initial models using the breast cancer dataset. See* bc_experiments.py.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 7-3: 使用乳腺癌数据集的初步模型。见* bc_experiments.py。'
- en: 'As before, we load the dataset and split it into training and testing data.
    We keep 455 of the 569 samples for training (80 percent), and the remaining 114
    samples are the test set (74 benign, 40 malignant). The dataset is already randomized,
    so we skip that step here. We then train nine models: Nearest Centroid (1), *k*-NN
    (2), Naïve Bayes (1), Decision Tree (1), Random Forest (2), linear SVM (1), and
    an RBF SVM (1). For the Support Vector Machines, we use the default *C* value,
    and for *γ*, we use 1/30 = 0.033333 since we have 30 features. Running this code
    gives us the scores in [Table 7-1](ch07.xhtml#ch7tab1).'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 如之前所述，我们加载数据集并将其分为训练数据和测试数据。我们保留569个样本中的455个用于训练（80%），剩下的114个样本作为测试集（74个良性，40个恶性）。数据集已经随机化，因此我们省略了这一步。接下来我们训练了九个模型：最近质心（1），*k*-NN（2），朴素贝叶斯（1），决策树（1），随机森林（2），线性SVM（1）以及RBF
    SVM（1）。对于支持向量机，我们使用默认的*C*值，*γ*则设为1/30 = 0.033333，因为我们有30个特征。运行这段代码给我们带来了[表 7-1](ch07.xhtml#ch7tab1)中的得分。
- en: '**Table 7-1:** Breast Cancer Model Scores'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 7-1：** 乳腺癌模型得分'
- en: '| **Model type** | **Score** |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| **模型类型** | **得分** |'
- en: '| --- | --- |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Nearest Centroid | 0.9649 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 最近质心 | 0.9649 |'
- en: '| 3-NN classifier | 0.9912 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 3-NN 分类器 | 0.9912 |'
- en: '| 7-NN classifier | 0.9737 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 7-NN 分类器 | 0.9737 |'
- en: '| Naïve Bayes (Gaussian) | 0.9825 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 朴素贝叶斯（高斯） | 0.9825 |'
- en: '| Decision Tree | 0.9474 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 决策树 | 0.9474 |'
- en: '| Random Forest (5) | 0.9298 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林（5棵树） | 0.9298 |'
- en: '| Random Forest (50) | 0.9737 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林（50棵树） | 0.9737 |'
- en: '| Linear SVM (C = 1) | 0.9737 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 线性SVM（C = 1） | 0.9737 |'
- en: '| RBF SVM (C = 1, *γ* = 0.03333) | 0.9825 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| RBF SVM（C = 1，*γ* = 0.03333） | 0.9825 |'
- en: Note the number in parentheses for the Random Forest classifiers is the number
    of estimators (number of trees in the forest).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，随机森林分类器括号中的数字是估计器的数量（森林中树的数量）。
- en: 'A few things jump out at us. First, perhaps surprisingly, the simple Nearest
    Centroid classifier is right nearly 97 percent of the time. We also see that all
    the other classifiers are doing better than the Nearest Centroid, except for the
    Decision Tree and the Random Forest with five trees. Somewhat surprisingly, the
    Naïve Bayes classifier does very well, matching the RBF SVM. The *k* = 3 Nearest
    Neighbor classifier does best of all, 99 percent accurate, even though we have
    30 features, meaning our 569 samples are points scattered in a 30-dimensional
    space. Recall, a weakness of *k*-NN is the curse of dimensionality: it requires
    more and more training samples as the number of features increases. The results
    with all the classifiers are good, so this is a hint to us that the separation
    between malignant and benign is, for this dataset, distinct. There isn’t much
    overlap between the two classes using these features.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个亮点需要我们注意。首先，可能令人惊讶的是，简单的最近质心分类器几乎有97%的准确率。我们还看到，除了决策树和五棵树的随机森林外，所有其他分类器的表现都优于最近质心分类器。有些令人意外的是，朴素贝叶斯分类器表现非常好，和
    RBF SVM 的得分相当。*k* = 3 的最近邻分类器表现最好，准确率达到99%，尽管我们有30个特征，这意味着我们的569个样本是在一个30维空间中散布的。回想一下，*k*-NN
    的一个弱点是维度灾难：随着特征数的增加，它需要更多的训练样本。所有分类器的结果都很好，因此这给我们一个提示：在这个数据集中，恶性和良性之间的区分非常明显。使用这些特征时，两个类别之间几乎没有重叠。
- en: So, are we done with this dataset? Hardly! In fact, we’ve just begun. What happens
    if we run the code a second time? Do we get the same scores? Would we expect not
    to? A second run gives us [Table 7-2](ch07.xhtml#ch7tab2).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这个数据集就处理完了吗？当然不是！事实上，我们才刚刚开始。如果我们第二次运行代码，会发生什么呢？我们得到的得分是一样的吗？我们不应该期望它们相同吧？第二次运行给我们带来了[表
    7-2](ch07.xhtml#ch7tab2)。
- en: '**Table 7-2:** Breast Cancer Scores, Second Run'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 7-2：** 乳腺癌得分，第二次运行'
- en: '| **Model type** | **Score** |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| **模型类型** | **得分** |'
- en: '| --- | --- |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Nearest Centroid | 0.9649 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 最近质心 | 0.9649 |'
- en: '| 3-NN classifier | 0.9912 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 3-NN 分类器 | 0.9912 |'
- en: '| 7-NN classifier | 0.9737 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 7-NN 分类器 | 0.9737 |'
- en: '| Naïve Bayes (Gaussian) | 0.9825 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 朴素贝叶斯（高斯） | 0.9825 |'
- en: '| Decision Tree | **0.9386** |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 决策树 | **0.9386** |'
- en: '| Random Forest (5) | **0.9474** |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林（5棵树） | **0.9474** |'
- en: '| Random Forest (50) | **0.9649** |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林（50棵树） | **0.9649** |'
- en: '| Linear SVM (C = 1) | 0.9737 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 线性SVM（C = 1） | 0.9737 |'
- en: '| RBF SVM (C = 1, *γ* = 0.03333) | 0.9825 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| RBF SVM（C = 1，*γ* = 0.03333） | 0.9825 |'
- en: 'We’ve highlighted the scores that changed. Why would anything change? A bit
    of reflection leads to an *aha!* moment: the Random Forest is just that, random,
    so naturally we’d expect different results run to run. What about the Decision
    Tree? In sklearn, the Decision Tree classifier will randomly select a feature
    and find the best split, so different runs will also lead to different trees.
    This is a variation on the basic decision tree algorithm we discussed in [Chapter
    6](ch06.xhtml#ch06).'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经突出显示了变化的分数。为什么会有变化呢？稍微反思一下就会有一个*啊哈！*的时刻：随机森林就是如此，随机的，因此我们自然会期望每次运行结果不同。那么决策树呢？在sklearn中，决策树分类器会随机选择一个特征并找到最佳分割点，因此不同的运行也会导致不同的树。这是我们在[第六章](ch06.xhtml#ch06)讨论的基本决策树算法的变种。
- en: 'All the other algorithms are fixed: for a given training dataset, they can
    lead to only one model. As an aside, the SVM implementation in sklearn does use
    a random number generator, so at times different runs will give slightly different
    results, but, conceptually, we’d expect the same model for the same input data.
    The tree-based classifiers, however, do change between training runs. We’ll explore
    this variation more next. For now, we need to add some rigor to our quick analysis.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 所有其他算法都是固定的：对于给定的训练数据集，它们只能生成一个模型。顺便提一下，sklearn中的SVM实现确实使用了随机数生成器，因此有时不同的运行会给出略微不同的结果，但从概念上讲，我们期望相同的输入数据会得到相同的模型。然而，基于树的分类器在训练过程中会发生变化。我们将在接下来进一步探讨这种变化。现在，我们需要为我们的快速分析增加一些严谨性。
- en: The Effect of Random Splits
  id: totrans-193
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 随机分割的影响
- en: Let’s change the split between training and testing data and see what happens
    to our results. We don’t need to list all the code again since the only change
    is to how `x_train` and `x_test` are defined. Before splitting, we randomize the
    order of the full dataset but do so by first fixing the pseudorandom number seed
    so that each run gives the same ordering to the dataset.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们改变训练数据和测试数据之间的分割，看看结果会发生什么变化。我们不需要再次列出所有代码，因为唯一的变化是`x_train`和`x_test`的定义。在分割之前，我们通过先固定伪随机数种子来随机化整个数据集的顺序，这样每次运行都会给数据集相同的顺序。
- en: Looking again at [Listing 7-3](ch07.xhtml#ch7lis3), insert the following code
    before ❶ so that we generate a fixed permutation of the dataset (`idx`).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 再次查看[列表7-3](ch07.xhtml#ch7lis3)，在❶之前插入以下代码，以便我们生成数据集（`idx`）的固定排列。
- en: '[PRE4]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: It’s fixed because we fixed the pseudorandom number generator seed value. We
    then reorder the samples (`x`) and labels (`y`) accordingly before splitting into
    train and test subsets as before. Running this code gives us the results in [Table
    7-3](ch07.xhtml#ch7tab3).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 它是固定的，因为我们固定了伪随机数生成器的种子值。然后，我们重新排序样本（`x`）和标签（`y`），在进行训练和测试子集的分割之前，和之前一样。运行这段代码给出了[表7-3](ch07.xhtml#ch7tab3)中的结果。
- en: '**Table 7-3:** Breast Cancer Scores After Randomizing the Dataset'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 7-3：** 随机化数据集后的乳腺癌分数'
- en: '| **Model type** | **Score** |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| **模型类型** | **分数** |'
- en: '| --- | --- |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Nearest Centroid | 0.9474 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 最近邻心态 | 0.9474 |'
- en: '| 3-NN classifier | 0.9912 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| 3-NN 分类器 | 0.9912 |'
- en: '| 7-NN classifier | 0.9912 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 7-NN 分类器 | 0.9912 |'
- en: '| Naïve Bayes (Gaussian) | 0.9474 |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 朴素贝叶斯（高斯） | 0.9474 |'
- en: '| Decision Tree | 0.9474 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 决策树 | 0.9474 |'
- en: '| Random Forest (5) | 0.9912 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林（5） | 0.9912 |'
- en: '| Random Forest (50) | 1.0000 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林（50） | 1.0000 |'
- en: '| Linear SVM (C = 1) | 0.9649 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 线性SVM（C = 1） | 0.9649 |'
- en: '| RBF SVM (C = 1, *γ* = 0.03333) | 0.9737 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| RBF SVM（C = 1，*γ* = 0.03333） | 0.9737 |'
- en: Notice these are entirely different from our earlier results. The *k*-NN classifiers
    are both equally good, the SVM classifiers are worse, and the 50-tree Random Forest
    achieves perfection on the test set. So, what is happening? Why are we getting
    all these different results run to run?
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这些结果与我们之前的完全不同。*k*-NN分类器都同样优秀，SVM分类器表现较差，而50棵树的随机森林在测试集上达到了完美。那究竟发生了什么呢？为什么每次运行结果都会有这么大的不同？
- en: 'We’re seeing the effect of the random sampling that builds the train and test
    splits. The first split just happened to use an ordering of samples that gave
    good results for one model type and less good results for other model types. The
    new split favors different model types. Which is correct? Both. Recall what the
    dataset represents: a sampling from some unknown parent distribution that generates
    the data that we actually have. If we think in those terms, we see that the dataset
    we have is an incomplete picture of the true parent distribution. It has biases,
    though we don’t know what they are necessarily, and is deficient in that there
    are parts of the parent distribution that the dataset does not represent well.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在看到构建训练集和测试集拆分的随机抽样效应。第一次拆分恰好使用了一个样本顺序，这对某些模型类型产生了良好的结果，而对其他模型类型产生了较差的结果。新的拆分则偏向不同的模型类型。哪个是正确的？两个都是。回想一下数据集所代表的内容：它是从某个未知的父分布中采样，生成我们实际拥有的数据。如果我们从这个角度思考，就会发现我们拥有的数据集是对真实父分布的不完整描述。它有偏差，尽管我们不一定知道这些偏差是什么，并且存在缺陷，因为数据集未能很好地代表父分布的某些部分。
- en: Further, when we split the data after randomizing the order, we might end up
    with a “bad” mix in the train or test portion—a mix of the data that does a poor
    job of representing the true distribution. If so, we might train a model to recognize
    a slightly different distribution that does not match the true distribution well,
    or the test set might be a bad mix and not be a fair representation of what the
    model has learned. This effect is even more pronounced when the proportion of
    the classes is such that one or more are rare and possibly not present in the
    train or test split. This is precisely the issue that caused us to introduce the
    idea of *k*-fold cross-validation in [Chapter 4](ch04.xhtml#ch04). With *k*-fold
    validation, we’ll be sure to use every sample as both train and test at some point
    and buy ourselves some protection against a bad split by averaging across all
    the folds.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，当我们在随机化顺序后拆分数据时，我们可能会遇到一个“坏”混合的训练集或测试集——一个未能很好代表真实分布的数据混合。如果是这样，我们可能会训练一个模型来识别一个稍微不同的分布，而这个分布与真实分布不匹配，或者测试集可能是一个坏的混合，并且不能公正地代表模型所学到的内容。当类别的比例使得一个或多个类别稀少并且可能在训练集或测试集中不存在时，这种影响尤为明显。这正是导致我们在[第4章](ch04.xhtml#ch04)引入*k*-折交叉验证概念的原因。通过*k*-折验证，我们可以确保每个样本在某些时候都会作为训练集和测试集使用，并通过对所有折叠求平均，为自己提供一定的保护，避免出现不良的拆分。
- en: However, before we apply *k*-fold validation to the breast cancer dataset, we
    should notice one essential thing. We modified the code of [Listing 7-3](ch07.xhtml#ch7lis3)
    to fix the pseudorandom number seed so that we could reorder the dataset in exactly
    the same way each time we run. We then ran the code and saw the results. If we
    rerun the code, we’ll get *exactly* the same output, even for the tree-based classifiers.
    This is not what we saw earlier. The tree classifiers are *stochastic*—they will
    generate a unique tree or forest each time—so we should expect the results to
    vary somewhat from run to run. But now they don’t vary; we get the same output
    each time. By setting the NumPy pseudorandom number seed explicitly, we fixed
    not only the ordering of the dataset, but also the ordering of the *pseudorandom
    sequence* sklearn will use to generate the tree models. This is because sklearn
    is also using the NumPy pseudorandom number generator. This is a subtle effect
    with potentially serious consequences and in a larger project might be very difficult
    to pick up as a bug. The solution is to set the seed to a random value after we’re
    done reordering the dataset. We can do this by adding one line after `y = y[idx]`
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在我们将*k*-折交叉验证应用于乳腺癌数据集之前，我们应该注意一件至关重要的事情。我们修改了[清单 7-3](ch07.xhtml#ch7lis3)中的代码，以修复伪随机数种子，这样每次运行时我们就可以完全相同地重新排序数据集。然后，我们运行了代码并查看了结果。如果我们重新运行代码，我们将获得*完全相同*的输出，即使是对于基于树的分类器。这与我们之前看到的情况不同。树分类器是*随机的*——它们每次都会生成一个独特的树或森林——所以我们应该预期结果会有所不同。但现在它们没有变化；我们每次都会得到相同的输出。通过明确设置NumPy伪随机数种子，我们不仅固定了数据集的排序，还固定了sklearn用来生成树模型的*伪随机序列*的排序。这是因为sklearn也使用了NumPy的伪随机数生成器。这是一个微妙的效果，可能会产生严重后果，在一个更大的项目中，可能很难发现它是一个bug。解决方法是在我们完成数据集重新排序后，将种子设置为随机值。我们可以在`y
    = y[idx]`之后添加一行代码来做到这一点。
- en: '[PRE5]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: so that the pseudorandom number generator is reset by using the system state,
    typically read from */dev/urandom*. Now when we run again, we’ll get different
    results for the tree models, as before.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 这样伪随机数生成器通过使用系统状态（通常从 */dev/urandom* 读取）来重置。现在，当我们再次运行时，我们会得到不同的树模型结果，正如之前一样。
- en: Adding k-fold Validation
  id: totrans-216
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 添加 *k*-折交叉验证
- en: To implement *k*-fold validation, we first need to pick a value for *k*. Our
    dataset has 569 samples. We want to split it so that there are a decent number
    of samples per fold because we want to make the test set a reasonable representation
    of the data. This argues toward making *k* small. However, we also want to average
    out the effect of a bad split, so we might want *k* to be larger. As with most
    things in life, a balance must be sought. If we set *k* = 5, we’ll get 113 samples
    per split (ignoring the final four samples, which should have no meaningful impact).
    This leaves 80 percent for training and 20 percent for test for each combination
    of folds, a reasonable thing to do. So, we’ll use *k* = 5, but we’ll write our
    code so that we can vary *k* if we want.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现 *k* 折交叉验证，我们首先需要选择一个 *k* 的值。我们的数据集有 569 个样本。我们希望将其拆分，以便每一折都有足够的样本，因为我们希望测试集能够合理地代表数据。这就倾向于选择一个较小的
    *k* 值。然而，我们也希望平衡一个不好的拆分影响，所以我们可能希望 *k* 较大。就像生活中的大多数事情一样，需要寻找平衡。如果我们设置 *k* = 5，我们将得到每一折
    113 个样本（忽略最后的四个样本，这些样本应该没有实质性影响）。这会为每种折叠组合保留 80% 用于训练，20% 用于测试，这是一个合理的做法。所以，我们将使用
    *k* = 5，但我们会编写代码，使得如果需要的话可以调整 *k* 的值。
- en: We already have an approach for training multiple models on a train/ test split.
    All we need to add is code to generate each of the *k* folds and then train the
    models on them. The code is in [Listing 7-4](ch07.xhtml#ch7lis4) and [Listing
    7-5](ch07.xhtml#ch7lis5), which show the helper functions and `main` function,
    respectively. Let’s start with [Listing 7-4](ch07.xhtml#ch7lis4).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经有了一种方法来训练多个模型并进行训练/测试拆分。我们需要做的就是添加生成每个 *k* 折叠的代码，然后在它们上面训练模型。代码位于[列表 7-4](ch07.xhtml#ch7lis4)和[列表
    7-5](ch07.xhtml#ch7lis5)，分别显示了辅助函数和 `main` 函数。我们从[列表 7-4](ch07.xhtml#ch7lis4)开始。
- en: import numpy as np
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: from sklearn.neighbors import NearestCentroid
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.neighbors import NearestCentroid
- en: from sklearn.neighbors import KNeighborsClassifier
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.neighbors import KNeighborsClassifier
- en: from sklearn.naive_bayes import GaussianNB, MultinomialNB
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.naive_bayes import GaussianNB, MultinomialNB
- en: from sklearn.tree import DecisionTreeClassifier
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.tree import DecisionTreeClassifier
- en: from sklearn.ensemble import RandomForestClassifier
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.ensemble import RandomForestClassifier
- en: from sklearn.svm import SVC
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.svm import SVC
- en: import sys
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: import sys
- en: 'def run(x_train, y_train, x_test, y_test, clf):'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 'def run(x_train, y_train, x_test, y_test, clf):'
- en: clf.fit(x_train, y_train)
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: clf.fit(x_train, y_train)
- en: return clf.score(x_test, y_test)
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: return clf.score(x_test, y_test)
- en: 'def split(x,y,k,m):'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 'def split(x,y,k,m):'
- en: ❶ ns = int(y.shape[0]/m)
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ ns = int(y.shape[0]/m)
- en: s = []
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: s = []
- en: 'for i in range(m):'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i in range(m):'
- en: ❷ s.append([x[(ns*i):(ns*i+ns)],
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ s.append([x[(ns*i):(ns*i+ns)],
- en: y[(ns*i):(ns*i+ns)]])
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: y[(ns*i):(ns*i+ns)]])
- en: x_test, y_test = s[k]
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: x_test, y_test = s[k]
- en: x_train = []
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: x_train = []
- en: y_train = []
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: y_train = []
- en: 'for i in range(m):'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i in range(m):'
- en: 'if (i==k):'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 'if (i==k):'
- en: continue
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: continue
- en: 'else:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 'else:'
- en: a,b = s[i]
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: a,b = s[i]
- en: x_train.append(a)
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: x_train.append(a)
- en: y_train.append(b)
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: y_train.append(b)
- en: ❸ x_train = np.array(x_train).reshape(((m-1)*ns,30))
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ x_train = np.array(x_train).reshape(((m-1)*ns,30))
- en: y_train = np.array(y_train).reshape((m-1)*ns)
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: y_train = np.array(y_train).reshape((m-1)*ns)
- en: return [x_train, y_train, x_test, y_test]
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: return [x_train, y_train, x_test, y_test]
- en: 'def pp(z,k,s):'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 'def pp(z,k,s):'
- en: m = z.shape[1]
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: m = z.shape[1]
- en: 'print("%-19s: %0.4f +/- %0.4f | " % (s, z[k].mean(),'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 'print("%-19s: %0.4f +/- %0.4f | " % (s, z[k].mean(),'
- en: z[k].std()/np.sqrt(m)), end='')
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: z[k].std()/np.sqrt(m)), end='')
- en: 'for i in range(m):'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i in range(m):'
- en: print("%0.4f " % z[k,i], end='')
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: print("%0.4f " % z[k,i], end='')
- en: print()
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: print()
- en: '*Listing 7-4: Using k-fold validation to evaluate the breast cancer dataset.
    Helper functions. See* bc_kfold.py.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 7-4：使用 *k*-折交叉验证评估乳腺癌数据集。辅助函数。见* bc_kfold.py。'
- en: '[Listing 7-4](ch07.xhtml#ch7lis4) begins by including all the modules we used
    before and then defines three functions: `run`, `split`, and `pp`. The `run` function
    looks familiar. It takes a train set, test set, and model instance, trains the
    model, and then scores the model against the test set. The `pp` function is a
    pretty-print function to show the per split scores along with the average score
    across all the splits. The average is shown as the mean ± the standard error of
    the mean. Recall that an sklearn score is the overall accuracy of the model on
    the test set, or the fraction of times that the model predicted the actual class
    of the test sample. Perfection is a score of 1.0, and complete failure is 0.0\.
    Complete failure is rare because even random guessing will get it right some fraction
    of the time.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '[Listing 7-4](ch07.xhtml#ch7lis4) 首先包含了我们之前使用的所有模块，然后定义了三个函数：`run`、`split`
    和 `pp`。`run` 函数看起来很熟悉。它接收训练集、测试集和模型实例，训练模型并在测试集上对模型进行评分。`pp` 函数是一个漂亮的打印函数，用于显示每个分割的得分以及所有分割的平均得分。平均值以均值
    ± 标准误差的形式显示。回想一下，sklearn 的得分是模型在测试集上的总体准确率，或者说模型预测测试样本实际类别的次数比例。完美的得分是 1.0，完全失败的得分是
    0.0。完全失败是罕见的，因为即使是随机猜测，也会在一定比例的情况下正确预测。'
- en: The only interesting function in [Listing 7-4](ch07.xhtml#ch7lis4) is `split`.
    Its arguments are the full dataset, `x`, the corresponding labels, `y`, the current
    fold number, `k`, and the total number of folds, `m`. We’ll divide the full dataset
    into *m* distinct sets, the folds, and use the *k*-th fold as test while merging
    the remaining *m –* 1 folds into a new training set. First, we set the number
    of samples per fold ❶. The loop then creates a list of folds, `s`. Each element
    of this list contains the feature vectors and labels of the fold ❷.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '[Listing 7-4](ch07.xhtml#ch7lis4) 中唯一有趣的函数是 `split`。它的参数包括完整数据集 `x`、对应的标签 `y`、当前的折数
    `k` 和总的折数 `m`。我们将把完整数据集分成 *m* 个不同的子集，称为折，并使用第 *k* 个折作为测试集，同时将其余的 *m –* 1 个折合并成新的训练集。首先，我们设置每个折的样本数量
    ❶。然后循环创建一个折的列表 `s`，该列表的每个元素包含了折的特征向量和标签 ❷。'
- en: The test set is simple, it’s the *k*-th fold, so we set those values next (`x_test`,
    `y_test`). The loop then takes the remaining *m –* 1 folds and merges them into
    a new training set, `x_train`, with labels, `y_train`.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 测试集很简单，它就是第 *k* 个折，所以接下来我们设置这些值（`x_test`，`y_test`）。然后循环将剩余的 *m –* 1 个折合并成新的训练集
    `x_train`，并附上标签 `y_train`。
- en: The two lines after the loop are a bit mysterious ❸. When the loop ends, `x_train`
    is a *list*, each element of which is a list representing the feature vectors
    of the fold we want in the training set. So we first make a NumPy array of this
    list and then reshape it so that `x_train` has 30 columns, the number of features
    per vector, and *n*[*s*](*m –* 1) rows, where *n*[*s*] is the number of samples
    per fold. Thus `x_train` becomes `x` minus the samples we put into the test fold,
    those of the *k*-th fold. We also build `y_train` so that the correct label goes
    with each the feature vector in `x_train`.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 循环结束后，接下来的两行代码有点神秘 ❸。当循环结束时，`x_train` 是一个 *列表*，其中的每个元素都是代表我们希望放入训练集的折的特征向量列表。因此，我们首先将这个列表转换为
    NumPy 数组，然后对其进行重塑，使得 `x_train` 具有 30 列，每个向量的特征数量，并且有 *n*[*s*](*m –* 1) 行，其中 *n*[*s*]
    是每个折中的样本数量。这样，`x_train` 就变成了 `x` 减去我们放入测试集中的样本，即第 *k* 个折的样本。我们还构建了 `y_train`，使得每个特征向量在
    `x_train` 中都有相应的正确标签。
- en: '[Listing 7-5](ch07.xhtml#ch7lis5) shows us how to use the helper functions.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '[Listing 7-5](ch07.xhtml#ch7lis5) 向我们展示了如何使用辅助函数。'
- en: 'def main():'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 'def main():'
- en: x = np.load("../data/breast/bc_features_standard.npy")
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: x = np.load("../data/breast/bc_features_standard.npy")
- en: y = np.load("../data/breast/bc_labels.npy")
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: y = np.load("../data/breast/bc_labels.npy")
- en: idx = np.argsort(np.random.random(y.shape[0]))
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: idx = np.argsort(np.random.random(y.shape[0]))
- en: x = x[idx]
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: x = x[idx]
- en: y = y[idx]
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: y = y[idx]
- en: ❶ m = int(sys.argv[1])
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ m = int(sys.argv[1])
- en: z = np.zeros((8,m))
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: z = np.zeros((8,m))
- en: 'for k in range(m):'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 'for k in range(m):'
- en: x_train, y_train, x_test, y_test = split(x,y,k,m)
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: x_train, y_train, x_test, y_test = split(x,y,k,m)
- en: z[0,k] = run(x_train, y_train, x_test, y_test,
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: z[0,k] = run(x_train, y_train, x_test, y_test,
- en: NearestCentroid())
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: NearestCentroid())
- en: z[1,k] = run(x_train, y_train, x_test, y_test,
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: z[1,k] = run(x_train, y_train, x_test, y_test,
- en: KNeighborsClassifier(n_neighbors=3))
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: KNeighborsClassifier(n_neighbors=3))
- en: z[2,k] = run(x_train, y_train, x_test, y_test,
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: z[2,k] = run(x_train, y_train, x_test, y_test,
- en: KNeighborsClassifier(n_neighbors=7))
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: KNeighborsClassifier(n_neighbors=7))
- en: z[3,k] = run(x_train, y_train, x_test, y_test,
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: z[3,k] = run(x_train, y_train, x_test, y_test,
- en: GaussianNB())
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: GaussianNB())
- en: z[4,k] = run(x_train, y_train, x_test, y_test,
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: z[4,k] = run(x_train, y_train, x_test, y_test,
- en: DecisionTreeClassifier())
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: DecisionTreeClassifier())
- en: z[5,k] = run(x_train, y_train, x_test, y_test,
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: z[5,k] = run(x_train, y_train, x_test, y_test,
- en: RandomForestClassifier(n_estimators=5))
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林分类器(n_estimators=5))
- en: z[6,k] = run(x_train, y_train, x_test, y_test,
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: z[6,k] = run(x_train, y_train, x_test, y_test,
- en: RandomForestClassifier(n_estimators=50))
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林分类器(n_estimators=50))
- en: z[7,k] = run(x_train, y_train, x_test, y_test,
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: z[7,k] = run(x_train, y_train, x_test, y_test,
- en: SVC(kernel="linear", C=1.0))
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: SVC(kernel="linear", C=1.0))
- en: pp(z,0,"Nearest"); pp(z,1,"3-NN")
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: pp(z,0,"最近"); pp(z,1,"3-NN")
- en: pp(z,2,"7-NN");    pp(z,3,"Naive Bayes")
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: pp(z,2,"7-NN");    pp(z,3,"朴素贝叶斯")
- en: pp(z,4,"Decision Tree");    pp(z,5,"Random Forest (5)")
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: pp(z,4,"决策树");    pp(z,5,"随机森林 (5)")
- en: pp(z,6,"Random Forest (50)");    pp(z,7,"SVM (linear)")
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: pp(z,6,"随机森林 (50)");    pp(z,7,"支持向量机 (线性)")
- en: '*Listing 7-5: Using k-fold validation to evaluate the breast cancer dataset.
    Main code. See* bc_kfold.py.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 7-5：使用 k-fold 验证评估乳腺癌数据集。主代码。参见* bc_kfold.py。'
- en: The first thing we do in `main` is load the full dataset and randomize the ordering.
    The number of folds, `m`, is read from the command line ❶ and used to create the
    output array, `z`. This array holds the per fold scores for each of the eight
    models we’ll train, so it has shape 8 × *m*. Recall, when running a Python script
    from the command line, any arguments passed after the script name are available
    in `sys.argv`, a list of strings. This is why the argument is passed to `int`
    to convert it to an integer ❶.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `main` 中，我们首先加载完整的数据集并随机化排序。折叠数 `m` 从命令行读取 ❶，并用于创建输出数组 `z`。该数组保存我们将训练的八个模型的每个折叠得分，因此其形状为
    8 × *m*。回想一下，当从命令行运行 Python 脚本时，任何在脚本名后传递的参数都可以在 `sys.argv` 中作为字符串列表使用。这就是为什么参数被传递给
    `int` 来转换为整数 ❶。
- en: Next, we loop over the *m* folds, where *k* is the fold that we’ll be using
    for test data. We create the split and then use the split to train the eight model
    types we trained previously. Each call to `run` trains a model of the type passed
    in and returns the score found by running that model against the *k*-th fold as
    test data. We store these results in `z`. Finally, we use `pp` to display the
    per model type and per fold scores along with the average score over all the folds.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们对 *m* 个折叠进行循环，其中 *k* 是我们将用作测试数据的折叠。我们创建分割，然后使用该分割训练之前训练的八种模型类型。每次调用 `run`
    都会训练一个传入类型的模型，并返回该模型在 *k* 号折叠上作为测试数据时的得分。我们将这些结果存储在 `z` 中。最后，我们使用 `pp` 显示每种模型类型和每个折叠的得分，以及所有折叠的平均得分。
- en: A sample run of this code, for *k* = 5 and showing only the mean score across
    folds, gives the results in [Table 7-4](ch07.xhtml#ch7tab4).
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码的一个示例运行，对于 *k* = 5，并且仅显示跨折叠的平均得分，结果见[表 7-4](ch07.xhtml#ch7tab4)。
- en: '**Table 7-4:** Breast Cancer Scores as Mean Over Five Folds'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 7-4：** 乳腺癌得分的五折平均值'
- en: '| **Model** | **Mean** ± **SE** |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| **模型** | **平均值** ± **标准误差** |'
- en: '| --- | --- |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Nearest Centroid | 0.9310 ± 0.0116 |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| 最近质心 | 0.9310 ± 0.0116 |'
- en: '| 3-NN | 0.9735 ± 0.0035 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| 3-NN | 0.9735 ± 0.0035 |'
- en: '| 7-NN | 0.9717 ± 0.0039 |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| 7-NN | 0.9717 ± 0.0039 |'
- en: '| Naïve Bayes | 0.9363 ± 0.0140 |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| 朴素贝叶斯 | 0.9363 ± 0.0140 |'
- en: '| Decision Tree | 0.9027 ± 0.0079 |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| 决策树 | 0.9027 ± 0.0079 |'
- en: '| Random Forest (5) | 0.9540 ± 0.0107 |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林 (5) | 0.9540 ± 0.0107 |'
- en: '| Random Forest (50) | 0.9540 ± 0.0077 |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林 (50) | 0.9540 ± 0.0077 |'
- en: '| SVM (linear) | 0.9699 ± 0.0096 |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| 支持向量机 (线性) | 0.9699 ± 0.0096 |'
- en: Here we’re showing the average performance of each model over all folds. One
    way to understand the results is that this is the sort of performance we should
    expect, per model type, if we were to train the model using *all* of the data
    in the dataset and test it against new samples from the same parent distribution.
    Indeed, in practice, we would do just this, as we can assume that the reason behind
    making the model in the first place is to use it for some purpose going forward.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们展示了每个模型在所有折叠上的平均表现。一种理解这些结果的方法是，如果我们使用数据集中的 *所有* 数据训练模型，并将其测试于来自同一父分布的新样本，那么每种模型类型的表现大致如此。实际上，我们通常会这样做，因为我们可以假设构建模型的原因本身就是为了将来用于某些目的。
- en: Run the code a second time with *k* = 5\. A new set of outputs appears. This
    is because we’re randomizing the order of the dataset on every run ([Listing 7-5](ch07.xhtml#ch7lis5)).
    This makes a new set of splits and implies that each model will be trained on
    a different subset mix of the full dataset on each run. So, we should expect different
    results. Let’s run the code 1,000 times with *k* = 5\. Note, training this many
    models takes about 20 minutes on a very standard desktop computer. For each run
    we’ll get an average score over the five folds. We then compute the mean of these
    averages, which is known as the *grand mean*. [Table 7-5](ch07.xhtml#ch7tab5)
    shows the results.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 再次运行代码，*k* = 5。此时会出现一组新的输出。这是因为我们在每次运行时都会随机化数据集的顺序（[列表 7-5](ch07.xhtml#ch7lis5)）。这会产生一组新的拆分，意味着每个模型每次运行时都将在数据集的不同子集上进行训练。所以，我们应该期待不同的结果。让我们将代码运行
    1,000 次，*k* = 5。请注意，在一台标准台式机上训练这么多模型大约需要 20 分钟。对于每次运行，我们将获得五折交叉验证的平均分数。然后我们计算这些平均值的均值，这称为*大均值*。[表
    7-5](ch07.xhtml#ch7tab5)显示了结果。
- en: '**Table 7-5:** Breast Cancer Scores as Grand Mean Over 1,000 Runs with Five
    Folds'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 7-5：** 乳腺癌得分作为 1,000 次运行中的大均值，包含五折交叉验证'
- en: '| **Model** | **Grand mean** ± **SE** |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| **模型** | **大均值** ± **标准误差** |'
- en: '| --- | --- |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Nearest Centroid | 0.929905 ± 0.000056 |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| 最近质心 | 0.929905 ± 0.000056 |'
- en: '| 3-NN | 0.966334 ± 0.000113 |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| 3-NN | 0.966334 ± 0.000113 |'
- en: '| 7-NN | 0.965496 ± 0.000110 |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| 7-NN | 0.965496 ± 0.000110 |'
- en: '| Naïve Bayes | 0.932973 ± 0.000095 |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| 朴素贝叶斯 | 0.932973 ± 0.000095 |'
- en: '| Decision Tree | 0.925706 ± 0.000276 |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| 决策树 | 0.925706 ± 0.000276 |'
- en: '| Random Forest (5) | 0.948378 ± 0.000213 |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林（5） | 0.948378 ± 0.000213 |'
- en: '| Random Forest (50) | 0.958845 ± 0.000135 |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林（50） | 0.958845 ± 0.000135 |'
- en: '| SVM (linear) | 0.971871 ± 0.000136 |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| SVM（线性） | 0.971871 ± 0.000136 |'
- en: We can take these grand means as an indication of how well we’d expect each
    model to do against a new set of unknown feature vectors. The small standard errors
    of the mean are an indication of how well the mean value is known, not how well
    a model of that type trained on a dataset will necessarily perform. We use the
    grand mean to help us order the models so we can select one over another.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这些大均值作为每个模型在未知特征向量的新集合上可能表现的指示。较小的标准误差表示均值的了解程度，而不是该类型的模型在数据集上训练后的实际表现。我们使用大均值帮助我们排序模型，从而选择最合适的模型。
- en: 'Ranking the models from highest score to lowest gives the following:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 根据得分从高到低对模型进行排名，结果如下：
- en: SVM (linear)
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: SVM（线性）
- en: '*k*-NN (*k* = 3)'
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*k*-NN（*k* = 3）'
- en: '*k*-NN (*k* = 7)'
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*k*-NN（*k* = 7）'
- en: Random Forest (50)
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机森林（50）
- en: Random Forest (5)
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机森林（5）
- en: Naïve Bayes (Gaussian)
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 朴素贝叶斯（高斯）
- en: Nearest Centroid
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最近质心
- en: Decision Tree
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 决策树
- en: This is interesting given that we might expect the SVM to be best, but would
    likely assume the Random Forests to do better than *k*-NN. The Decision Tree was
    not as good as we thought, and was less accurate than the Nearest Centroid classifier.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 这很有趣，因为我们可能预期 SVM 会表现最好，但可能会认为随机森林会比*k*-NN表现更好。决策树的表现不如我们预期，准确度低于最近质心分类器。
- en: Some comments are in order here. First, note that these results are derived
    from the training of 8,000 different models on 1,000 different orderings of the
    dataset. When we study neural networks, we’ll see much longer training times.
    Experimenting with classical machine learning models is generally easy to do since
    each change to a parameter doesn’t require a lengthy training session.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里需要做一些说明。首先，注意这些结果是从在数据集的 1,000 种不同排列上训练的 8,000 个不同模型中得出的。当我们研究神经网络时，会看到更长的训练时间。实验经典机器学习模型通常很容易进行，因为每次修改参数时不需要长时间的训练。
- en: Second, we didn’t try to optimize any of the model hyperparameters. Some of
    these hyperparameters are indirect, like assuming that the features are normally
    distributed so that the Gaussian Naïve Bayes classifier is a reasonable choice,
    while others are numerical, like the number of neighbors in *k*-NN or the number
    of trees in a Random Forest. If we want to thoroughly develop a good classifier
    for this dataset using a classic model, we’ll have to explore some of these hyperparameters.
    Ideally, we’d repeat the experiments many, many times for each new hyperparameter
    setting to arrive at a tight mean value for the score, as we have previously with
    the grand means over 1,000 runs. We’ll play a bit more with hyperparameters in
    the next section, where we see how we can search for good ones that work well
    with our dataset.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们没有尝试优化任何模型超参数。这些超参数中有些是间接的，比如假设特征服从正态分布，以便高斯朴素贝叶斯分类器是一个合理的选择，而有些则是数值型的，例如*k*-NN中的邻居数或随机森林中的树木数。如果我们想要使用经典模型为这个数据集彻底开发一个好的分类器，我们就需要探索一些这些超参数。理想情况下，我们会针对每一个新的超参数设置，重复实验很多次，以便得到一个紧密的均值分数，正如我们以前在1,000次运行中的大均值所做的那样。在下一节中，我们将更多地玩转超参数，看看如何找到适合我们数据集的良好超参数。
- en: Searching for Hyperparameters
  id: totrans-333
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 搜索超参数
- en: Let’s explore the effect of some of the hyperparameters on various model types.
    Specifically, let’s see if we can optimize our choice of *k* for *k*-NN, forest
    size for Random Forest, and the *C* margin size of the linear SVM.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索一些超参数对各种模型类型的影响。具体来说，我们来看看是否能优化*k*-NN中的*k*选择、随机森林中的森林大小以及线性SVM中的*C*边界大小。
- en: Fine-Tuning Our k-NN Classifier
  id: totrans-335
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 微调我们的*k*-NN分类器
- en: Because the number of neighbors in a *k*-NN classifier is an integer, typically
    odd, it’s straightforward to repeat our five-fold cross validation experiment
    while varying *k* for *k* ∈ *{*1,3,5,7,9,11,13,15*}*. To do this, we need only
    change the main loop in [Listing 7-5](ch07.xhtml#ch7lis5) so that each call to
    `run` uses `KNeighborsClassifier` with a different number of neighbors, as follows.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 因为*k*-NN分类器中的邻居数是一个整数，通常是奇数，所以很容易在变化*k* ∈ *{*1,3,5,7,9,11,13,15*}*时，重复我们的五折交叉验证实验。为此，我们只需更改[列表7-5](ch07.xhtml#ch7lis5)中的主循环，使得每次调用`run`时使用不同数量的邻居的`KNeighborsClassifier`，如下所示。
- en: '[PRE6]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The grand mean of the scores for 1,000 repetitions of the five-fold cross-validation
    code using a different random ordering of the full dataset each time gives the
    results in [Table 7-6](ch07.xhtml#ch7tab6).
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 1,000次重复五折交叉验证代码的得分大均值，使用每次不同的随机排序整个数据集，结果见[表7-6](ch07.xhtml#ch7tab6)。
- en: '**Table 7-6:** Breast Cancer Scores as Grand Mean for Different k Values and
    Five-Fold Validation'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '**表7-6：** 不同k值和五折验证下乳腺癌评分的均值'
- en: '| ***k*** | **Grand mean** ± **SE** |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| ***k*** | **大均值** ± **标准误** |'
- en: '| --- | --- |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 1 | 0.951301 ± 0.000153 |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.951301 ± 0.000153 |'
- en: '| 3 | 0.966282 ± 0.000112 |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0.966282 ± 0.000112 |'
- en: '| 5 | 0.965998 ± 0.000097 |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 0.965998 ± 0.000097 |'
- en: '| 7 | 0.96520 ± 0.000108 |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 0.96520 ± 0.000108 |'
- en: '| **9** | **0.967011** ± **0.000100** |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| **9** | **0.967011** ± **0.000100** |'
- en: '| 11 | 0.965069 ± 0.000107 |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| 11 | 0.965069 ± 0.000107 |'
- en: '| 13 | 0.962400 ± 0.000106 |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| 13 | 0.962400 ± 0.000106 |'
- en: '| 15 | 0.959976 ± 0.000101 |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| 15 | 0.959976 ± 0.000101 |'
- en: We’ve highlighted the *k* = 9 because it returned the highest score. This indicates
    that we might want to use *k* = 9 for this dataset.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 我们强调*k* = 9，因为它返回了最高的分数。这表明我们可能需要在这个数据集上使用*k* = 9。
- en: Fine-Tuning Our Random Forest
  id: totrans-351
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 微调我们的随机森林
- en: Let’s look at the Random Forest model. The sklearn `RandomForestClassifier`
    class has quite a few hyperparameters that we could manipulate. To avoid being
    excessively pedantic, we’ll seek only an optimal number of trees in the forest.
    This is the `n_estimators` parameter. As we did for *k* in *k*-NN, we’ll search
    over a range of forest sizes and select the one that gives the best grand mean
    score for 1,000 runs at five folds each per run.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看随机森林模型。sklearn的`RandomForestClassifier`类有相当多的超参数可以调整。为了避免过于繁琐，我们只寻找森林中树木的最佳数量。这就是`n_estimators`参数。就像在*k*-NN中调整*k*一样，我们将搜索不同的森林大小范围，选择在每次运行五折交叉验证中1,000次运行后，获得最佳大均值分数的森林大小。
- en: This is a one-dimensional grid-like search. We varied *k* by one, but for the
    number of trees in the forest, we need to cover a larger scale. We don’t expect
    there to be a meaningful difference between 10 trees in the forest or 11, especially
    considering that each Random Forest training session will lead to a different
    set of trees even if the number of trees is fixed. We saw this effect several
    times in the previous section. Instead, let’s vary the number of trees by selecting
    from *n*[*t*] ∈ *{*5,20,50,100,200,500,1000,5000*}* where *n*[*t*] is the number
    of trees in the forest (number of estimators). Running this search gives us the
    grand means in [Table 7-7](ch07.xhtml#ch7tab7).
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个一维网格搜索。我们通过一变量*k*进行了变化，但对于森林中的树木数量，我们需要覆盖更大的范围。我们不希望在森林中使用10棵树或11棵树之间存在有意义的差异，特别是考虑到即使树木数量固定，每次随机森林训练会导致不同的树组合。在上一节中我们多次看到了这种效果。相反，让我们通过选择*n*[t]
    ∈ *{5, 20, 50, 100, 200, 500, 1000, 5000}*，其中*n*[t]是森林中的树木数量（评估器数量）来变化树的数量。运行这个搜索给我们在[表格
    7-7](ch07.xhtml#ch7tab7)中的总平均值。
- en: '**Table 7-7:** Breast Cancer Scores as Grand Mean for Different Random Forest
    Sizes and Five-Fold Validation'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '**表格 7-7：** 不同随机森林大小及五折交叉验证的乳腺癌评分总平均'
- en: '| ***n[t]*** | **Grand mean** ± **SE** |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| ***n[t]*** | **总平均** ± **SE** |'
- en: '| --- | --- |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 5 | 0.948327 ± 0.000206 |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 0.948327 ± 0.000206 |'
- en: '| 20 | 0.956808 ±0.000166 |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| 20 | 0.956808 ±0.000166 |'
- en: '| 50 | 0.959048 ± 0.000139 |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| 50 | 0.959048 ± 0.000139 |'
- en: '| 100 | 0.959740 ± 0.000130 |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| 100 | 0.959740 ± 0.000130 |'
- en: '| 200 | 0.959913 ± 0.000122 |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| 200 | 0.959913 ± 0.000122 |'
- en: '| 500 | 0.960049 ± 0.000117 |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| 500 | 0.960049 ± 0.000117 |'
- en: '| 750 | 0.960147 ± 0.000118 |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| 750 | 0.960147 ± 0.000118 |'
- en: '| 1000 | 0.960181 ± 0.000116 |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| 1000 | 0.960181 ± 0.000116 |'
- en: The first thing to notice is that the differences are very small, though if
    you run the Mann–Whitney U test, you’ll see that the difference between *n*[*t*]
    = 5 (worst) and *n*[*t*] = 1000 (best) is statistically significant. However,
    the difference between *n*[*t*] = 200 and *n*[*t*] = 1000 is not significant.
    Here we need to make a judgment call. Setting *n*[*t*] = 1000 did give the best
    result but it’s indistinguishable, for practical purposes, from *n*[*t*] = 500
    or even *n*[*t*] = 100\. Since runtime for a Random Forest scales linearly in
    the number of trees, using *n*[*t*] = 100 results in a classifier that is on average
    10× faster than using *n*[*t*] = 1000\. So, depending upon the task, we might
    select *n*[*t*] = 100 over *n*[*t*] = 1000 for that reason.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要注意的是，差异非常小，尽管如果进行曼-惠特尼U检验，您会发现*n*[t] = 5（最差）和*n*[t] = 1000（最佳）之间的差异在统计上是显著的。然而，*n*[t]
    = 200和*n*[t] = 1000之间的差异不显著。在这里，我们需要做出判断。设置*n*[t] = 1000确实给出了最佳结果，但从实际角度来看，与*n*[t]
    = 500甚至*n*[t] = 100相比，几乎无法区分。由于随机森林的运行时间与树木数量成线性关系，使用*n*[t] = 100的分类器平均比使用*n*[t]
    = 1000的快10倍。因此，根据任务的不同，我们可能因此选择*n*[t] = 100而不是*n*[t] = 1000。
- en: Fine-Tuning Our SVMs
  id: totrans-366
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 微调我们的SVMs
- en: Let’s turn our attention to the linear SVM. For the linear kernel, we’ll adjust
    *C*. Note, sklearn has other parameters, as it did for the Random Forest, but
    we’ll leave them at their default settings.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们把注意力转向线性支持向量机（SVM）。对于线性核函数，我们将调整*C*。请注意，scikit-learn有其他参数，就像随机森林一样，但我们将保留它们的默认设置。
- en: What range of *C* should we search over? The answer is problem dependent but
    the sklearn default value of *C* = 1 is a good starting point. We’ll select *C*
    values around 1 but over several orders of magnitude. Specifically, we’ll select
    from *C* ∈ *{*0.001,0.01,0.1,1.0,2.0,10.0,50.0,100.0*}*. Running one thousand
    five-fold validations, each for a different random ordering of the full dataset,
    gives grand means as shown in [Table 7-8](ch07.xhtml#ch7tab8).
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该搜索哪个*C*的范围？答案取决于问题，但scikit-learn的默认值*C* = 1是一个很好的起点。我们将选择*C*在*{0.001, 0.01,
    0.1, 1.0, 2.0, 10.0, 50.0, 100.0}*范围内。进行一千次五折验证，每次对完整数据集进行不同的随机排序，得到如[表格 7-8](ch07.xhtml#ch7tab8)所示的总平均值。
- en: '**Table 7-8:** Breast Cancer Scores as Grand Mean for Different SVM C Values
    and Five-Fold Validation'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '**表格 7-8：** 不同SVM C值的乳腺癌评分及五折交叉验证的总平均'
- en: '| **C** | **Grand mean** ± **SE** |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| **C** | **总平均** ± **SE** |'
- en: '| --- | --- |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 0.001 | 0.938500 ± 0.000066 |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| 0.001 | 0.938500 ± 0.000066 |'
- en: '| 0.01 | 0.967151 ± 0.000089 |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| 0.01 | 0.967151 ± 0.000089 |'
- en: '| 0.1 | 0.975943 ± 0.000101 |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| 0.1 | 0.975943 ± 0.000101 |'
- en: '| 1.0 | 0.971890 ± 0.000141 |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| 1.0 | 0.971890 ± 0.000141 |'
- en: '| 2.0 | 0.969994 ± 0.000144 |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| 2.0 | 0.969994 ± 0.000144 |'
- en: '| 10.0 | 0.966239 ± 0.000154 |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| 10.0 | 0.966239 ± 0.000154 |'
- en: '| 50.0 | 0.959637 ± 0.000186 |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| 50.0 | 0.959637 ± 0.000186 |'
- en: '| 100.0 | 0.957006 ± 0.000189 |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| 100.0 | 0.957006 ± 0.000189 |'
- en: '*C* = 0.1 gives the best accuracy. While, statistically, the difference between
    *C* = 0.1 and *C* = 1 is meaningful, in practice the difference is only about
    0.4 percent, so the default value of *C* = 1 would likewise be a reasonable choice.
    Further refinement of *C* is possible because we see that *C* = 0.01 and *C* =
    2 give the same accuracy, while *C* = 0.1 is higher than either, implying that
    if the *C* curve is smooth, there’s a maximum accuracy for some *C* in [0.01,2.0].'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '*C* = 0.1 给出了最佳准确度。虽然从统计学上讲，*C* = 0.1 与 *C* = 1 之间的差异是有意义的，但在实践中，差异仅约为 0.4％，因此
    *C* = 1 的默认值同样是一个合理的选择。对*C*进行进一步优化是可能的，因为我们发现*C* = 0.01 和 *C* = 2 的准确度相同，而 *C*
    = 0.1 的准确度高于它们，意味着如果*C*曲线是平滑的，那么在[0.01,2.0]范围内某个*C*值会达到最大的准确度。'
- en: 'Finding the right *C* for our dataset is a crucial part of successfully using
    a linear SVM. Our preceding rough run used a one-dimensional grid search. We do
    expect, since *C* is continuous, that a plot of the accuracy as a function of
    *C* will also be smooth. If that’s the case, one can imagine searching for the
    right *C*, not with a grid search but with an optimization algorithm. In practice,
    however, the randomness of the ordering of the dataset and its effect on the output
    of *k*-fold cross-validation results will probably make any *C* found by an optimization
    algorithm too specific to the problem at hand. Grid search over a larger scale,
    with possibly one level of refinement, is sufficient in most cases. The take-home
    message is: do spend some time looking for the proper *C* value to maximize the
    effectiveness of the linear SVM.'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 找到适合我们数据集的*C*是成功使用线性支持向量机（SVM）的关键部分。我们之前的粗略运行使用了一维网格搜索。由于*C*是连续的，我们预期准确度与*C*的关系图也会是平滑的。如果是这样的话，可以设想通过优化算法来搜索合适的*C*，而不是使用网格搜索。然而，在实践中，数据集的排序随机性及其对*k*折交叉验证结果的影响，可能使得通过优化算法找到的任何*C*值都过于特定于当前问题。对更大范围的网格进行搜索，并进行可能的一次精细化，通常在大多数情况下已足够。关键的信息是：确实需要花些时间来寻找合适的*C*值，以最大化线性SVM的效果。
- en: Observant readers will have noticed that the preceding analysis has ignored
    the RBF kernel SVM. Let’s revisit it now and see how to do a simple two-dimensional
    grid search over *C* and *γ*, where *γ* is the parameter associated with the RBF
    (Gaussian) kernel. sklearn has the `GridSearchCV` class to perform sophisticated
    grid searching. We’re not using it here to be pedagogical and show how to do simple
    grid searches directly. It’s especially important for this kernel to select good
    values for both of these parameters.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 细心的读者会注意到，前面的分析忽略了 RBF 核心支持向量机（SVM）。我们现在重新审视一下，并看看如何对*C*和*γ*进行简单的二维网格搜索，其中*γ*是与RBF（高斯）核相关的参数。sklearn有一个`GridSearchCV`类可以执行复杂的网格搜索。我们在这里不使用它，是为了教学目的，展示如何直接进行简单的网格搜索。对于这个核来说，选择这两个参数的好值尤其重要。
- en: For the search, we’ll use the same range of *C* values as we used for the linear
    case. For *γ* we’ll use powers of two, 2^(*p*), times the sklearn default value,
    1/30 = 0.03333 for *p* ∈ [*–*4,3]. The search will, for the current *C* value,
    do five-fold validation over the dataset for each *γ* value before moving to the
    next *C* value so that all pairs of (*C*,*γ*) are considered. The pair that results
    in the largest score (accuracy) will be output. The code is in [Listing 7-6](ch07.xhtml#ch7lis6).
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 对于搜索，我们将使用与线性案例相同范围的*C*值。对于*γ*，我们将使用以二的幂次方为基础的值，2^(*p*)，乘以 sklearn 的默认值，1/30
    = 0.03333，其中*p* ∈ [*–*4,3]。对于当前的*C*值，搜索将在每个*γ*值上对数据集进行五折交叉验证，然后再转到下一个*C*值，以确保所有的(*C*,*γ*)组合都被考虑。最终结果将输出得分（准确度）最高的组合。代码见[Listing
    7-6](ch07.xhtml#ch7lis6)。
- en: import numpy as np
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: from sklearn.svm import SVC
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.svm import SVC
- en: 'def run(x_train, y_train, x_test, y_test, clf):'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 'def run(x_train, y_train, x_test, y_test, clf):'
- en: clf.fit(x_train, y_train)
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: clf.fit(x_train, y_train)
- en: return clf.score(x_test, y_test)
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: return clf.score(x_test, y_test)
- en: 'def split(x,y,k,m):'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 'def split(x,y,k,m):'
- en: ns = int(y.shape[0]/m)
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: ns = int(y.shape[0]/m)
- en: s = []
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: s = []
- en: 'for i in range(m):'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i in range(m):'
- en: s.append([x[(ns*i):(ns*i+ns)], y[(ns*i):(ns*i+ns)]])
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: s.append([x[(ns*i):(ns*i+ns)], y[(ns*i):(ns*i+ns)]])
- en: x_test, y_test = s[k]
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: x_test, y_test = s[k]
- en: x_train = []
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: x_train = []
- en: y_train = []
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: y_train = []
- en: 'for i in range(m):'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i in range(m):'
- en: 'if (i==k):'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 'if (i==k):'
- en: continue
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: continue
- en: 'else:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 'else:'
- en: a,b = s[i]
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: a,b = s[i]
- en: x_train.append(a)
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: x_train.append(a)
- en: y_train.append(b)
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: y_train.append(b)
- en: x_train = np.array(x_train).reshape(((m-1)*ns,30))
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: x_train = np.array(x_train).reshape(((m-1)*ns,30))
- en: y_train = np.array(y_train).reshape((m-1)*ns)
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: y_train = np.array(y_train).reshape((m-1)*ns)
- en: return [x_train, y_train, x_test, y_test]
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: return [x_train, y_train, x_test, y_test]
- en: 'def main():'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 'def main():'
- en: m = 5
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: m = 5
- en: x = np.load("../data/breast/bc_features_standard.npy")
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: x = np.load("../data/breast/bc_features_standard.npy")
- en: y = np.load("../data/breast/bc_labels.npy")
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: y = np.load("../data/breast/bc_labels.npy")
- en: idx = np.argsort(np.random.random(y.shape[0]))
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: idx = np.argsort(np.random.random(y.shape[0]))
- en: x = x[idx]
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: x = x[idx]
- en: y = y[idx]
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: y = y[idx]
- en: ❶ Cs = np.array([0.01,0.1,1.0,2.0,10.0,50.0,100.0])
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ Cs = np.array([0.01,0.1,1.0,2.0,10.0,50.0,100.0])
- en: gs = (1./30)*2.0**np.array([-4,-3,-2,-1,0,1,2,3])
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: gs = (1./30)*2.0**np.array([-4,-3,-2,-1,0,1,2,3])
- en: zmax = 0.0
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: zmax = 0.0
- en: '❷ for C in Cs:'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 对于 C 在 Cs 中：
- en: 'for g in gs:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 g 在 gs 中：
- en: z = np.zeros(m)
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: z = np.zeros(m)
- en: 'for k in range(m):'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 k 在 range(m) 中：
- en: x_train, y_train, x_test, y_test = split(x,y,k,m)
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: x_train, y_train, x_test, y_test = split(x,y,k,m)
- en: z[k] = run(x_train, y_train, x_test, y_test,
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: z[k] = run(x_train, y_train, x_test, y_test,
- en: SVC(C=C,gamma=g,kernel="rbf"))
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: SVC(C=C,gamma=g,kernel="rbf"))
- en: '❸ if (z.mean() > zmax):'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 如果 (z.mean() > zmax)：
- en: zmax = z.mean()
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: zmax = z.mean()
- en: bestC = C
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: bestC = C
- en: bestg = g
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: bestg = g
- en: print("best C     = %0.5f" % bestC)
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: print("best C     = %0.5f" % bestC)
- en: print("     gamma = %0.5f" % bestg)
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: print("     gamma = %0.5f" % bestg)
- en: print("   accuracy= %0.5f" % zmax)
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: print("   准确度 = %0.5f" % zmax)
- en: '*Listing 7-6: A two-dimensional grid search for C and  for an RBF kernel SVM.
    Breast cancer dataset. See* bc_rbf_svm_search.py.'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 7-6：用于 RBF 核 SVM 的 C 和 γ 的二维网格搜索。乳腺癌数据集。参见* bc_rbf_svm_search.py。'
- en: The two helper functions, `run` and `split`, are exactly the same as we used
    before (see [Listing 7-4](ch07.xhtml#ch7lis4)); all the action is in `main`. We
    fix the number of folds at five and then load and randomize the full dataset.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 两个辅助函数，`run` 和 `split`，与之前使用的完全相同（见 [列表 7-4](ch07.xhtml#ch7lis4)）；所有的操作都在 `main`
    中。我们将折叠的数量固定为五，然后加载并随机化整个数据集。
- en: We then define the specific *C* and *γ* values to search over ❶. Note how `gs`
    is defined. The first part is 1/30, the reciprocal of the number of features.
    This is the default value for *γ* used by sklearn. We then multiply this factor
    by an array, (2^(*–*4),2^(*–*3),2^(*–*1),2⁰,2¹,2²,2³), to get the final *γ* values
    we’ll search over. Notice that one of the *γ* values is exactly the default sklearn
    uses since 2⁰ = 1.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们定义了要搜索的特定 *C* 和 *γ* 值 ❶。注意 `gs` 是如何定义的。第一部分是 1/30，表示特征数的倒数。这是 sklearn 默认使用的
    *γ* 值。然后，我们将这个因子乘以一个数组，（2^(*–*4),2^(*–*3),2^(*–*1),2⁰,2¹,2²,2³)），得到最终要搜索的 *γ*
    值。注意其中一个 *γ* 值正好是 sklearn 使用的默认值，因为 2⁰ = 1。
- en: The double loop ❷ iterates over all possible pairs of *C* and *γ*. For each
    one, we do five-fold validation to get a set of five scores in `z`. We then ask
    if the mean of this set is greater than the current maximum (*z*[max]) and if
    so, update the maximum and keep the *C* and *γ* values as our current bests ❸.
    When the loops over *C* and *γ* exit, we have our best values in `bestC` and `bestg`.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 双重循环 ❷ 遍历所有可能的 *C* 和 *γ* 配对。对于每一对，我们进行五折交叉验证，得到 `z` 中的五个得分。然后，我们检查这些得分的均值是否大于当前的最大值
    (*z*[max])，如果是，就更新最大值并保留 *C* 和 *γ* 的值作为当前的最佳值 ❸。当 *C* 和 *γ* 的循环结束时，我们就得到了 `bestC`
    和 `bestg` 的最佳值。
- en: If we run this code repeatedly, we’ll get different outputs each time. This
    is because we’re randomizing the order of the full dataset, which will alter the
    subsets in the folds, leading to a different mean score over the folds. For example,
    10 runs produced the output in [Table 7-9](ch07.xhtml#ch7tab9).
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们反复运行这段代码，我们每次都会得到不同的输出。这是因为我们在随机化整个数据集的顺序，这会改变折叠中的子集，从而导致不同的折叠均值。例如，10 次运行生成了
    [表 7-9](ch07.xhtml#ch7tab9) 中的输出。
- en: '**Table 7-9:** Breast Cancer Scores for an RBF SVM with Different C and *γ*
    Values Averaged Over 10 Runs'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 7-9：** 不同 C 和 *γ* 值的 RBF SVM 在乳腺癌数据集上的得分，经过 10 次运行平均'
- en: '| ***C*** | ***γ*** | ***accuracy*** |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '| ***C*** | ***γ*** | ***准确度*** |'
- en: '| --- | --- | --- |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 1 | 0.03333 | 0.97345 |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.03333 | 0.97345 |'
- en: '| 2 | 0.03333 | 0.98053 |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0.03333 | 0.98053 |'
- en: '| 10 | 0.00417 | 0.97876 |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 0.00417 | 0.97876 |'
- en: '| 10 | 0.00417 | 0.97699 |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 0.00417 | 0.97699 |'
- en: '| 10 | 0.00417 | 0.98053 |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 0.00417 | 0.98053 |'
- en: '| 10 | 0.01667 | 0.98053 |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 0.01667 | 0.98053 |'
- en: '| 10 | 0.01667 | 0.97876 |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 0.01667 | 0.97876 |'
- en: '| 10 | 0.01667 | 0.98053 |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 0.01667 | 0.98053 |'
- en: '| 1 | 0.03333 | 0.97522 |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.03333 | 0.97522 |'
- en: '| 10 | 0.00417 | 0.97876 |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 0.00417 | 0.97876 |'
- en: These results hint that (*C*,*γ*) = (10,0.00417) is a good combination. If we
    use these values to generate a grand mean over 1,000 runs of five-fold validation
    as before, we get an overall accuracy of 0.976991, or 97.70 percent, which is
    the highest grand mean accuracy of any model type we trained on the breast cancer
    histology dataset.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果表明 (*C*,*γ*) = (10,0.00417) 是一个不错的组合。如果我们使用这些值来生成 1,000 次五折交叉验证的总体均值，就能得到
    0.976991 的准确度，或 97.70％，这是我们在乳腺癌组织学数据集上训练的所有模型类型中最高的总体均值准确度。
- en: The breast cancer dataset is not a large dataset. We were able to use *k*-fold
    validation to find a good model that worked well with it. Now, let’s move from
    a pure vector-only dataset to one that is actually image-based and much larger,
    the MNIST dataset.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 乳腺癌数据集并不是一个大数据集。我们能够使用*k*-折验证找到一个适合该数据集的良好模型。现在，让我们从一个纯粹的仅向量数据集转到一个实际上是基于图像的且更大的数据集——MNIST数据集。
- en: Experiments with the MNIST Dataset
  id: totrans-451
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MNIST数据集实验
- en: The last dataset we’ll work with in this chapter is the vector version of the
    MNIST handwritten digit dataset (see [Chapter 5](ch05.xhtml#ch05)). Recall, this
    dataset consists of 28×28 pixel grayscale images of handwritten digits, [0,9],
    one digit centered per image. This dataset is by far the most common workhorse
    dataset in machine learning, especially in deep learning, and we’ll use it throughout
    the remainder of the book.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中我们将处理的最后一个数据集是MNIST手写数字数据集的向量版本（请参见[第5章](ch05.xhtml#ch05)）。回想一下，这个数据集由28×28像素的灰度手写数字图像组成，数字范围是[0,9]，每个图像中心只有一个数字。这个数据集是机器学习中最常用的基础数据集，尤其在深度学习中应用广泛，我们将在本书剩余部分使用它。
- en: Testing the Classical Models
  id: totrans-453
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 测试经典模型
- en: MNIST contains 60,000 training images, roughly evenly split among the digits,
    and 10,000 test images. Since we have a lot of training data, at least for classic
    models like those we’re concerned with here, we won’t make use of *k*-fold validation,
    though we certainly could. We’ll train on the training data and test on the testing
    data and trust that the two come from a common parent distribution (they do).
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST包含60,000张训练图像，数字类别大致均匀分布，以及10,000张测试图像。由于我们有大量的训练数据，至少对于我们这里关心的经典模型来说，我们不会使用*k*-折验证，尽管实际上是可以使用的。我们将使用训练数据进行训练，使用测试数据进行测试，并相信这两者来自同一母体分布（它们确实如此）。
- en: Since our classic models expect vector inputs, we’ll use the vector form of
    the MNIST dataset we created in [Chapter 5](ch05.xhtml#ch05). The images are unraveled
    so that the first 28 elements of the vector are row 0, the next 28 are row 1,
    and so on for an input vector of 28 × 28 = 784 elements. The images are stored
    as 8-bit grayscale, so the data values run from 0 to 255\. We’ll consider three
    versions of the dataset. The first is the raw byte version. The second is a version
    where we scale the data to [0,1) by dividing by 256, the number of possible grayscale
    values. The third is a normalized version where, per “feature” (really, pixel),
    we subtract the mean of that feature across the dataset and then divide by the
    standard deviation. This will let us explore how the range of the feature values
    affects things, if at all.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的经典模型期望输入是向量形式，因此我们将使用我们在[第5章](ch05.xhtml#ch05)中创建的MNIST数据集的向量版本。这些图像被展开，向量的前28个元素对应第一行，接下来的28个元素对应第二行，以此类推，形成一个28
    × 28 = 784个元素的输入向量。图像以8位灰度存储，因此数据值的范围是0到255。我们将考虑数据集的三种版本。第一种是原始字节版本。第二种是将数据缩放到[0,1)的版本，通过除以256，即灰度值的可能范围。第三种是标准化版本，其中每个“特征”（实际上是像素），我们会减去该特征在数据集中的均值，然后除以标准差。这将帮助我们探讨特征值的范围如何影响结果。
- en: '[Figure 7-1](ch07.xhtml#ch7fig1) shows examples of the original images and
    the resulting normalized vectors raveled back into images and scaled [0,255].
    Normalizing affects the appearance but does not destroy spatial relationships
    among the parts of the digit images. Just scaling the data to [0,1) will result
    in images that look the same as those on the top of [Figure 7-1](ch07.xhtml#ch7fig1).'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7-1](ch07.xhtml#ch7fig1)展示了原始图像的例子，以及标准化后的向量将其重新拼接回图像并缩放到[0,255]的效果。标准化影响了外观，但不会破坏数字图像中各部分之间的空间关系。仅仅将数据缩放到[0,1)将得到与[图7-1](ch07.xhtml#ch7fig1)顶部相同的图像。'
- en: '![image](Images/07fig01.jpg)'
  id: totrans-457
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/07fig01.jpg)'
- en: '*Figure 7-1: Original MNIST digits (top) and normalized versions used by the
    models (bottom)*'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: '*图7-1：原始MNIST数字（上）和模型使用的标准化版本（下）*'
- en: The code we’ll use is very similar to what we used previously, but for reasons
    that will be explained next, we will replace the `SVC` class with a new SVM class,
    `LinearSVC`. First, take a look at the helper functions in [Listing 7-7](ch07.xhtml#ch7lis7).
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的代码与之前的非常相似，但出于接下来将解释的原因，我们将用一个新的SVM类`LinearSVC`替换`SVC`类。首先，查看[清单7-7](ch07.xhtml#ch7lis7)中的辅助函数。
- en: import time
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: import time
- en: import numpy as np
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: from sklearn.neighbors import NearestCentroid
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.neighbors import NearestCentroid
- en: from sklearn.neighbors import KNeighborsClassifier
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.neighbors import KNeighborsClassifier
- en: from sklearn.naive_bayes import GaussianNB, MultinomialNB
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.naive_bayes import GaussianNB, MultinomialNB
- en: from sklearn.tree import DecisionTreeClassifier
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.tree import DecisionTreeClassifier
- en: from sklearn.ensemble import RandomForestClassifier
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.ensemble import RandomForestClassifier
- en: from sklearn.svm import LinearSVC
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.svm import LinearSVC
- en: from sklearn import decomposition
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn import decomposition
- en: 'def run(x_train, y_train, x_test, y_test, clf):'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 'def run(x_train, y_train, x_test, y_test, clf):'
- en: s = time.time()
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: s = time.time()
- en: clf.fit(x_train, y_train)
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: clf.fit(x_train, y_train)
- en: e_train = time.time() - s
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: e_train = time.time() - s
- en: s = time.time()
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: s = time.time()
- en: score = clf.score(x_test, y_test)
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: score = clf.score(x_test, y_test)
- en: e_test = time.time() - s
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: e_test = time.time() - s
- en: print("score = %0.4f (time, train=%8.3f, test=%8.3f)"
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: print("score = %0.4f (time, train=%8.3f, test=%8.3f)")
- en: '% (score, e_train, e_test))'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: '% (score, e_train, e_test))'
- en: 'def train(x_train, y_train, x_test, y_test):'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 'def train(x_train, y_train, x_test, y_test):'
- en: 'print("    Nearest Centroid          : ", end='''')'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 'print("    最近质心          : ", end='''')'
- en: run(x_train, y_train, x_test, y_test, NearestCentroid())
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test, NearestCentroid())
- en: 'print("    k-NN classifier (k=3)     : ", end='''')'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 'print("    k-NN 分类器 (k=3)     : ", end='''')'
- en: run(x_train, y_train, x_test, y_test,
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test,
- en: KNeighborsClassifier(n_neighbors=3))
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: KNeighborsClassifier(n_neighbors=3))
- en: 'print("    k-NN classifier (k=7)     : ", end='''')'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 'print("    k-NN 分类器 (k=7)     : ", end='''')'
- en: run(x_train, y_train, x_test, y_test,
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test,
- en: KNeighborsClassifier(n_neighbors=7))
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: KNeighborsClassifier(n_neighbors=7))
- en: 'print("    Naive Bayes (Gaussian)    : ", end='''')'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 'print("    朴素贝叶斯 (高斯)    : ", end='''')'
- en: run(x_train, y_train, x_test, y_test, GaussianNB())
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test, GaussianNB())
- en: 'print("    Decision Tree             : ", end='''')'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 'print("    决策树             : ", end='''')'
- en: run(x_train, y_train, x_test, y_test, DecisionTreeClassifier())
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test, DecisionTreeClassifier())
- en: 'print("    Random Forest (trees=  5) : ", end='''')'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 'print("    随机森林 (树=  5) : ", end='''')'
- en: run(x_train, y_train, x_test, y_test,
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test,
- en: RandomForestClassifier(n_estimators=5))
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: RandomForestClassifier(n_estimators=5))
- en: 'print("    Random Forest (trees= 50) : ", end='''')'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 'print("    随机森林 (树= 50) : ", end='''')'
- en: run(x_train, y_train, x_test, y_test,
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test,
- en: RandomForestClassifier(n_estimators=50))
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: RandomForestClassifier(n_estimators=50))
- en: 'print("    Random Forest (trees=500) : ", end='''')'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 'print("    随机森林 (树=500) : ", end='''')'
- en: run(x_train, y_train, x_test, y_test,
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test,
- en: RandomForestClassifier(n_estimators=500))
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: RandomForestClassifier(n_estimators=500))
- en: 'print("    Random Forest (trees=1000): ", end='''')'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 'print("    随机森林 (树=1000): ", end='''')'
- en: run(x_train, y_train, x_test, y_test,
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test,
- en: RandomForestClassifier(n_estimators=1000))
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: RandomForestClassifier(n_estimators=1000))
- en: 'print("    LinearSVM (C=0.01)        : ", end='''')'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 'print("    线性SVM (C=0.01)        : ", end='''')'
- en: run(x_train, y_train, x_test, y_test, LinearSVC(C=0.01))
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test, LinearSVC(C=0.01))
- en: 'print("    LinearSVM (C=0.1)         : ", end='''')'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 'print("    线性SVM (C=0.1)         : ", end='''')'
- en: run(x_train, y_train, x_test, y_test, LinearSVC(C=0.1))
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test, LinearSVC(C=0.1))
- en: 'print("    LinearSVM (C=1.0)         : ", end='''')'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 'print("    线性SVM (C=1.0)         : ", end='''')'
- en: run(x_train, y_train, x_test, y_test, LinearSVC(C=1.0))
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test, LinearSVC(C=1.0))
- en: 'print("    LinearSVM (C=10.0)        : ", end='''')'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 'print("    线性SVM (C=10.0)        : ", end='''')'
- en: run(x_train, y_train, x_test, y_test, LinearSVC(C=10.0))
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test, LinearSVC(C=10.0))
- en: '*Listing 7-7: Training differently scaled versions of the MNIST dataset using
    classic models. Helper functions. See* mnist_experiments.py.'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: '*Listing 7-7: 使用经典模型训练不同规模的 MNIST 数据集。辅助函数。详见* mnist_experiments.py。'
- en: The `run` function of [Listing 7-7](ch07.xhtml#ch7lis7) is also similar to those
    used previously, except we’ve added code to time how long training and testing
    takes. These times are reported along with the score. We added this code for MNIST
    because, unlike the tiny iris and breast cancer datasets, MNIST has a larger number
    of training samples so that runtime differences among the model types will start
    to show themselves. The `train` function is new, but all it does is wrap calls
    to `run` for the different model types.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: '[Listing 7-7](ch07.xhtml#ch7lis7) 中的 `run` 函数也类似于之前使用的函数，唯一不同的是，我们添加了计时代码来测量训练和测试所需的时间。这些时间与得分一起报告。我们为
    MNIST 添加了这段代码，因为与小型的鸢尾花和乳腺癌数据集不同，MNIST 拥有更多的训练样本，因此不同模型类型之间的运行时间差异开始显现出来。`train`
    函数是新的，但它所做的只是包装对 `run` 函数的调用，用于不同的模型类型。'
- en: Now take a look at [Listing 7-8](ch07.xhtml#ch7lis8), which contains the `main`
    function.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 现在看看 [Listing 7-8](ch07.xhtml#ch7lis8)，它包含了 `main` 函数。
- en: 'def main():'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 'def main():'
- en: x_train = np.load("mnist_train_vectors.npy").astype("float64")
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: x_train = np.load("mnist_train_vectors.npy").astype("float64")
- en: y_train = np.load("mnist_train_labels.npy")
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: y_train = np.load("mnist_train_labels.npy")
- en: x_test = np.load("mnist_test_vectors.npy").astype("float64")
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: x_test = np.load("mnist_test_vectors.npy").astype("float64")
- en: y_test = np.load("mnist_test_labels.npy")
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: y_test = np.load("mnist_test_labels.npy")
- en: print("Models trained on raw [0,255] images:")
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: print("在原始[0,255]图像上训练的模型：")
- en: train(x_train, y_train, x_test, y_test)
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: train(x_train, y_train, x_test, y_test)
- en: print("Models trained on raw [0,1) images:")
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: print("在原始[0,1)图像上训练的模型：")
- en: train(x_train/256.0, y_train, x_test/256.0, y_test)
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: train(x_train/256.0, y_train, x_test/256.0, y_test)
- en: ❶ m = x_train.mean(axis=0)
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ m = x_train.mean(axis=0)
- en: s = x_train.std(axis=0) + 1e-8
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: s = x_train.std(axis=0) + 1e-8
- en: x_ntrain = (x_train - m) / s
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: x_ntrain = (x_train - m) / s
- en: x_ntest  = (x_test - m) / s
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: x_ntest = (x_test - m) / s
- en: print("Models trained on normalized images:")
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: print("在标准化图像上训练的模型：")
- en: train(x_ntrain, y_train, x_ntest, y_test)
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: train(x_ntrain, y_train, x_ntest, y_test)
- en: ❷ pca = decomposition.PCA(n_components=15)
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ pca = decomposition.PCA(n_components=15)
- en: pca.fit(x_ntrain)
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: pca.fit(x_ntrain)
- en: x_ptrain = pca.transform(x_ntrain)
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: x_ptrain = pca.transform(x_ntrain)
- en: x_ptest = pca.transform(x_ntest)
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: x_ptest = pca.transform(x_ntest)
- en: print("Models trained on first 15 PCA components of normalized images:")
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: print("在标准化图像的前15个PCA组件上训练的模型：")
- en: train(x_ptrain, y_train, x_ptest, y_test)
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: train(x_ptrain, y_train, x_ptest, y_test)
- en: '*Listing 7-8: Training differently scaled versions of the MNIST dataset using
    classic models. Main function. See* mnist_experiments.py.'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表7-8：使用经典模型训练不同缩放版本的MNIST数据集。主函数。请参见* mnist_experiments.py。'
- en: The `main` function of [Listing 7-8](ch07.xhtml#ch7lis8) loads the data and
    then trains the models using the raw byte values. It then repeats the training
    using a scaled [0,1) version of the data and a scaled version of the testing data.
    These are the first two versions of the dataset we’ll use.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表7-8](ch07.xhtml#ch7lis8)中的`main`函数加载数据，然后使用原始字节值训练模型。接着，它使用数据的缩放[0,1)版本和测试数据的缩放版本重复训练。这些是我们将使用的前两个数据集版本。'
- en: Normalizing the data requires knowledge of the per feature means and standard
    deviations ❶. Note, we add a small value to the standard deviations to make up
    for pixels that have a standard deviation of zero. We can’t divide by zero, after
    all. We need to normalize the test data, but which means and which standard deviations
    should we use? Generally, we have more training data than testing data, so using
    the means and standard deviations from the training data makes sense; they are
    a better representation of the true means and standard deviations of the parent
    distribution that generated the data in the first place. However, at times, there
    may be slight differences between the training and testing data distributions,
    in which case it might make sense to consider the testing means and standard deviations.
    In this case, because the MNIST training and test datasets were created together,
    there’s no difference, so the training values are what we’ll use. Note that the
    same per feature means and standard deviations will need to be used for all new,
    unknown samples, too.
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 数据标准化需要了解每个特征的均值和标准差❶。注意，我们向标准差中添加一个小值，以弥补标准差为零的像素。毕竟，我们不能除以零。我们需要对测试数据进行标准化，但应该使用哪个均值和标准差呢？通常，训练数据比测试数据多，所以使用训练数据的均值和标准差是有意义的；它们更能代表最初生成数据的母体分布的真实均值和标准差。然而，有时训练和测试数据的分布可能会有些微差异，在这种情况下，考虑测试数据的均值和标准差可能更合适。在这种情况下，由于MNIST的训练和测试数据集是一起创建的，所以没有差异，因此我们将使用训练数据的值。请注意，所有新的未知样本也需要使用相同的每个特征的均值和标准差。
- en: Next, we apply PCA to the dataset just as we did for the iris data in [Chapter
    5](ch05.xhtml#ch05) ❷. Here we’re keeping the first 15 components. These account
    for just over 33 percent of the variance in the data and reduce the feature vector
    from 784 features (the pixels) to 15 features (the principal components). Then
    we train the models using these features.
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们像[第5章](ch05.xhtml#ch05) ❷中的鸢尾花数据那样对数据集应用PCA。这里我们保留前15个主成分，这些主成分仅占数据方差的33%以上，并将特征向量从784个特征（像素）减少到15个特征（主成分）。然后，我们使用这些特征训练模型。
- en: Running this code produces a wealth of output that we can learn from. Let’s
    first consider the scores per model type and data source. These are in [Table
    7-10](ch07.xhtml#ch7tab10); values in parentheses are the number of trees in the
    Random Forest.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码会产生大量输出，我们可以从中学习。首先，我们考虑每种模型类型和数据源的评分。这些内容在[表7-10](ch07.xhtml#ch7tab10)中；括号中的值表示随机森林中的树木数量。
- en: '**Table 7-10:** MNIST Model Scores for Different Preprocessing Steps'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: '**表7-10：** 不同预处理步骤的MNIST模型评分'
- en: '| **Model** | **Raw [0,255]** | **Scaled [0,1)** | **Normalized** | **PCA**
    |'
  id: totrans-541
  prefs: []
  type: TYPE_TB
  zh: '| **模型** | **原始[0,255]** | **缩放[0,1)** | **标准化** | **PCA** |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-542
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Nearest Centroid | 0.8203 | 0.8203 | 0.8092 | 0.7523 |'
  id: totrans-543
  prefs: []
  type: TYPE_TB
  zh: '| 最近质心 | 0.8203 | 0.8203 | 0.8092 | 0.7523 |'
- en: '| *k*-NN (*k* = 3) | 0.9705 | 0.9705 | 0.9452 | 0.9355 |'
  id: totrans-544
  prefs: []
  type: TYPE_TB
  zh: '| *k*-NN (*k* = 3) | 0.9705 | 0.9705 | 0.9452 | 0.9355 |'
- en: '| *k*-NN (*k* = 7) | 0.9694 | 0.9694 | 0.9433 | 0.9370 |'
  id: totrans-545
  prefs: []
  type: TYPE_TB
  zh: '| *k*-NN (*k* = 7) | 0.9694 | 0.9694 | 0.9433 | 0.9370 |'
- en: '| Naïve Bayes | 0.5558 | 0.5558 | 0.5239 | 0.7996 |'
  id: totrans-546
  prefs: []
  type: TYPE_TB
  zh: '| 朴素贝叶斯 | 0.5558 | 0.5558 | 0.5239 | 0.7996 |'
- en: '| Decision Tree | 0.8773 | 0.8784 | 0.8787 | 0.8403 |'
  id: totrans-547
  prefs: []
  type: TYPE_TB
  zh: '| 决策树 | 0.8773 | 0.8784 | 0.8787 | 0.8403 |'
- en: '| Random Forest (5) | 0.9244 | 0.9244 | 0.9220 | 0.8845 |'
  id: totrans-548
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林 (5) | 0.9244 | 0.9244 | 0.9220 | 0.8845 |'
- en: '| Random Forest (50) | 0.9660 | 0.9661 | 0.9676 | 0.9215 |'
  id: totrans-549
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林 (50) | 0.9660 | 0.9661 | 0.9676 | 0.9215 |'
- en: '| Random Forest (500) | 0.9708 | 0.9709 | 0.9725 | 0.9262 |'
  id: totrans-550
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林 (500) | 0.9708 | 0.9709 | 0.9725 | 0.9262 |'
- en: '| Random Forest (1000) | 0.9715 | 0.9716 | 0.9719 | 0.9264 |'
  id: totrans-551
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林 (1000) | 0.9715 | 0.9716 | 0.9719 | 0.9264 |'
- en: '| LinearSVM (C = 0.01) | 0.8494 | 0.9171 | 0.9158 | 0.8291 |'
  id: totrans-552
  prefs: []
  type: TYPE_TB
  zh: '| LinearSVM (C = 0.01) | 0.8494 | 0.9171 | 0.9158 | 0.8291 |'
- en: '| LinearSVM (C = 0.1) | 0.8592 | 0.9181 | 0.9163 | 0.8306 |'
  id: totrans-553
  prefs: []
  type: TYPE_TB
  zh: '| LinearSVM (C = 0.1) | 0.8592 | 0.9181 | 0.9163 | 0.8306 |'
- en: '| LinearSVM (C = 1.0) | 0.8639 | 0.9182 | 0.9079 | 0.8322 |'
  id: totrans-554
  prefs: []
  type: TYPE_TB
  zh: '| LinearSVM (C = 1.0) | 0.8639 | 0.9182 | 0.9079 | 0.8322 |'
- en: '| LinearSVM (C = 10.0) | 0.8798 | 0.9019 | 0.8787 | 0.7603 |'
  id: totrans-555
  prefs: []
  type: TYPE_TB
  zh: '| LinearSVM (C = 10.0) | 0.8798 | 0.9019 | 0.8787 | 0.7603 |'
- en: Look at the Nearest Centroid scores. These make sense as we move from left to
    right across the different versions of the dataset. For the raw data, the center
    location of each of the 10 classes leads to a simple classifier with an accuracy
    of 82 percent—not too bad considering random guessing would have an accuracy closer
    to 10 percent (1/10 for 10 classes). Scaling the data by a constant won’t change
    the relative relationship between the per class centroids so we’d expect the same
    performance in column 2 of [Table 7-10](ch07.xhtml#ch7tab10) as in column 1.
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下最近质心的得分。随着我们从左到右遍历数据集的不同版本，这些得分是有意义的。对于原始数据，每个10类的质心位置可以得到一个简单的分类器，准确率为82%——考虑到随机猜测的准确率接近10%（10类问题的1/10），这个结果还算不错。通过常数缩放数据不会改变各类质心之间的相对关系，因此我们可以预期在[表7-10](ch07.xhtml#ch7tab10)的第2列中的表现与第1列相同。
- en: Normalizing, however, does more than divide the data by a constant. We saw the
    effect clearly in [Figure 7-1](ch07.xhtml#ch7fig1). This alteration, at least
    for the MNIST dataset, changes the centroids’ relationships to each other and
    results in a decrease in accuracy to 80.9 percent.
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，归一化不仅仅是将数据除以一个常数。我们在[图7-1](ch07.xhtml#ch7fig1)中清楚地看到了这一效果。至少对于MNIST数据集来说，这一变化改变了质心之间的关系，导致准确率下降至80.9%。
- en: Finally, using PCA to reduce the number of features from 784 to 15 has a severe
    negative impact, resulting in an accuracy of only 75.2 percent. Note the word
    *only*. In the past, before the advent of deep learning, an accuracy of 75 percent
    on a problem with 10 classes would generally have been considered to be pretty
    good. Of course, it really isn’t. Who would get in a self-driving car that has
    an accident one time out of every four trips? We want to do better.
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用PCA将特征数量从784减少到15会产生严重的负面影响，导致准确率仅为75.2%。注意这里的词语*仅仅*。过去，在深度学习出现之前，10类问题的准确率达到75%通常被认为是相当不错的。当然，实际上并不是。谁会乘坐那种每四次出行就出一次事故的自动驾驶汽车呢？我们想要做得更好。
- en: Let’s consider the *k*-NN classifiers next. We see similar performance for both
    *k* = 3 and *k* = 7 and the same sort of trend as we saw with the Nearest Centroid
    classifier. This is to be expected given how similar the two types of models actually
    are. The difference in accuracy between the two (centroid and *k*-NN) is dramatic,
    however. An accuracy of 97 percent is generally regarded as good. But still, who
    would opt for elective surgery with a 3 percent failure rate?
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们来看看*k*-NN分类器。我们可以看到无论是*k* = 3还是*k* = 7，其表现都相似，且与最近质心分类器所看到的趋势相同。这是可以预期的，因为这两种模型在本质上非常相似。然而，二者（质心和*k*-NN）之间的准确率差异却很大。97%的准确率通常被认为是不错的。但仍然，谁会选择一种失败率为3%的选择性手术呢？
- en: Things get interesting when we look at the Naïve Bayes classifier. Here all
    the versions of the dataset perform poorly, though still five times better than
    guessing. We see a large jump in accuracy with the PCA processed dataset, from
    56 percent to 80 percent. This is the only model type to improve after using PCA.
    Why might this be? Remember, we’re using Gaussian Naïve Bayes, which means our
    independence assumption is coupled with an assumption that the continuous feature
    values are, per feature, really drawn from a normal distribution whose parameters,
    the mean and standard deviation, we can estimate from the feature values themselves.
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们查看朴素贝叶斯分类器时，情况变得有趣。这里，所有版本的数据集表现都较差，尽管仍然比随机猜测好五倍。我们看到在PCA处理的数据集上，准确率大幅提高，从56%提升到80%。这是唯一一个在使用PCA后表现有所改善的模型。为什么会这样呢？请记住，我们使用的是高斯朴素贝叶斯，这意味着我们的独立性假设与假设连续特征值在每个特征上都来自一个正态分布，并且我们可以通过特征值本身来估计该分布的参数（均值和标准差）。
- en: Now recall what PCA does, geometrically. It’s the equivalent of rotating the
    feature vectors onto a new set of coordinates aligned with the largest orthogonal
    directions derivable from the dataset. The word *orthogonal* implies that no part
    of a direction overlaps with any other part of any other direction. Think of the
    x-, y-, and z-axes of a three-dimensional plot. No part of the *x* is along the
    *y* or *z*, and so forth. This is what PCA does. Therefore, PCA makes the first
    assumption of Naïve Bayes more likely to be true, that the new features are indeed
    independent of each other. Add in the Gaussian assumption as to the distribution
    of the per pixel values, and we have an explanation for what we see in [Table
    7-10](ch07.xhtml#ch7tab10).
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 现在回想一下PCA在几何上的作用。它相当于将特征向量旋转到一个新的坐标系，该坐标系与从数据集中推导出的最大正交方向对齐。*正交*一词意味着一个方向的任何部分都不会与任何其他方向的部分重叠。可以想象三维图中的x轴、y轴和z轴。*x*轴的任何部分都不与*y*轴或*z*轴重叠，依此类推。这就是PCA所做的。因此，PCA使得朴素贝叶斯的第一个假设更有可能成立，即新特征确实相互独立。再加上关于每个像素值分布的高斯假设，我们就能解释在[表7-10](ch07.xhtml#ch7tab10)中看到的现象。
- en: The tree-based classifiers, Decision Tree and Random Forest, perform much the
    same until we get to the PCA version of the dataset. Indeed, there is no difference
    between the raw data and the data scaled by 256\. Again, this is to be expected
    as all scaling by a constant does is scale the decision thresholds for each of
    the nodes in the body of the tree or trees. As before, working with reduced dimensionality
    vectors via PCA results in a loss of accuracy because potentially important information
    has been discarded.
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 基于树的分类器——决策树和随机森林——的表现非常相似，直到我们处理PCA版本的数据集。事实上，原始数据和经过256缩放的数据之间没有区别。同样，这是可以预期的，因为通过常数进行缩放所做的只是调整树中每个节点的决策阈值。像之前一样，通过PCA处理后的低维向量导致准确率下降，因为可能重要的信息被丢弃了。
- en: For any data source, we see scores that make sense relative to each other. As
    before, the single Decision Tree performs worst, which it should except for simple
    cases since it’s competing against a collection of trees via the Random Forests.
    For the Random Forests, we see that the score improves as the number of trees
    in the forest increases—again expected. However, the improvement comes with diminishing
    returns. There’s a significant improvement when going from 5 trees to 50 trees,
    but a minimal improvement in going from 500 trees to 1,000 trees.
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何数据源，我们看到的得分相对彼此是有意义的。如前所述，单棵决策树表现最差，除了简单的情况外，这是预期的，因为它要与通过随机森林集成的树集合竞争。对于随机森林，我们看到得分随着森林中树的数量增加而提高——这也是预期的。然而，这种提升是递减的。从5棵树到50棵树时，提升显著，但从500棵树增加到1000棵树时，提升几乎可以忽略不计。
- en: Before we look at the SVM results, let’s understand why we made the switch from
    the `SVC` class to `LinearSVC`. As the name suggests, `LinearSVC` implements only
    a linear kernel. The `SVC` class is more generic and can implement other kernels,
    so why switch?
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看SVM结果之前，让我们先理解一下为什么我们从`SVC`类切换到了`LinearSVC`。顾名思义，`LinearSVC`仅实现了一个线性核。`SVC`类更通用，可以实现其他核函数，那么为什么要切换呢？
- en: The reason has to do with runtime. In computer science, there are specific definitions
    of complexity and an entire branch devoted to the analysis of algorithms and how
    they perform as their inputs scale larger and larger. All we’ll concern ourselves
    with here is *big-O* notation. This is a way of characterizing how the runtime
    of an algorithm changes as the input (or the number of inputs) gets larger and
    larger.
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 其原因与运行时间有关。在计算机科学中，有特定的复杂度定义，并且有一个专门的分支来分析算法以及它们如何随着输入的增大而表现得不同。我们在这里关注的仅仅是
    *大-O* 符号。这是一种描述算法运行时间如何随着输入（或输入的数量）增大而变化的方法。
- en: For example, a classic bubble sort algorithm works just fine on a few dozen
    numbers to be sorted. But, as the input gets larger (more numbers to be sorted),
    the runtime increases not linearly but quadratically, meaning the time to sort
    the numbers, *t*, is proportional to the *square* of the number of numbers to
    be sorted, *t* ∝ *n*², which is written as *O*(*n*²). So, the bubble sort is an
    order *n*² algorithm. In general, we want algorithms that are better than *n*²,
    more like *n*, written as *O*(*n*), or even independent of *n*, written as *O*(1).
    It turns out that the kernel algorithm for training an SVM is *worse* than *O*(*n*²)
    so that when the number of training samples increases, the runtime explodes. This
    is one reason for the switch from the `SVC` class to `LinearSVC`, which doesn’t
    use kernels.
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个经典的冒泡排序算法在对几十个数字进行排序时运行得很好。但是，随着输入量的增大（需要排序的数字增多），运行时间的增加并不是线性增长，而是二次增长，这意味着排序数字所需的时间，*t*，与需要排序的数字数量的*平方*成正比，*t*
    ∝ *n*²，这可以表示为 *O*(*n*²)。因此，冒泡排序是一种 *n*² 复杂度的算法。一般来说，我们希望算法的复杂度优于 *n*²，最好是 *n*
    复杂度，表示为 *O*(*n*)，甚至是与 *n* 无关，表示为 *O*(1)。事实证明，用于训练支持向量机（SVM）的核心算法的复杂度比 *O*(*n*²)还要差，因此当训练样本数增加时，运行时间会爆炸。这也是从
    `SVC` 类切换到 `LinearSVC` 类的原因之一，后者不使用核方法。
- en: 'The second reason for the switch has to do with the fact that Support Vector
    Machines are designed for binary classification—only two classes. The MNIST dataset
    has 10 classes, so something different has to be done. There are multiple approaches.
    According to the sklearn documentation, the `SVC` class uses a *one-versus-one*
    approach that trains pairs of classifiers, one class versus another: class 0 versus
    class 1, class 1 versus class 2, class 0 versus class 2, and so on. This means
    it ends up training not one but *m*(*m –* 1)/2 classifiers for *m* = 10 classes,
    or 10(10 *–* 1)/2 = 45 separate classifiers. This isn’t efficient in this case.
    The `LinearSVC` classifier uses a *one-versus-rest* approach. This means it trains
    an SVM to classify “0” versus “1–9”, then “1” versus “0, 2–9”, and so on, for
    a total of only 10 classifiers, one for each digit.'
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 切换的第二个原因与支持向量机的设计有关，支持向量机是为二分类问题设计的——只有两个类别。MNIST 数据集有 10 个类别，因此需要采取不同的方法。有多种方法可供选择。根据
    sklearn 文档，`SVC` 类使用一种 *一对一* 的方法，训练成对的分类器，一个类别与另一个类别对比：类别 0 对比 类别 1，类别 1 对比 类别
    2，类别 0 对比 类别 2，依此类推。这意味着它最终训练的不仅仅是一个，而是 *m*(*m –* 1)/2 个分类器，对于 *m* = 10 个类别而言，即
    10(10 *–* 1)/2 = 45 个独立的分类器。在这种情况下，这种方法并不高效。`LinearSVC` 分类器则使用 *一对多* 的方法。这意味着它训练一个
    SVM 来分类“0”对“1–9”，然后是“1”对“0, 2–9”，依此类推，总共只训练 10 个分类器，每个数字一个。
- en: It’s with the SVM classifiers that we see a definite benefit to scaling the
    data versus the raw byte inputs. We also see that the optimal *C* value is likely
    between *C* = 0.1 and *C* = 1.0\. Note that simple [0,1) scaling leads to SVM
    models that outperform (for this one dataset!) the models trained on the normalized
    data. The effect is small but consistent for different *C* values. And, as we
    saw before, dropping the dimensionality from 784 features to only 15 features
    via PCA leads to a rather large loss of accuracy. PCA seems not to have helped
    in this case. We’ll come back to it in a bit and see if we can understand why.
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 SVM 分类器时，我们可以看到数据缩放相对于原始字节输入的明显优势。我们还看到最佳的 *C* 值可能介于 *C* = 0.1 和 *C* = 1.0
    之间。请注意，简单的 [0,1) 缩放方式导致的 SVM 模型，在这个数据集上，比使用归一化数据训练的模型表现更好。这个效果虽小，但对于不同的 *C* 值来说是一致的。而且，正如我们之前看到的那样，通过
    PCA 将维度从 784 个特征降到只有 15 个特征，导致准确率有相当大的损失。在这个案例中，PCA 似乎并没有起到帮助作用。我们稍后会回到这个问题，看看是否能理解为什么。
- en: Analyzing Runtimes
  id: totrans-569
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 分析运行时间
- en: Let’s now look at the runtime performance of the algorithms. [Table 7-11](ch07.xhtml#ch7tab11)
    shows the train and test times, in seconds, for each model type and dataset version.
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看一下这些算法的运行时性能。[表 7-11](ch07.xhtml#ch7tab11)展示了每种模型类型和数据集版本的训练和测试时间，单位为秒。
- en: Look at the test times. This is how long each model takes to classify all 10,000
    digit images in the test set. The first thing that jumps out at us is that *k*-NN
    is slow. Classifying the test set takes over 10 minutes when full feature vectors
    are used! It’s only when we drop down to the first 15 PCA components that we see
    reasonable *k*-NN runtimes. This is a good example of the price we pay for a seemingly
    simple idea. Recall, the *k*-NN classifier finds the *k* closest training samples
    to the unknown sample we wish to classify. Here *closest* means in a Euclidean
    sense, like the distance between two points on a graph, except in this case we
    don’t have two or three dimensions but 784.
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下测试时间。这是每个模型分类测试集中的所有10,000个数字图像所需的时间。首先引人注目的是，*k*-NN速度较慢。当使用完整的特征向量时，分类测试集需要超过10分钟！只有当我们减少到前15个PCA组件时，*k*-NN的运行时间才变得合理。这是一个典型例子，展示了我们为一个看似简单的思路付出的代价。回想一下，*k*-NN分类器通过找到最接近的*
    k *个训练样本来分类我们希望识别的未知样本。在这里，“最接近”是指在欧几里得空间中的距离，就像图表上两点之间的距离一样，只不过在这个案例中，我们的空间维度不是二维或三维，而是784维。
- en: '**Table 7-11:** Training and Testing Times (Seconds) for Each Model Type'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 7-11：** 各模型类型的训练和测试时间（秒）'
- en: '|  | **Raw [0,255]** | **Scaled [0,1)** | **Normalized** | **PCA** |'
  id: totrans-573
  prefs: []
  type: TYPE_TB
  zh: '|  | **原始 [0,255]** | **缩放 [0,1)** | **归一化** | **PCA** |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-574
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| **Model** | train | test | train | test | train | test | train | test |'
  id: totrans-575
  prefs: []
  type: TYPE_TB
  zh: '| **模型** | 训练 | 测试 | 训练 | 测试 | 训练 | 测试 | 训练 | 测试 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-576
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Nearest Centroid | 0.23 | 0.03 | 0.24 | 0.03 | 0.24 | 0.03 | 0.01 | 0.00
    |'
  id: totrans-577
  prefs: []
  type: TYPE_TB
  zh: '| 最近质心 | 0.23 | 0.03 | 0.24 | 0.03 | 0.24 | 0.03 | 0.01 | 0.00 |'
- en: '| *K*-NN (*K* = 3) | 33.24 | 747.34 | 33.63 | 747.22 | 33.66 | 699.58 | 0.09
    | 3.64 |'
  id: totrans-578
  prefs: []
  type: TYPE_TB
  zh: '| *K*-NN (*K* = 3) | 33.24 | 747.34 | 33.63 | 747.22 | 33.66 | 699.58 | 0.09
    | 3.64 |'
- en: '| *K*-NN (*K* = 7) | 33.45 | 746.00 | 33.69 | 746.65 | 33.68 | 709.62 | 0.09
    | 4.65 |'
  id: totrans-579
  prefs: []
  type: TYPE_TB
  zh: '| *K*-NN (*K* = 7) | 33.45 | 746.00 | 33.69 | 746.65 | 33.68 | 709.62 | 0.09
    | 4.65 |'
- en: '| Naïve Bayes | 0.80 | 0.88 | 0.85 | 0.90 | 0.83 | 0.94 | 0.02 | 0.01 |'
  id: totrans-580
  prefs: []
  type: TYPE_TB
  zh: '| 朴素贝叶斯 | 0.80 | 0.88 | 0.85 | 0.90 | 0.83 | 0.94 | 0.02 | 0.01 |'
- en: '| Decision Tree | 25.42 | 0.03 | 25.41 | 0.02 | 25.42 | 0.02 | 2.10 | 0.00
    |'
  id: totrans-581
  prefs: []
  type: TYPE_TB
  zh: '| 决策树 | 25.42 | 0.03 | 25.41 | 0.02 | 25.42 | 0.02 | 2.10 | 0.00 |'
- en: '| Random Forest (5) | 2.65 | 0.06 | 2.70 | 0.06 | 2.61 | 0.06 | 1.20 | 0.03
    |'
  id: totrans-582
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林 (5) | 2.65 | 0.06 | 2.70 | 0.06 | 2.61 | 0.06 | 1.20 | 0.03 |'
- en: '| Random Forest (50) | 25.56 | 0.46 | 25.14 | 0.46 | 25.27 | 0.46 | 12.06 |
    0.25 |'
  id: totrans-583
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林 (50) | 25.56 | 0.46 | 25.14 | 0.46 | 25.27 | 0.46 | 12.06 | 0.25 |'
- en: '| Random Forest (500) | 252.65 | 4.41 | 249.69 | 4.47 | 249.19 | 4.45 | 121.10
    | 2.51 |'
  id: totrans-584
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林 (500) | 252.65 | 4.41 | 249.69 | 4.47 | 249.19 | 4.45 | 121.10 | 2.51
    |'
- en: '| Random Forest (1000) | 507.52 | 8.86 | 499.23 | 8.71 | 499.10 | 8.91 | 242.44
    | 5.00 |'
  id: totrans-585
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林 (1000) | 507.52 | 8.86 | 499.23 | 8.71 | 499.10 | 8.91 | 242.44 | 5.00
    |'
- en: '| LinearSVM (C = 0.01) | 169.45 | 0.02 | 5.93 | 0.02 | 232.93 | 0.02 | 16.91
    | 0.00 |'
  id: totrans-586
  prefs: []
  type: TYPE_TB
  zh: '| 线性SVM (C = 0.01) | 169.45 | 0.02 | 5.93 | 0.02 | 232.93 | 0.02 | 16.91 |
    0.00 |'
- en: '| LinearSVM (C = 0.1) | 170.58 | 0.02 | 36.00 | 0.02 | 320.17 | 0.02 | 37.46
    | 0.00 |'
  id: totrans-587
  prefs: []
  type: TYPE_TB
  zh: '| 线性SVM (C = 0.1) | 170.58 | 0.02 | 36.00 | 0.02 | 320.17 | 0.02 | 37.46 |
    0.00 |'
- en: '| LinearSVM (C = 1.0) | 170.74 | 0.02 | 96.34 | 0.02 | 488.06 | 0.02 | 66.49
    | 0.00 |'
  id: totrans-588
  prefs: []
  type: TYPE_TB
  zh: '| 线性SVM (C = 1.0) | 170.74 | 0.02 | 96.34 | 0.02 | 488.06 | 0.02 | 66.49 |
    0.00 |'
- en: '| LinearSVM (C = 10.0) | 170.46 | 0.02 | 154.34 | 0.02 | 541.69 | 0.02 | 86.87
    | 0.00 |'
  id: totrans-589
  prefs: []
  type: TYPE_TB
  zh: '| 线性SVM (C = 10.0) | 170.46 | 0.02 | 154.34 | 0.02 | 541.69 | 0.02 | 86.87
    | 0.00 |'
- en: Therefore, for each of the test samples, we need to find the *k* = 3 or *k*
    = 7 closest points in the training data. The naïve way to do this is to calculate
    the distance between the unknown sample and each of the 60,000 training samples,
    sort them, look at the *k* smallest distances, and vote to decide the output class
    label. This is a lot of work because we have 60,000 training samples and 10,000
    test samples for a total of 600,000,000 distance calculations. It isn’t as bad
    as all that because sklearn automatically selects the algorithm used to find the
    nearest neighbors, and decades of research has uncovered “better than brute force”
    approaches. Curious readers will want to investigate the terms *K-D-tree* and
    *Ball tree* (sometimes called *Metric tree*). See “An Empirical Comparison of
    Exact Nearest Neighbor Algorithms” by Kibriya and Frank (2007). Still, because
    of the extreme difference in runtimes between the other model types and *k*-NN,
    it’s necessary to remember just how slow *k*-NN can be if the dataset is large.
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于每个测试样本，我们需要在训练数据中找到距离最小的*k* = 3或*k* = 7个点。做这件事的朴素方法是计算未知样本与每一个60,000个训练样本之间的距离，对它们进行排序，查看距离最小的*k*个，并通过投票决定输出的类别标签。这是一项繁琐的工作，因为我们有60,000个训练样本和10,000个测试样本，总共需要进行600,000,000次距离计算。虽然这听起来很糟糕，但幸运的是，sklearn会自动选择用于查找最近邻的算法，而且几十年的研究已经发现了“比暴力计算更好的”方法。感兴趣的读者可以查阅术语*K-D树*和*Ball树*（有时也叫*度量树*）。见Kibriya和Frank（2007）发布的《准确最近邻算法的经验比较》。不过，由于其他模型类型和*k*-NN之间运行时间的极大差异，还是有必要记住，当数据集很大时，*k*-NN可能非常慢。
- en: The next slowest test times are for the Random Forest classifiers. We understand
    why the forest with 500 trees takes 10 times longer to run than the forest with
    50 trees; we have 10 times as many trees to evaluate. Training times also scale
    linearly. Reducing the size of the feature vectors with PCA improves things but
    not by a factor of 50 (784 features divided by 15 PCA features ≈ 50), so the performance
    difference is not primarily influenced by the size of the feature vector.
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来最慢的测试时间是对于随机森林分类器。我们理解为什么包含500棵树的森林比包含50棵树的森林需要花费10倍的时间来运行；因为我们需要评估的树木数量是前者的10倍。训练时间也线性增长。使用PCA减少特征向量的大小可以改善性能，但并没有提高50倍（784个特征除以15个PCA特征≈50），因此性能差异主要不是由特征向量的大小影响的。
- en: The linear SVMs are the next slowest to train after the Random Forests, but
    their execution time is extremely low. Long training times and short classification
    (inference) times are a hallmark of many model types. The simplest models are
    quick to train and quick to use, like Nearest Centroid or Naïve Bayes, but in
    general, “slow to train, quick to use” is a safe assumption. It’s especially true
    of neural networks.
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 线性SVM是继随机森林之后训练速度第二慢的模型，但其执行时间非常低。许多模型类型的特点是训练时间长，分类（推理）时间短。最简单的模型训练快速且使用迅速，比如最近质心或朴素贝叶斯，但一般来说，“训练慢，使用快”是一个可靠的假设。神经网络尤其如此。
- en: Using PCA hurt the performance of the models except for the Naïve Bayes classifier.
    Let’s do an experiment to see the effect of PCA as the number of PCA components
    changes.
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 使用PCA会降低模型的性能，除了朴素贝叶斯分类器之外。我们来做一个实验，观察PCA成分数量变化时对模型效果的影响。
- en: Experimenting with PCA Components
  id: totrans-594
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实验PCA组件
- en: For [Tables 7-10](ch07.xhtml#ch7tab10) and [7-11](ch07.xhtml#ch7tab11), we selected
    15 PCA components that represented about 33 percent of the variance in the dataset.
    This value was selected at random. You could imagine training models using some
    other number of principal components.
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 对于[表7-10](ch07.xhtml#ch7tab10)和[表7-11](ch07.xhtml#ch7tab11)，我们选择了15个PCA成分，它们代表了数据集中大约33%的方差。这个值是随机选择的。你可以想象，使用其他数量的主成分来训练模型。
- en: Let’s examine the effect of the number of PCA components used on the accuracy
    of the resulting model. We’ll vary the number of components from 10 to 780, which
    is basically all the features in the image. For each number of components, we’ll
    train a Naïve Bayes classifier, a Random Forest of 50 trees, and a linear SVM
    with *C* = 1.0\. The code to do this is in [Listing 7-9](ch07.xhtml#ch7lis9).
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查使用不同数量的PCA成分对结果模型准确性的影响。我们将成分数量从10变到780，基本涵盖了图像中的所有特征。对于每个成分数量，我们将训练一个朴素贝叶斯分类器、一个包含50棵树的随机森林和一个线性SVM（*C*
    = 1.0）。实现这一点的代码见[列表7-9](ch07.xhtml#ch7lis9)。
- en: 'def main():'
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: 'def main():'
- en: x_train = np.load("../data/mnist/mnist_train_vectors.npy")
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: x_train = np.load("../data/mnist/mnist_train_vectors.npy")
- en: .astype("float64")
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: .astype("float64")
- en: y_train = np.load("../data/mnist/mnist_train_labels.npy")
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: y_train = np.load("../data/mnist/mnist_train_labels.npy")
- en: x_test = np.load("../data/mnist/mnist_test_vectors.npy").astype("float64")
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: x_test = np.load("../data/mnist/mnist_test_vectors.npy").astype("float64")
- en: y_test = np.load("../data/mnist/mnist_test_labels.npy")
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: y_test = np.load("../data/mnist/mnist_test_labels.npy")
- en: m = x_train.mean(axis=0)
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: m = x_train.mean(axis=0)
- en: s = x_train.std(axis=0) + 1e-8
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: s = x_train.std(axis=0) + 1e-8
- en: x_ntrain = (x_train - m) / s
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: x_ntrain = (x_train - m) / s
- en: x_ntest  = (x_test - m) / s
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: x_ntest  = (x_test - m) / s
- en: n = 78
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: n = 78
- en: pcomp = np.linspace(10,780,n, dtype="int16")
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: pcomp = np.linspace(10,780,n, dtype="int16")
- en: nb=np.zeros((n,4))
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: nb=np.zeros((n,4))
- en: rf=np.zeros((n,4))
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: rf=np.zeros((n,4))
- en: sv=np.zeros((n,4))
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: sv=np.zeros((n,4))
- en: tv=np.zeros((n,2))
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: tv=np.zeros((n,2))
- en: 'for i,p in enumerate(pcomp):'
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i,p in enumerate(pcomp):'
- en: ❶ pca = decomposition.PCA(n_components=p)
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ pca = decomposition.PCA(n_components=p)
- en: pca.fit(x_ntrain)
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: pca.fit(x_ntrain)
- en: (*\newpage*)
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: (*\newpage*)
- en: xtrain = pca.transform(x_ntrain)
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: xtrain = pca.transform(x_ntrain)
- en: xtest = pca.transform(x_ntest)
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: xtest = pca.transform(x_ntest)
- en: tv[i,:] = [p, pca.explained_variance_ratio_.sum()]
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: tv[i,:] = [p, pca.explained_variance_ratio_.sum()]
- en: ❷ sc,etrn,etst =run(xtrain, y_train, xtest, y_test, GaussianNB())
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ sc,etrn,etst =run(xtrain, y_train, xtest, y_test, GaussianNB())
- en: nb[i,:] = [p,sc,etrn,etst]
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: nb[i,:] = [p,sc,etrn,etst]
- en: sc,etrn,etst =run(xtrain, y_train, xtest, y_test,
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: sc,etrn,etst =run(xtrain, y_train, xtest, y_test,
- en: RandomForestClassifier(n_estimators=50))
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: RandomForestClassifier(n_estimators=50))
- en: rf[i,:] = [p,sc,etrn,etst]
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: rf[i,:] = [p,sc,etrn,etst]
- en: sc,etrn,etst =run(xtrain, y_train, xtest, y_test, LinearSVC(C=1.0))
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: sc,etrn,etst =run(xtrain, y_train, xtest, y_test, LinearSVC(C=1.0))
- en: sv[i,:] = [p,sc,etrn,etst]
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: sv[i,:] = [p,sc,etrn,etst]
- en: np.save("mnist_pca_tv.npy", tv)
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("mnist_pca_tv.npy", tv)
- en: np.save("mnist_pca_nb.npy", nb)
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("mnist_pca_nb.npy", nb)
- en: np.save("mnist_pca_rf.npy", rf)
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("mnist_pca_rf.npy", rf)
- en: np.save("mnist_pca_sv.npy", sv)
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("mnist_pca_sv.npy", sv)
- en: '*Listing 7-9: Model accuracy as a function of the number of PCA components
    used. See* mnist_pca.py.'
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 7-9：模型精度作为所用 PCA 组件数量的函数。见* mnist_pca.py。'
- en: First, we load the MNIST dataset and compute the normalized version. This is
    the version that we’ll use with PCA. Next, we set up storage for the results.
    The variable `pcomp` stores the specific number of PCA components that will be
    used from 10 to 780 in steps of 10\. Then we start a loop over the number PCA
    components. We find the requested number of components (`p`) and map the dataset
    to the actual dataset trained and tested (`xtrain`, `xtest`) ❶.
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们加载 MNIST 数据集并计算其标准化版本。这是我们将与 PCA 一起使用的版本。接下来，我们为结果设置存储空间。变量 `pcomp` 存储将从
    10 到 780 以 10 为步长的 PCA 组件数量。然后，我们开始对 PCA 组件数量进行循环。我们找到所请求的组件数量（`p`），并将数据集映射到实际训练和测试的数据集（`xtrain`，`xtest`）❶。
- en: We also store the actual amount of variance in the dataset explained by the
    current number of principal components (`tv`). We’ll plot this value later to
    see how quickly the number of components covers the majority of the variance in
    the dataset.
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还存储了当前 PCA 组件数量解释的数据集中实际方差量（`tv`）。稍后我们将绘制这个值，看看组件数量是如何迅速涵盖数据集的大部分方差的。
- en: Next, we train and test a Gaussian Naïve Bayes classifier using the current
    number of features ❷. The `run` function called here is virtually identical to
    that used in [Listing 7-7](ch07.xhtml#ch7lis7) except that it returns the score,
    the training time, and the testing time. These are captured and put into the appropriate
    output array (`nb`). Then we do the same for the Random Forest and linear SVM.
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用当前特征数量训练和测试高斯朴素贝叶斯分类器❷。这里调用的 `run` 函数与[列表 7-7](ch07.xhtml#ch7lis7)中使用的几乎相同，唯一不同的是它返回分数、训练时间和测试时间。这些结果会被捕获并存储到相应的输出数组（`nb`）中。然后我们对随机森林和线性支持向量机（SVM）进行相同的操作。
- en: When the loop completes, we have all the data we need and we store the NumPy
    arrays on disk for plotting. Running this code takes some time, but the output,
    when plotted, leads to [Figure 7-2](ch07.xhtml#ch7fig2).
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: 当循环完成时，我们已经获得了所有需要的数据，并将 NumPy 数组存储到磁盘以供绘图。运行这段代码需要一些时间，但当绘制输出时，结果会显示在[图 7-2](ch07.xhtml#ch7fig2)中。
- en: The solid curve shows the fraction of the total variance in the dataset explained
    by the current number of PCA components (x-axis). This curve will reach a maximum
    of 1.0 when all the features in the dataset are used. It’s helpful in this case
    because it shows how quickly adding new components explains major orientations
    of the data. For MNIST, we see that about 90 percent of the variance is explained
    by using less than half the possible number of PCA components.
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: 实线曲线显示了数据集中当前 PCA 组件数量所解释的总方差比例（x 轴）。当数据集中的所有特征都被使用时，这条曲线将达到 1.0 的最大值。在这种情况下，它很有帮助，因为它展示了添加新组件后如何迅速解释数据的主要方向。对于
    MNIST，我们可以看到，通过使用不到一半的 PCA 组件数量，约 90% 的方差得到了解释。
- en: '![image](Images/07fig02.jpg)'
  id: totrans-637
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/07fig02.jpg)'
- en: '*Figure 7-2: Results of the PCA search*'
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7-2：PCA 搜索结果*'
- en: The remaining three curves plot the accuracy of the resulting models on the
    test data. The best-performing model, in this case, is the Random Forest with
    50 trees (triangles). This is followed by the linear SVM (squares) and then Naïve
    Bayes (circles). These curves show how the number of PCA components tracks with
    accuracy, and while the Random Forest and SVM change only slowly as PCA changes,
    we see that the Naïve Bayes classifier rapidly loses accuracy as the number of
    PCA components increases. Even the Random Forest and SVM decrease as the number
    of PCA components increases, which we might expect because the curse of dimensionality
    will eventually creep in. It seems likely that the dramatically different behavior
    of the Naïve Bayes classifier is due to violations of the independence assumption
    as the number of components used increases.
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: 剩余的三条曲线绘制了在测试数据上得到的模型准确率。在这种情况下，表现最好的模型是具有 50 棵树的随机森林（标记为三角形）。其后是线性 SVM（标记为方块）和朴素贝叶斯（标记为圆圈）。这些曲线展示了
    PCA 组件数量与准确率的关系，尽管随机森林和 SVM 随着 PCA 变化只有缓慢变化，但我们看到，朴素贝叶斯分类器随着 PCA 组件数量增加迅速下降准确率。即使是随机森林和
    SVM，随着 PCA 组件数量的增加，准确率也会下降，这可能是因为维度灾难最终会显现。看起来，朴素贝叶斯分类器的剧烈不同表现是由于随着使用的组件数量增加，独立性假设被违反。
- en: The maximum accuracy and the number of PCA components where it occurs are shown
    in [Table 7-12](ch07.xhtml#ch7tab12).
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: 最大准确率及其发生时的 PCA 组件数量显示在[表 7-12](ch07.xhtml#ch7tab12)中。
- en: '**Table 7-12:** Maximum Accuracy on MNIST by Model and Number of Components'
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 7-12：** 按模型和成分数量计算的 MNIST 最大准确率'
- en: '| **Model** | **Accuracy** | **Components** | **Variance** |'
  id: totrans-642
  prefs: []
  type: TYPE_TB
  zh: '| **模型** | **准确率** | **成分数** | **方差** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-643
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Naïve Bayes | 0.81390 | 20 | 0.3806 |'
  id: totrans-644
  prefs: []
  type: TYPE_TB
  zh: '| 朴素贝叶斯 | 0.81390 | 20 | 0.3806 |'
- en: '| Random Forest (50) | 0.94270 | 100 | 0.7033 |'
  id: totrans-645
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林 (50) | 0.94270 | 100 | 0.7033 |'
- en: '| Linear SVM (C = 1.0) | 0.91670 | 370 | 0.9618 |'
  id: totrans-646
  prefs: []
  type: TYPE_TB
  zh: '| 线性 SVM (C = 1.0) | 0.91670 | 370 | 0.9618 |'
- en: '[Table 7-12](ch07.xhtml#ch7tab12) tracks with the plot in [Figure 7-2](ch07.xhtml#ch7fig2).
    Interestingly, the SVM does not reach a maximum until nearly all the features
    in the original dataset are used. Also, the best accuracy found for the Random
    Forest and SVM is not as good as seen previously for other versions of the dataset
    that did not use PCA. So, for these models, PCA is not a benefit; it is, however,
    for the Naïve Bayes classifier.'
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 7-12](ch07.xhtml#ch7tab12) 与[图 7-2](ch07.xhtml#ch7fig2)中的图形一致。有趣的是，SVM 直到几乎使用了原始数据集中的所有特征时，才达到了最大值。此外，随机森林和
    SVM 的最佳准确率不如之前未使用 PCA 的其他数据集版本中的准确率。因此，对于这些模型来说，PCA 并没有带来好处；然而，对于朴素贝叶斯分类器来说，PCA
    是有益的。'
- en: Scrambling Our Dataset
  id: totrans-648
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 打乱我们的数据集
- en: 'Before we leave this section, let’s look at one more experiment that we’ll
    come back to in [Chapter 9](ch09.xhtml#ch09) and again in [Chapter 12](ch12.xhtml#ch12).
    In [Chapter 5](ch05.xhtml#ch05), we made a version of the MNIST dataset that scrambled
    the order of the pixels in the digit images. The scrambling wasn’t random: the
    same pixel in each input image was moved to the same position in the output image,
    resulting in images that, at least to us, no longer look like the original digit,
    as [Figure 7-3](ch07.xhtml#ch7fig3) shows. How might this scrambling affect the
    accuracy of the models we’ve been using in this chapter?'
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们离开这一部分之前，让我们看一个更多的实验，我们将在[第 9 章](ch09.xhtml#ch09)和[第 12 章](ch12.xhtml#ch12)再次提到。在[第
    5 章](ch05.xhtml#ch05)中，我们创建了一个 MNIST 数据集版本，打乱了数字图像中像素的顺序。打乱不是随机的：每个输入图像中的相同像素被移动到输出图像中的相同位置，结果是这些图像至少对我们来说不再像原始数字，正如[图
    7-3](ch07.xhtml#ch7fig3)所示。这种打乱可能会如何影响我们在本章中使用的模型的准确率？
- en: '![image](Images/07fig03.jpg)'
  id: totrans-650
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/07fig03.jpg)'
- en: '*Figure 7-3: Original MNIST digits (top) and scrambled versions of the same
    digit (bottom).*'
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7-3：原始 MNIST 数字（顶部）和相同数字的打乱版本（底部）。*'
- en: Let’s repeat the experiment code of [Listing 7-8](ch07.xhtml#ch7lis8), this
    time running only the scaled [0,1) version of the scrambled MNIST images. Since
    the only difference to the original code is the source filenames and the fact
    that we call `run` only once, we’ll forgo a new listing.
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重复[清单 7-8](ch07.xhtml#ch7lis8)中的实验代码，这次只运行打乱过的 MNIST 图像的缩放[0,1)版本。由于与原始代码的唯一区别是源文件名以及我们只调用一次
    `run`，因此我们不再列出新的代码。
- en: Placing the accuracy results side by side gives us [Table 7-13](ch07.xhtml#ch7tab13).
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: 将准确率结果并排显示，得到[表 7-13](ch07.xhtml#ch7tab13)。
- en: '**Table 7-13:** MNIST Scores by Model Type for Unscrambled and Scrambled Digits'
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 7-13：** 未打乱和打乱数字的 MNIST 模型类型得分'
- en: '| **Model** | **Unscrambled [0,1)** | **Scrambled [0,1)** |'
  id: totrans-655
  prefs: []
  type: TYPE_TB
  zh: '| **模型** | **未混乱 [0,1)** | **混乱 [0,1)** |'
- en: '| --- | --- | --- |'
  id: totrans-656
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Nearest Centroid | 0.8203 | 0.8203 |'
  id: totrans-657
  prefs: []
  type: TYPE_TB
  zh: '| 最近质心 | 0.8203 | 0.8203 |'
- en: '| *k*-NN (*k* = 3) | 0.9705 | 0.9705 |'
  id: totrans-658
  prefs: []
  type: TYPE_TB
  zh: '| *k*-NN (*k* = 3) | 0.9705 | 0.9705 |'
- en: '| *k*-NN (*k* = 7) | 0.9694 | 0.9694 |'
  id: totrans-659
  prefs: []
  type: TYPE_TB
  zh: '| *k*-NN (*k* = 7) | 0.9694 | 0.9694 |'
- en: '| Naïve Bayes | 0.5558 | 0.5558 |'
  id: totrans-660
  prefs: []
  type: TYPE_TB
  zh: '| 朴素贝叶斯 | 0.5558 | 0.5558 |'
- en: '| Decision Tree | 0.8784 | 0.8772 |'
  id: totrans-661
  prefs: []
  type: TYPE_TB
  zh: '| 决策树 | 0.8784 | 0.8772 |'
- en: '| Random Forest (5) | 0.9244 | 0.9214 |'
  id: totrans-662
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林 (5) | 0.9244 | 0.9214 |'
- en: '| Random Forest (50) | 0.9661 | 0.9651 |'
  id: totrans-663
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林 (50) | 0.9661 | 0.9651 |'
- en: '| Random Forest (500) | 0.9709 | 0.9721 |'
  id: totrans-664
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林 (500) | 0.9709 | 0.9721 |'
- en: '| Random Forest (1000) | 0.9716 | 0.9711 |'
  id: totrans-665
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林 (1000) | 0.9716 | 0.9711 |'
- en: '| LinearSVM (C = 0.01) | 0.9171 | 0.9171 |'
  id: totrans-666
  prefs: []
  type: TYPE_TB
  zh: '| 线性SVM (C = 0.01) | 0.9171 | 0.9171 |'
- en: '| LinearSVM (C = 0.1) | 0.9181 | 0.9181 |'
  id: totrans-667
  prefs: []
  type: TYPE_TB
  zh: '| 线性SVM (C = 0.1) | 0.9181 | 0.9181 |'
- en: '| LinearSVM (C = 1.0) | 0.9182 | 0.9185 |'
  id: totrans-668
  prefs: []
  type: TYPE_TB
  zh: '| 线性SVM (C = 1.0) | 0.9182 | 0.9185 |'
- en: '| LinearSVM (C = 10.0) | 0.9019 | 0.8885 |'
  id: totrans-669
  prefs: []
  type: TYPE_TB
  zh: '| 线性SVM (C = 10.0) | 0.9019 | 0.8885 |'
- en: Here we see virtually no difference between the scrambled and unscrambled results.
    In fact, for several models, the results are identical. For stochastic models,
    like the Random Forests, the results are still very similar. Is this surprising?
    Perhaps at first, but if we think about it for a bit, we realize that it really
    shouldn’t be.
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们几乎看不到混乱和未混乱结果之间的差异。实际上，对于几个模型，结果是完全相同的。对于像随机森林这样的随机模型，结果仍然非常相似。这个结果令人惊讶吗？也许一开始会有些惊讶，但如果我们稍微思考一下，我们会意识到其实不应该令人惊讶。
- en: 'All of the classic models are holistic: they operate on the entire feature
    vector as a single entity. While we can’t see the digits anymore because our vision
    does not operate holistically, the *information* present in the image is still
    there, so the models are just as happy with the scrambled as unscrambled inputs.
    When we get to [Chapter 12](ch12.xhtml#ch12), we’ll encounter a different result
    of this experiment.'
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
  zh: 所有经典模型都是整体的：它们在整个特征向量上作为一个整体进行操作。虽然我们不能再看到数字，因为我们的视觉并不以整体方式运作，但图像中存在的*信息*依然存在，所以模型对混乱输入和未混乱输入的反应是一样的。当我们到达[第12章](ch12.xhtml#ch12)时，我们将遇到这个实验的不同结果。
- en: Classical Model Summary
  id: totrans-672
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 经典模型总结
- en: What follows is a summary of the pros and cons related to each of the classical
    model types we have explored in this chapter. This can be used as a quick list
    for future reference. It will also take some of the observations we made via our
    experiments and make them more concrete.
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是对本章中我们探讨的每种经典模型类型的优缺点总结。它可以作为未来参考的快速列表。它还将结合我们通过实验得出的观察结果，使其更加具体。
- en: Nearest Centroid
  id: totrans-674
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 最近质心
- en: This is the simplest of all the models and can serve as a baseline. It’s seldom
    adequate unless the task at hand is particularly easy. The single centroid for
    each class is needlessly restrictive. You could use a more generalized approach
    that first finds an appropriate number of centroids for each class and then groups
    them together to build the classifier. In the extreme, this approaches *k*-NN
    but is still simpler in that the number of centroids is likely far less than the
    number of training samples. We’ll leave the implementation of this variation as
    an exercise for the motivated reader.
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
  zh: 这是所有模型中最简单的，可以作为基准。除非任务特别简单，否则很少足够。每个类别的单一质心是不必要的限制。你可以使用更通用的方法，首先为每个类别找到合适的质心数，然后将它们组合起来构建分类器。在极端情况下，这接近于*k*-NN，但仍然更简单，因为质心的数量可能远少于训练样本的数量。我们将把这种变体的实现作为激励读者的练习。
- en: Pros
  id: totrans-676
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 优点
- en: As we saw in this chapter, the implementation of a Nearest Centroid classifier
    takes only a handful of code. Additionally, Nearest Centroid is not restricted
    to binary models and readily supports multiclass models, like the irises. Training
    is very fast and since only one centroid is stored per class, the memory overhead
    is likewise very small. When used to label an unknown sample, run time is also
    very small because the distance from the sample to each class centroid is all
    that needs to be computed.
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本章看到的，最近质心分类器的实现只需要几行代码。此外，最近质心并不限于二分类模型，并且可以轻松支持多分类模型，如鸢尾花数据集。训练速度非常快，由于每个类别只存储一个质心，内存开销也非常小。当用于标记一个未知样本时，运行时间也非常短，因为只需要计算样本与每个类别质心的距离。
- en: Cons
  id: totrans-678
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 缺点
- en: Nearest Centroid makes a simplistic assumption about the distribution of the
    classes in the feature space—one that’s seldom met in practice. As a consequence
    of this assumption, the Nearest Centroid classifier is only highly accurate when
    the classes form a single tight group in the feature space and the groups are
    distant from each other like isolated islands.
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: 最近质心法对类在特征空间中的分布做出了简单的假设——这一假设在实际中很少成立。由于这一假设，最近质心分类器只有在类形成一个紧密的单一群体，且各群体像孤岛一样远离时，才会非常准确。
- en: '*k*-Nearest Neighbors'
  id: totrans-680
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '*k*-最近邻'
- en: 'This is the simplest model to train since there’s no training: we store the
    training set and use it to classify new instances by finding the *k* nearest training
    set vectors and voting.'
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最简单的模型，因为不需要训练：我们存储训练集，通过查找与未知实例最接近的*k*个训练样本并进行投票，来对新实例进行分类。
- en: Pros
  id: totrans-682
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 优点
- en: As just mentioned, no training required makes *k*-NN particularly attractive.
    It also can perform quite well, especially if the number of training samples is
    large relative to the dimensionality of the feature space (that is, the number
    of features in the feature vector). Multiclass support is implicit and doesn’t
    require a special approach.
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，*k*-最近邻方法由于不需要训练而特别具有吸引力。它在训练样本相对于特征空间的维度数（即特征向量中的特征数）较大时，表现也非常好。多类别支持是隐式的，不需要特殊的处理。
- en: Cons
  id: totrans-684
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 缺点
- en: 'The simplicity of “training” comes at a cost: classification is slow because
    of the need to look at every training example to find the nearest neighbors to
    the unknown feature vector. Decades of research, still underway, have sped up
    the search to improve the naïve implementation of looking at every training sample
    every time, but, as we saw in this chapter, classification is still slow, especially
    when compared to the speed of other model types (for example, SVM).'
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: “训练”的简化带来了代价：分类速度慢，因为需要查看每一个训练样本，以找到与未知特征向量最接近的邻居。几十年的研究仍在进行中，旨在加速搜索，改善每次查看所有训练样本的朴素实现。但正如我们在本章中看到的，分类仍然很慢，尤其是与其他模型类型（例如，SVM）的速度相比。
- en: Naïve Bayes
  id: totrans-686
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Naïve Bayes
- en: This model is conceptually simple and efficient, and surprisingly valid even
    when the core assumption of feature independence isn’t met.
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型概念简单且高效，即使核心假设——特征独立性假设——没有得到满足，仍然表现得出奇的有效。
- en: Pros
  id: totrans-688
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 优点
- en: Naïve Bayes is fast to train and fast to classify with, both positives. It also
    supports multiclass models instead of just binary, and other than continuous features.
    As long as the probability of a particular feature value can be computed, we can
    apply Naïve Bayes.
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: Naïve Bayes方法训练速度快、分类速度快，都是其优点。它还支持多类别模型，而不仅仅是二分类，并且适用于非连续特征。只要某个特征值的概率能够计算出来，我们就可以应用Naïve
    Bayes方法。
- en: Cons
  id: totrans-690
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 缺点
- en: The feature independence assumption central to Naïve Bayes is seldom true in
    practice. The more correlated the features (the more a change in, say, feature
    *x*[2] implies that *x*[3] will change), the poorer the performance of the model
    (in all likelihood).
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
  zh: Naïve Bayes的特征独立性假设在实际中很少成立。特征之间的相关性越强（例如，特征*x*[2]的变化可能意味着特征*x*[3]也会变化），模型的表现通常越差。
- en: While Naïve Bayes works directly with discrete valued features, using continuous
    features often involves a second level of assumption, as when we assumed that
    the continuous breast cancer dataset features were well represented as samples
    from a Gaussian distribution. This second assumption, which is also likely seldom
    true in practice, means that we need to estimate the parameters of the distribution
    from the dataset instead of using histograms to stand in for the actual feature
    probabilities.
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Naïve Bayes直接处理离散值特征，但使用连续特征时通常需要做第二层假设，就像我们假设乳腺癌数据集的连续特征可以很好地被高斯分布的样本表示一样。这一假设在实际中通常也不成立，这意味着我们需要从数据集中估算分布的参数，而不是使用直方图代替实际的特征概率。
- en: Decision Trees
  id: totrans-693
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 决策树
- en: This model is useful when it’s important to be able to understand, in human
    terms, why a particular class was selected.
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
  zh: 当需要用人类可理解的方式解释为何选择某个特定类别时，这个模型非常有用。
- en: Pros
  id: totrans-695
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 优点
- en: 'Decision Trees are reasonably fast to train. They’re also fast to use for classifying.
    Multiclass models are not a problem and are not restricted to using just continuous
    features. A Decision Tree can justify its answer by showing the particular steps
    used to reach a decision: the series of questions asked from the root to the leaf.'
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树的训练速度相当快。它们在分类时也很快。多类模型不是问题，且不限于使用连续特征。决策树可以通过展示从根到叶的提问过程来为其答案提供依据。
- en: Cons
  id: totrans-697
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 缺点
- en: Decision Trees are prone to overfitting—to learning elements of the training
    data that are not generally true of the parent distribution. Also, interpretability
    degrades as the tree increases in size. Tree depth needs to be balanced with the
    quality of the decisions (labels) as the leaves of the tree. This directly affects
    the error rate.
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树容易出现过拟合——即学习到训练数据中不普遍适用于母体分布的部分内容。而且，随着树的规模增大，易解释性会降低。树的深度需要与作为树叶的决策（标签）质量进行平衡，这直接影响错误率。
- en: Random Forests
  id: totrans-699
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 随机森林
- en: This is a more powerful form of Decision Tree that uses randomness to reduce
    the overfitting problem. Random Forests are one of the best performing of the
    classic model types and apply to a wide range of problem domains.
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种比决策树更强大的形式，它通过引入随机性来减少过拟合问题。随机森林是经典模型类型中表现最好的之一，适用于广泛的应用领域。
- en: Pros
  id: totrans-701
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 优点
- en: Like Decision Trees, Random Forests support multiclass models and other than
    continuous features. They are reasonably fast to train and to use for inference.
    Random Forests are also robust to differences in scale between features in the
    feature vector. In general, the accuracy improves, with diminishing returns, as
    the size of the forest grows.
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
  zh: 与决策树类似，随机森林支持多类模型并且不限于使用连续特征。它们的训练速度合理，并且在推理时也很快。随机森林对特征向量中各特征的尺度差异具有较强的鲁棒性。一般来说，随着森林规模的增加，准确率会提高，但增益逐渐减小。
- en: Cons
  id: totrans-703
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 缺点
- en: The easy interpretability of a Decision Tree disappears with a Random Forest.
    While each tree in the forest can justify its decision, the combined effect of
    the forest as a whole can be difficult to understand.
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树的易解释性在随机森林中消失。虽然森林中的每棵树都可以为其决策提供理由，但森林作为一个整体的组合效果可能很难理解。
- en: The inference runtime of a forest scales linearly with the number of trees.
    However, this can be mitigated by parallelization since each tree in the forest
    is making a calculation that does not depend on any other tree until combining
    the output of all trees to make an overall decision.
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
  zh: 森林的推理运行时间随着树木数量的增加呈线性增长。然而，可以通过并行化来缓解这个问题，因为森林中的每棵树进行的计算不依赖于其他树，直到将所有树的输出合并以做出最终决策。
- en: As stochastic models, the overall performance of a forest varies from training
    session to training session for the same dataset. In general, this isn’t an issue,
    but a pathological forest could exist—if possible, train the forest several times
    to get a sense of the actual performance.
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
  zh: 作为随机模型，森林的整体性能在每次训练中会有所不同，即使是同一个数据集。一般来说，这不是问题，但可能会存在病态森林——如果可能的话，建议训练森林多次，以便了解实际的表现。
- en: Support Vector Machines
  id: totrans-707
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 支持向量机
- en: Before the “rebirth” of neural networks, Support Vector Machines were generally
    considered to provide the pinnacle of model performance when they were applicable
    and well-tuned.
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络“重生”之前，支持向量机通常被认为是当它们适用且经过良好调优时，模型表现的巅峰。
- en: Pros
  id: totrans-709
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 优点
- en: SVMs can give show excellent performance when properly tuned. Inference is very
    fast once trained.
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机（SVM）在适当调整时能够表现出色。训练完成后，推理速度非常快。
- en: Cons
  id: totrans-711
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 缺点
- en: Multiclass models are not directly supported. Extensions for multiclass problems
    require training multiple models whether using one-versus-one or one-versus-rest
    approaches. Additionally, SVMs expect only continuous features and feature scaling
    matters; normalization or other scaling is often necessary to get good performance.
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
  zh: 不直接支持多类模型。对于多类问题的扩展需要训练多个模型，无论是采用一对一还是一对多的方法。此外，SVM只期望连续特征，特征缩放非常重要；通常需要进行归一化或其他缩放处理，以获得良好的表现。
- en: Large datasets are difficult to train when using other than linear kernels,
    and SVMs often require careful tuning of margin and kernel parameters (*C*, *γ*),
    though this can be mitigated somewhat by search algorithms that seek the best
    hyperparameter values.
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: 使用非线性核时，大型数据集训练非常困难，且SVM通常需要仔细调整边际和核参数（*C*，*γ*），尽管通过寻找最佳超参数值的搜索算法可以在一定程度上缓解这个问题。
- en: When to Use Classical Models
  id: totrans-714
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 何时使用经典模型
- en: The classical models may be *classic*, but they are still appropriate under
    the right conditions. In this section, we’ll discuss when you should consider
    a classical model instead of a more modern approach.
  id: totrans-715
  prefs: []
  type: TYPE_NORMAL
  zh: 经典模型或许是*经典*的，但在适当的条件下它们仍然是合适的。在这一部分，我们将讨论何时应该考虑使用经典模型，而不是更现代的方法。
- en: Handling Small Datasets
  id: totrans-716
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 处理小数据集
- en: 'One of the best reasons for working with a classic model is when the dataset
    is small. If you have only a few tens or hundreds of examples, then a classic
    model might be a good fit, whereas a deep learning model might not have enough
    training data to condition itself to the problem. Of course, there are exceptions.
    A deep neural network can, via transfer learning, sometimes learn from relatively
    few examples. Other approaches, like zero-shot or few-shot learning, may also
    allow a deep network to learn from a small dataset. However, these techniques
    are far beyond the scope of what we want to address in this book. For us, the
    rule of thumb is: when the dataset is small, consider using a classic model.'
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
  zh: 使用经典模型的最佳理由之一是当数据集较小时。如果你只有几十个或几百个例子，那么经典模型可能非常合适，而深度学习模型可能没有足够的训练数据来使自己适应问题。当然，也有例外。深度神经网络可以通过迁移学习，有时从相对较少的例子中学习。其他方法，如零样本学习或少样本学习，也可能允许深度网络从小数据集中学习。然而，这些技术超出了本书所要讨论的范围。对我们来说，经验法则是：当数据集较小的时候，可以考虑使用经典模型。
- en: Dealing with Reduced Computational Requirements
  id: totrans-718
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 处理减少计算需求的情况
- en: Another reason to consider a classic model is when computational requirements
    must be kept to a minimum. Deep neural networks are notoriously demanding of computational
    resources. The thousands, millions, and even billions of connections in a deep
    network all require extensive calculation. Implementing such a model on a small
    handheld device, or on an embedded microcontroller, will not work, or at least
    not work in any reasonable timeframe.
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑经典模型的另一个原因是，当计算需求必须保持在最低限度时。深度神经网络在计算资源上是出了名的消耗大。深度网络中的成千上万、百万乃至数十亿的连接都需要大量计算。在小型手持设备或嵌入式微控制器上实现这样的模型是不可行的，或者至少无法在合理的时间内实现。
- en: 'In such cases, you might consider a classic model that doesn’t require a lot
    of overhead. Simple models like Nearest Centroid or Naïve Bayes are good candidates.
    So are Decision Trees and Support Vector Machines, once trained. From the previous
    experiments, *k*-NN is probably not a good candidate unless the feature space
    or training set is small. This leads to our next rule of thumb: when computation
    must be kept to a minimum, consider using a classic model.'
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，你可以考虑使用不需要大量计算开销的经典模型。像最近质心或朴素贝叶斯这样的简单模型是不错的选择。经过训练的决策树和支持向量机也是如此。从之前的实验来看，除非特征空间或训练集很小，*k*-NN可能不是一个好的选择。这引出了我们的下一个经验法则：当计算必须保持在最低限度时，可以考虑使用经典模型。
- en: Having Explainable Models
  id: totrans-721
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 拥有可解释的模型
- en: 'Some classic models can explain themselves by revealing exactly *how* they
    arrived at their answer for a given unknown input. This includes Decision Trees,
    by design, but also *k*-NN (by showing the labels of the *k* voters), Nearest
    Centroid (by virtue of the selected centroid), and even Naïve Bayes (by the selected
    posterior probability). By way of contrast, deep neural networks are black boxes—they
    do not explain themselves—and it’s an active area of research to learn how to
    get a deep network to give some reason for its decision. This research has not
    been entirely unsuccessful, to be sure, but it’s still far from looking like the
    decision path in a tree classifier. Therefore, we can give another rule of thumb:
    when it’s essential to know how the classifier makes its decision, consider using
    a classic model.'
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
  zh: 一些经典模型可以通过揭示它们如何得出对于给定未知输入的答案来解释自己。这包括决策树，按设计原理如此，但也包括*k*-NN（通过显示*k*个投票者的标签）、最近质心（通过选择的质心）甚至朴素贝叶斯（通过选择的后验概率）。相比之下，深度神经网络是黑箱——它们无法自我解释——并且这是一个活跃的研究领域，旨在学习如何让深度网络给出其决策的理由。毫无疑问，这项研究并非完全没有成功，但它距离像树形分类器中的决策路径一样清晰还有很大差距。因此，我们可以得出另一个经验法则：当了解分类器是如何做出决策时至关重要时，可以考虑使用经典模型。
- en: Working with Vector Inputs
  id: totrans-723
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用向量输入
- en: 'Our final rule of thumb, acknowledging, of course, that there are indeed others
    we could give, has to do with the form of the inputs to the model. Modern deep
    learning systems often work with inputs that are not an amalgamation of separate
    features put into a single vector but instead are multidimensional inputs, such
    as images, where the “features” (pixels) are not different from each other but
    of the same kind and often highly correlated (the red pixel of the apple likely
    has a red pixel next to it, for example). A color image is a three-dimensional
    beast: there are three color images, one for the red channel, one for the blue
    channel, and one for the green channel. If the inputs are images from other sources,
    like satellites, there might be four to eight or more channels per image. A convolutional
    neural network is designed precisely for inputs such as these and will look for
    spatial patterns characteristic of the classes the network is trying to learn
    about. See [Chapter 12](ch12.xhtml#ch12) for more details.'
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的最终经验法则，当然，承认还有其他我们可以提出的规则，涉及到模型输入的形式。现代深度学习系统通常处理的输入并不是将单独的特征融合成一个向量，而是多维输入，例如图像，其中的“特征”（像素）彼此之间并不不同，而是相同类型并且通常高度相关（例如，苹果的红色像素旁边可能有一个红色像素）。彩色图像是一个三维的对象：有三个彩色图像，分别对应红色通道、蓝色通道和绿色通道。如果输入是来自其他来源的图像，如卫星图像，每张图像可能有四个、八个或更多的通道。卷积神经网络正是为这种类型的输入设计的，它将寻找与网络试图学习的类别相关的空间模式。更多细节请参见[第12章](ch12.xhtml#ch12)。
- en: 'But if the input to the model is a vector, especially a vector where the particular
    features are not related to each other (the key assumption of the Naïve Bayes
    classifier), then a classic model might be appropriate, since there’s no need
    to look for structure among the features beyond the global interpretation that
    the classic models perform by considering the input as a single monolithic entity.
    Therefore, we might give the rule as: when the input is a feature vector without
    spatial structure (unlike an image), especially if the features are not related
    to each other, consider using a classic model.'
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果模型的输入是一个向量，特别是特征之间没有关联的向量（例如朴素贝叶斯分类器的主要假设），那么经典模型可能是合适的，因为不需要在特征之间寻找结构，经典模型通过将输入视为一个单一的整体来进行全局解释。因此，我们可以提出如下规则：当输入是没有空间结构的特征向量（不同于图像），特别是特征之间没有关联时，可以考虑使用经典模型。
- en: It’s important to remember that these are rule-of-thumb suggestions, and that
    they aren’t always applicable to a particular problem. Also, it’s possible to
    use deep networks even if these rules seem to apply; it’s just that they may not
    give the best performance, or might be overkill, like using a shotgun to kill
    a fly. The main point of this book is to build intuition so that when a situation
    arises, we’ll know how to use the techniques we are exploring to maximum advantage.
    Pasteur said, “In the fields of observation, chance favors only the prepared mind”
    (lecture at the University of Lille, December 1854), and we wholeheartedly agree.
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要记住，这些只是经验法则的建议，并不是所有情况下都适用某个特定问题。此外，即使这些规则似乎适用，也可以使用深度网络；只是它们可能无法提供最佳性能，或者可能过于复杂，像用猎枪打苍蝇一样。本书的主要目的是培养直觉，以便当情况发生时，我们能知道如何最大限度地利用我们正在探索的技术。巴斯德曾说过，“在观察的领域中，机会只青睐于有准备的头脑”（1854年12月在里尔大学的讲座），我们完全同意。
- en: Summary
  id: totrans-727
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we worked with six common classical machine learning models:
    Nearest Centroid, *k*-Nearest Neighbors, Naïve Bayes, Decision Trees, Random Forests,
    and Support Vector Machines. We applied them to three datasets that were developed
    in [Chapter 5](ch05.xhtml#ch05): irises, breast cancer, and MNIST digits. We used
    the results of the experiments with these datasets to gain insight into the strengths
    and weaknesses of each model type along with the effect of different data preprocessing
    steps. We ended the chapter with a discussion of the classic models and when it
    might be appropriate to use them.'
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们使用了六种常见的经典机器学习模型：最近质心、*k*最近邻、朴素贝叶斯、决策树、随机森林和支持向量机。我们将它们应用于[第5章](ch05.xhtml#ch05)中开发的三个数据集：鸢尾花、乳腺癌和MNIST数字。我们通过这些数据集的实验结果深入了解了每种模型类型的优缺点，并分析了不同数据预处理步骤的影响。最后，我们在本章结束时讨论了经典模型以及在何种情况下适合使用它们。
- en: In the next chapter, we’ll move on from the classic models and begin our exploration
    of neural networks, the backbone of modern deep learning.
  id: totrans-729
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将从经典模型出发，开始探索神经网络——现代深度学习的基石。
