- en: '**5'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**5'
- en: BUILDING DATASETS**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**构建数据集**'
- en: '![image](Images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/common.jpg)'
- en: The previous chapter had a lot of detailed advice. Now let’s put it all into
    practice to build the datasets we’ll use throughout the remainder of the book.
    Some of these datasets are well suited to traditional models, because they consist
    of feature vectors. Others are better suited to deep learning models that work
    with multidimensional inputs—in particular, images, or things that can be visualized
    as images.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 前一章有很多详细的建议。现在让我们把这些都付诸实践，构建我们将在本书剩余部分使用的数据集。部分数据集非常适合传统模型，因为它们由特征向量组成。其他一些则更适合深度学习模型，这些模型处理的是多维输入，特别是图像，或可以视作图像的内容。
- en: We’ll work through acquiring the raw data and preprocessing the data to make
    it suitable for our tools. We won’t make actual training/validation/test splits
    until we use these datasets for specific models. It is worth noting here that
    preprocessing the data to make it suitable for a model is often one of the most
    labor-intensive of machine learning tasks. All the same, if it is not done, or
    not done well, your model may end up being far less useful than you want it to
    be.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过获取原始数据并对其进行预处理，使其适合我们的工具使用。直到我们将这些数据集应用于特定模型时，我们才会进行实际的训练/验证/测试数据划分。值得注意的是，预处理数据使其适合模型使用通常是机器学习任务中最为繁重的部分。尽管如此，如果没有做好这一步，或者做得不好，你的模型可能远不如你期望的那样有效。
- en: Irises
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 鸢尾花
- en: 'Perhaps the most classic of all machine learning datasets is the iris flower
    dataset, developed in 1936 by R. A. Fisher in his paper, “The Use of Multiple
    Measurements in Taxonomic Problems.” It’s a small dataset of three classes with
    50 samples in each class. There are four features: sepal width, sepal length,
    petal width, and petal length, all in centimeters. The three classes are *I. setosa*,
    *I. versicolour*, and *I. virginica*. This dataset is built into sklearn, but
    we’ll instead download it from the University of California, Irvine, Machine Learning
    Repository to practice working with externally sourced data and introduce a rich
    collection of datasets suitable for many traditional machine learning models.
    The main repository is located at *[https://archive.ics.uci.edu/ml/index.php](https://archive.ics.uci.edu/ml/index.php)*,
    but you can download the irises dataset directly from *[https://archive.ics.uci.edu/ml/datasets/iris/](https://archive.ics.uci.edu/ml/datasets/iris/)*.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 也许所有机器学习数据集中最经典的就是鸢尾花数据集，该数据集由R. A. Fisher于1936年在他的论文《多重测量在分类问题中的应用》中开发。它是一个包含三类、每类50个样本的小型数据集。数据集包括四个特征：萼片宽度、萼片长度、花瓣宽度和花瓣长度，单位为厘米。这三类分别是*I.
    setosa*、*I. versicolour*和*I. virginica*。这个数据集已经内置于sklearn库中，但我们将从加利福尼亚大学欧文分校的机器学习库下载它，以练习如何处理外部数据并引入适合许多传统机器学习模型的丰富数据集。主要的库位于*[https://archive.ics.uci.edu/ml/index.php](https://archive.ics.uci.edu/ml/index.php)*，你可以直接从*[https://archive.ics.uci.edu/ml/datasets/iris/](https://archive.ics.uci.edu/ml/datasets/iris/)*下载鸢尾花数据集。
- en: 'At the time of this writing, this dataset has been downloaded nearly 1.8 million
    times. You can download it by selecting the **Data Folder** link near the top
    of the page, then right-clicking and saving the *iris.data* file, ideally to a
    new directory called *iris*. Let’s take a look at the start of this file:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 截至本文撰写时，这个数据集已经被下载了近180万次。你可以通过点击页面顶部附近的**数据文件夹**链接，然后右键点击并保存*iris.data*文件，最好保存到一个名为*iris*的新目录中来下载它。我们来看看这个文件的开头部分：
- en: 5.1,3.5,1.4,0.2,Iris-setosa
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 5.1,3.5,1.4,0.2,Iris-setosa
- en: 4.9,3.0,1.4,0.2,Iris-setosa
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 4.9,3.0,1.4,0.2,Iris-setosa
- en: 4.7,3.2,1.3,0.2,Iris-setosa
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 4.7,3.2,1.3,0.2,Iris-setosa
- en: 4.6,3.1,1.5,0.2,Iris-setosa
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 4.6,3.1,1.5,0.2,Iris-setosa
- en: 5.0,3.6,1.4,0.2,Iris-setosa
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 5.0,3.6,1.4,0.2,Iris-setosa
- en: 5.4,3.9,1.7,0.4,Iris-setosa
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 5.4,3.9,1.7,0.4,Iris-setosa
- en: 4.6,3.4,1.4,0.3,Iris-setosa
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 4.6,3.4,1.4,0.3,Iris-setosa
- en: Because the class names at the end of each line are all the same, we should
    immediately suspect that the samples are sorted by class. Looking at the rest
    of the file confirms this. So, as emphasized in [Chapter 4](ch04.xhtml#ch04),
    we must be sure to randomize the data before training a model. Also, we need to
    replace the class names with integer labels. We can load the dataset into Python
    with the script in [Listing 5-1](ch05.xhtml#ch5lis1).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每行末尾的类名都相同，我们应该立刻怀疑样本是按类排序的。查看文件的其余部分证实了这一点。因此，正如[第4章](ch04.xhtml#ch04)中强调的那样，我们在训练模型之前，必须确保对数据进行随机化。此外，我们还需要将类名替换为整数标签。我们可以使用[清单5-1](ch05.xhtml#ch5lis1)中的脚本将数据集加载到Python中。
- en: import numpy as np
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: '❶ with open("iris.data") as f:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '❶ with open("iris.data") as f:'
- en: lines = [i[:-1] for i in f.readlines()]
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: lines = [i[:-1] for i in f.readlines()]
- en: ❷ n = ["Iris-setosa","Iris-versicolor","Iris-virginica"]
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ n = ["Iris-setosa","Iris-versicolor","Iris-virginica"]
- en: x = [n.index(i.split(",")[-1]) for i in lines if i != ""]
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: x = [n.index(i.split(",")[-1]) for i in lines if i != ""]
- en: x = np.array(x, dtype="uint8")
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: x = np.array(x, dtype="uint8")
- en: ❸ y = [[float(j) for j in i.split(",")[:-1]] for i in lines if i != ""]
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ y = [[float(j) for j in i.split(",")[:-1]] for i in lines if i != ""]
- en: y = np.array(y)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: y = np.array(y)
- en: ❹ i = np.argsort(np.random.random(x.shape[0]))
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ i = np.argsort(np.random.random(x.shape[0]))
- en: x = x[i]
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: x = x[i]
- en: y = y[i]
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: y = y[i]
- en: ❺ np.save("iris_features.npy", y)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ np.save("iris_features.npy", y)
- en: np.save("iris_labels.npy", x)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("iris_labels.npy", x)
- en: '*Listing 5-1: Loading the raw iris dataset and mapping to our standard format*'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 5-1：加载原始鸢尾花数据集并映射到我们的标准格式*'
- en: First, we load the text file containing the data. The list comprehension removes
    the extraneous newline character ❶. Next, we create the vector of labels by converting
    the text label into an integer, 0–2\. The last element in the list, created by
    splitting a line along commas, is the text label. We want NumPy arrays, so we
    turn the list into one. The uint8 is unnecessary, but since the labels are never
    negative and they’re never larger than 2, we save a bit of space by making the
    data type an unsigned 8-bit integer ❷.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们加载包含数据的文本文件。列表推导式去除了多余的换行符❶。接着，我们通过将文本标签转换为整数（0到2之间）来创建标签向量。通过按逗号分割每行生成的列表中的最后一个元素是文本标签。我们希望使用NumPy数组，因此我们将列表转换为数组。虽然uint8并不是必须的，但由于标签从不为负，且最大值不超过2，因此通过将数据类型设置为无符号8位整数，我们节省了一些空间❷。
- en: Creating the feature vectors as a 150-row by 4-column matrix comes next via
    a double list comprehension. The outer comprehension (i) moves over lines from
    the file, and the inner one (j) takes the list of measurements for each sample
    and turns them into floating-point numbers. We then convert the list of lists
    into a 2D NumPy array ❸. We finish by randomizing the dataset as we did previously
    ❹, and, finally, we write the NumPy arrays to disk so we can use them later ❺.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，通过双重列表推导式创建一个150行4列的特征向量矩阵。外部推导式（i）遍历文件中的每一行，内部推导式（j）则将每个样本的测量数据转换为浮点数。我们随后将这些列表的列表转换为一个2D
    NumPy数组❸。接着，我们像之前一样对数据集进行随机化❹，最后，将NumPy数组保存到磁盘，以便以后使用❺。
- en: '[Figure 5-1](ch05.xhtml#ch5fig1) shows a box plot of the features. This is
    a well-behaved dataset, but the second feature does have some possible outliers.
    Because the features all have similar scales, we’ll use the features as they are.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5-1](ch05.xhtml#ch5fig1)展示了特征的箱线图。这是一个行为良好的数据集，但第二个特征确实存在一些可能的离群值。由于所有特征的尺度相似，我们将保持原样使用这些特征。'
- en: '![image](Images/05fig01.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/05fig01.jpg)'
- en: '*Figure 5-1: Box plot of the four iris dataset features*'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 5-1：四个鸢尾花数据集特征的箱线图*'
- en: Breast Cancer
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 乳腺癌
- en: Our second dataset, the Wisconsin Diagnostic Breast Cancer dataset, is also
    in sklearn, and you can also download it from the UCI Machine Learning Repository.
    We’ll follow the preceding procedure and download the dataset to see how to process
    it. This seems unnecessary, true, but just as it’s crucial to build a good dataset
    to have any hope of training a good model, it’s equally important to learn how
    to work with data sources that are not in the format we want. Should you one day
    decide to make machine learning and data science a career, you’ll be faced with
    this issue on a near-daily basis.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第二个数据集，威斯康星诊断乳腺癌数据集，也包含在sklearn中，你也可以从UCI机器学习库下载它。我们将遵循前述过程并下载数据集，看看如何处理它。虽然这看起来似乎没有必要，但正如构建一个良好的数据集对于训练一个好的模型至关重要一样，学会处理那些不符合我们期望格式的数据源同样重要。如果有一天你决定将机器学习和数据科学作为职业，你将几乎每天都面临这个问题。
- en: Download the dataset by going to *[https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)/](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)/)*.
    Then, click the **Data Folder** link, and save the *wdbc.data* file.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 通过访问*[https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)/](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)/)*下载数据集。然后，点击**Data
    Folder**链接，保存*wdbc.data*文件。
- en: 'This dataset contains cell measurements taken from slides of fine-needle biopsies
    of breast masses. There are 30 continuous features and two classes: malignant
    (cancer, 212 samples) and benign (no cancer, 357 samples). This is also a popular
    dataset, with over 670,000 downloads. The first line of the file is shown here:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集包含从乳腺肿块的细针活检切片中提取的细胞测量数据。共有30个连续特征和两个类别：恶性（癌症，212个样本）和良性（无癌症，357个样本）。这也是一个热门数据集，下载量超过670,000次。文件的第一行如下所示：
- en: 842302,M,17.99,10.38,122.8,1001,0.1184, ...
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 842302,M,17.99,10.38,122.8,1001,0.1184, ...
- en: The first element in that line is a patient ID number that we don’t need to
    worry about. The second element is the label—*M* for malignant, and *B* for benign.
    The rest of the numbers in the line are 30 measurements related to cell size.
    The features themselves are of different scales, so besides creating the raw dataset,
    we’ll also create a standardized version. As this is the entirety of the dataset
    and we’ll have to hold some of it back for testing, we don’t need to record the
    per feature means and standard deviations in this case. If we were able to acquire
    more data generated in the same way, perhaps from an old file that was forgotten
    about, we would need to keep these values so that we could standardize the new
    inputs. The script to build this dataset, and to generate a summary box plot,
    is in [Listing 5-2](ch05.xhtml#ch5lis2).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 该行的第一个元素是患者ID号，我们不需要关心。第二个元素是标签——*M*表示恶性，*B*表示良性。该行中的其余数字是与细胞大小相关的30个测量值。特征本身具有不同的尺度，因此除了创建原始数据集外，我们还将创建一个标准化版本。由于这是整个数据集，并且我们将需要将其一部分保留用于测试，因此我们不需要记录每个特征的均值和标准差。如果我们能够获得更多相同方式生成的数据，可能来自一个被遗忘的旧文件，我们就需要保存这些值，以便可以对新的输入进行标准化。构建这个数据集并生成总结箱型图的脚本在[清单
    5-2](ch05.xhtml#ch5lis2)中。
- en: import numpy as np
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: import matplotlib.pyplot as plt
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: import matplotlib.pyplot as plt
- en: '❶ with open("wdbc.data") as f:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '❶ with open("wdbc.data") as f:'
- en: lines = [i[:-1] for i in f.readlines() if i != ""]
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: lines = [i[:-1] for i in f.readlines() if i != ""]
- en: ❷ n = ["B","M"]
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ n = ["B","M"]
- en: x = np.array([n.index(i.split(",")[1]) for i in lines],dtype="uint8")
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: x = np.array([n.index(i.split(",")[1]) for i in lines],dtype="uint8")
- en: y = np.array([[float(j) for j in i.split(",")[2:]] for i in lines])
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: y = np.array([[float(j) for j in i.split(",")[2:]] for i in lines])
- en: i = np.argsort(np.random.random(x.shape[0]))
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: i = np.argsort(np.random.random(x.shape[0]))
- en: x = x[i]
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: x = x[i]
- en: y = y[i]
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: y = y[i]
- en: z = (y - y.mean(axis=0)) / y.std(axis=0)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: z = (y - y.mean(axis=0)) / y.std(axis=0)
- en: ❸ np.save("bc_features.npy", y)
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ np.save("bc_features.npy", y)
- en: np.save("bc_features_standard.npy", z)
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("bc_features_standard.npy", z)
- en: np.save("bc_labels.npy", x)
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("bc_labels.npy", x)
- en: plt.boxplot(z)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: plt.boxplot(z)
- en: plt.show()
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: plt.show()
- en: '*Listing 5-2: Loading the raw breast cancer dataset*'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 5-2：加载原始乳腺癌数据集*'
- en: The first thing we do is read in the raw text data ❶. We then extract each label
    and map it to 0 for benign and 1 for malignant. Note here that we used 1 for the
    natural target case, so that a model outputting a probability value is indicating
    likelihood of finding cancer ❷. We extract the 30 features per sample as floats
    using a nested list comprehension to first pull out the text of the features (i)
    and then map them to floats (j). This produces a nested list, which NumPy conveniently
    converts into a matrix of 569 rows and 30 columns.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们做的第一件事是读取原始文本数据❶。然后，我们提取每个标签并将其映射为0（良性）和1（恶性）。注意，这里我们使用1表示自然目标情况，这样一个输出概率值的模型就表示了找到癌症的可能性❷。我们提取每个样本的30个特征作为浮点数，使用嵌套列表推导式首先提取特征文本（i），然后将其映射为浮点数（j）。这会生成一个嵌套列表，NumPy会将其方便地转换为一个具有569行和30列的矩阵。
- en: Next, we randomize the dataset and calculate the standardized version by subtracting,
    per feature, the mean value of that feature and dividing by the standard deviation.
    We’ll work with this version and examine it in the box plot of [Figure 5-2](ch05.xhtml#ch5fig2)
    ❸, which shows all 30 features after standardization.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将数据集进行随机化，并通过减去每个特征的均值并除以标准差来计算标准化版本。我们将使用这个版本，并在[图 5-2](ch05.xhtml#ch5fig2)的箱型图❸中进行分析，该图展示了标准化后的所有30个特征。
- en: '![image](Images/05fig02.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/05fig02.jpg)'
- en: '*Figure 5-2: Box plot of the 30 breast cancer dataset features*'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 5-2：乳腺癌数据集30个特征的箱型图*'
- en: 'We don’t need to know in this case what the features represent. We’ll work
    with the dataset under the assumption that the selected features are sufficient
    to the task of determining malignancy. Our models will indicate to us whether
    or not this is the case. The features are now all of the same scale as we can
    see by the location of the boxes on the y-axis: they’re all covering basically
    the same range. One characteristic of the data is immediately evident—namely,
    that there are many apparent outliers, as called out by the interquartile range
    (see [Figure 4-6](ch04.xhtml#ch4fig6)). These aren’t necessarily bad values, but
    they are an indicator that the data isn’t normally distributed—it doesn’t, per
    feature, follow a bell-curve-type distribution.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们不需要了解特征代表什么。我们将假设所选择的特征足以判断恶性程度。我们的模型会告诉我们这些假设是否成立。现在，所有特征的尺度都相同，从
    y 轴上方框的位置可以看出：它们都覆盖了基本相同的范围。数据的一个特征立刻显现出来——即，明显的离群值，这由四分位间距（见 [Figure 4-6](ch04.xhtml#ch4fig6)）标识出来。这些离群值不一定是错误的值，但它们表明数据并不是正态分布的——它在每个特征上都不遵循钟形曲线分布。
- en: MNIST Digits
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MNIST 数字
- en: Our next dataset isn’t typically composed of feature vectors, but is instead
    made up of thousands of small images of handwritten digits. This dataset is the
    workhorse of modern machine learning, and one of the first datasets deep learning
    researchers go to when looking to test new ideas. It’s overused, but that’s because
    it’s so well understood and simple to work with.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的下一个数据集通常不是由特征向量组成的，而是由成千上万张手写数字的小图像构成。这个数据集是现代机器学习中的“工作马”，也是深度学习研究人员测试新想法时最先使用的数据集之一。它虽然过于常用，但之所以被广泛使用，是因为它非常易于理解并且简单易用。
- en: The dataset has a long history, but the version we’ll use, the most common version,
    is known simply as the *MNIST dataset*. The canonical source for the dataset,
    [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/), includes
    some background material. To save time, we’ll use Keras to download and format
    the dataset.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集有着悠久的历史，但我们将使用的版本是最常见的版本，通常被称为 *MNIST 数据集*。数据集的权威来源是 [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)，其中包括一些背景材料。为了节省时间，我们将使用
    Keras 来下载并格式化数据集。
- en: Keras will return the dataset as 3D NumPy arrays. The first dimension is the
    number of images—60,000 for training and 10,000 for test. The second and third
    dimensions are the pixels of the images. The images are 28×28 pixels in size.
    Each pixel is an unsigned 8-bit integer, [0,255].
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 将数据集作为 3D NumPy 数组返回。第一个维度是图像的数量——训练集为 60,000 张，测试集为 10,000 张。第二个和第三个维度是图像的像素值。每张图像的大小为
    28×28 像素。每个像素是一个无符号的 8 位整数，范围为 [0,255]。
- en: Because we want to work with models that expect vectors as inputs, and because
    we want to use this dataset to illustrate certain properties of models later in
    the book, we’ll create additional datasets from this initial one. To do so, first
    we’ll unravel the images to form feature vectors so that we can use this dataset
    with traditional models that expect vector inputs. Second, we’ll use images, but
    we’ll permute the order of the images in the dataset. We’ll permute the order
    of the pixels of each image in the same way, so while the pixels will no longer
    be in the order that produces the digit image, the reordering will be deterministic,
    and applied consistently across all images. Third, we’ll create an unraveled feature
    vector version of these permuted images. We’ll use these additional datasets to
    explore differences between traditional neural networks and convolutional neural
    network models.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们希望使用期望向量作为输入的模型，并且希望利用这个数据集来展示书中后续模型的一些特性，我们将基于这个初始数据集创建额外的数据集。首先，我们将展开图像，形成特征向量，这样就能用传统模型来处理这个数据集，传统模型期望输入的是向量。其次，我们将继续使用图像，但会打乱数据集中图像的顺序。我们会以相同的方式打乱每张图像的像素顺序，因此尽管像素不再按原来的顺序排列，生成数字图像，但这种重排是确定性的，并且在所有图像中一致应用。第三，我们将为这些打乱顺序的图像创建展开后的特征向量版本。我们将使用这些额外的数据集来探索传统神经网络和卷积神经网络模型之间的差异。
- en: Use [Listing 5-3](ch05.xhtml#ch5lis3) to build the dataset files.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 [Listing 5-3](ch05.xhtml#ch5lis3) 来构建数据集文件。
- en: import numpy as np
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: import keras
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: import keras
- en: from keras.datasets import mnist
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: from keras.datasets import mnist
- en: ❶ (xtrn, ytrn), (xtst, ytst) = mnist.load_data()
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ (xtrn, ytrn), (xtst, ytst) = mnist.load_data()
- en: idx = np.argsort(np.random.random(ytrn.shape[0]))
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: idx = np.argsort(np.random.random(ytrn.shape[0]))
- en: xtrn = xtrn[idx]
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: xtrn = xtrn[idx]
- en: ytrn = ytrn[idx]
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ytrn = ytrn[idx]
- en: idx = np.argsort(np.random.random(ytst.shape[0]))
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: idx = np.argsort(np.random.random(ytst.shape[0]))
- en: xtst = xtst[idx]
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: xtst = xtst[idx]
- en: ytst = ytst[idx]
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ytst = ytst[idx]
- en: np.save("mnist_train_images.npy", xtrn)
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("mnist_train_images.npy", xtrn)
- en: np.save("mnist_train_labels.npy", ytrn)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("mnist_train_labels.npy", ytrn)
- en: np.save("mnist_test_images.npy", xtst)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("mnist_test_images.npy", xtst)
- en: np.save("mnist_test_labels.npy", ytst)
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("mnist_test_labels.npy", ytst)
- en: ❷ xtrnv = xtrn.reshape((60000,28*28))
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ xtrnv = xtrn.reshape((60000,28*28))
- en: xtstv = xtst.reshape((10000,28*28))
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: xtstv = xtst.reshape((10000,28*28))
- en: np.save("mnist_train_vectors.npy", xtrnv)
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("mnist_train_vectors.npy", xtrnv)
- en: np.save("mnist_test_vectors.npy", xtstv)
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("mnist_test_vectors.npy", xtstv)
- en: ❸ idx = np.argsort(np.random.random(28*28))
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ idx = np.argsort(np.random.random(28*28))
- en: 'for i in range(60000):'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i in range(60000):'
- en: xtrnv[i,:] = xtrnv[i,idx]
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: xtrnv[i,:] = xtrnv[i,idx]
- en: 'for i in range(10000):'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i in range(10000):'
- en: xtstv[i,:] = xtstv[i,idx]
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: xtstv[i,:] = xtstv[i,idx]
- en: np.save("mnist_train_scrambled_vectors.npy", xtrnv)
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("mnist_train_scrambled_vectors.npy", xtrnv)
- en: np.save("mnist_test_scrambled_vectors.npy", xtstv)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("mnist_test_scrambled_vectors.npy", xtstv)
- en: ❹ t = np.zeros((60000,28,28))
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ t = np.zeros((60000,28,28))
- en: 'for i in range(60000):'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i in range(60000):'
- en: t[i,:,:] = xtrnv[i,:].reshape((28,28))
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: t[i,:,:] = xtrnv[i,:].reshape((28,28))
- en: np.save("mnist_train_scrambled_images.npy", t)
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("mnist_train_scrambled_images.npy", t)
- en: t = np.zeros((10000,28,28))
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: t = np.zeros((10000,28,28))
- en: 'for i in range(10000):'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i in range(10000):'
- en: t[i,:,:] = xtstv[i,:].reshape((28,28))
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: t[i,:,:] = xtstv[i,:].reshape((28,28))
- en: np.save("mnist_test_scrambled_images.npy", t)
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("mnist_test_scrambled_images.npy", t)
- en: '*Listing 5-3: Loading and building the various MNIST datasets*'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 5-3：加载和构建各种 MNIST 数据集*'
- en: We start by telling Keras to load the MNIST dataset ❶. When run for the first
    time, Keras will show a message about downloading the dataset. After that, it
    won’t show the message again.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先告诉 Keras 加载 MNIST 数据集 ❶。首次运行时，Keras 会显示一条关于下载数据集的消息。之后，再次运行时不会显示该消息。
- en: The dataset itself is stored in four NumPy arrays. The first, xtrn, has a shape
    of (60000, 28, 28) for the 60,000 training images, each 28×28 pixels. The associated
    labels are in ytrn as integers, [0,9]. The 10,000 test images are in xtst with
    labels in ytst. We also randomize the order of the samples and write the arrays
    to disk for future use.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集本身存储在四个 NumPy 数组中。第一个数组 xtrn 的形状为 (60000, 28, 28)，包含 60,000 张训练图像，每张图像大小为
    28×28 像素。相关标签存储在 ytrn 中，值为整数 [0,9]。10,000 张测试图像存储在 xtst 中，标签存储在 ytst 中。我们还随机化样本顺序，并将数组写入磁盘以供将来使用。
- en: Next, we unravel the training and test images and turn them into vectors of
    784 elements ❷. Unraveling takes the first row of pixels followed by the second
    row and so on until all rows are laid end to end. We get 784 elements because
    28 × 28 = 784.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将训练集和测试集图像展开，转化为 784 元素的向量 ❷。展开操作将每一行像素按顺序连接在一起，直到所有行都被排列成一行。由于 28 × 28
    = 784，因此得到 784 个元素。
- en: Following this, we generate a permutation of the 784 elements in the unraveled
    vectors (idx) ❸.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们生成展开向量中 784 个元素的排列（idx） ❸。
- en: We use the permuted vectors to form new, scrambled, digit images and store them
    on disk ❹. The scrambled images are made from the scrambled vectors by undoing
    the unravel operation. In NumPy, this is just a call to the reshape method of
    the vector arrays. Note that at no time do we alter the relative ordering of the
    images, so we need to store only one file each for the train and test labels.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用打乱的向量形成新的、打乱的数字图像并存储到磁盘 ❹。这些打乱的图像是通过撤销展开操作，从打乱的向量中生成的。在 NumPy 中，这只是调用向量数组的
    reshape 方法。请注意，在整个过程中，我们不会改变图像的相对顺序，因此我们只需为训练和测试标签分别存储一个文件。
- en: '[Figure 5-3](ch05.xhtml#ch5fig3) shows representative digits from the MNIST
    dataset.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5-3](ch05.xhtml#ch5fig3) 展示了 MNIST 数据集中的代表性数字。'
- en: '![image](Images/05fig03.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/05fig03.jpg)'
- en: '*Figure 5-3: Representative MNIST digit images*'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 5-3：代表性的 MNIST 数字图像*'
- en: We don’t need to standardize the images, as we know they’re all on the same
    scale already, since they’re pixels. We’ll sometimes scale them as we use them,
    but for now we can leave them on disk as byte grayscale images. The dataset is
    reasonably balanced; [Table 5-1](ch05.xhtml#ch5tab1) shows the training distribution.
    Therefore, we don’t need to worry about imbalanced data.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们无需标准化这些图像，因为我们知道它们已经在相同的尺度上，因为它们是像素图像。我们有时会在使用时对它们进行缩放，但现在可以将它们作为字节灰度图像存储在磁盘上。数据集本身比较平衡；[表
    5-1](ch05.xhtml#ch5tab1) 显示了训练集的分布。因此，我们不需要担心数据不平衡的问题。
- en: '**Table 5-1:** Digit Frequencies for the MNIST Training Set'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 5-1：** MNIST 训练集的数字频率'
- en: '| **Digit** | **Count** |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| **数字** | **数量** |'
- en: '| --- | --- |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 0 | 5,923 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 5,923 |'
- en: '| 1 | 6,742 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 6,742 |'
- en: '| 2 | 5,958 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 5,958 |'
- en: '| 3 | 6,131 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 6,131 |'
- en: '| 4 | 5,842 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 5,842 |'
- en: '| 5 | 5,421 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 5,421 |'
- en: '| 6 | 5,918 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 5,918 |'
- en: '| 7 | 6,265 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 6,265 |'
- en: '| 8 | 5,851 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 5,851 |'
- en: '| 9 | 5,949 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 5,949 |'
- en: CIFAR-10
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CIFAR-10
- en: '*CIFAR-10* is another standard deep learning dataset that’s small enough for
    us to use without requiring a lot of training time or a GPU. As with MNIST, we
    can extract the dataset with Keras, which will download it the first time it’s
    requested. The source page for CIFAR-10 is at *https://www.cs.toronto.edu/\%7Ekriz/cifar.html*.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '*CIFAR-10* 是另一个标准的深度学习数据集，足够小，使得我们在不需要大量训练时间或 GPU 的情况下也能使用。与 MNIST 一样，我们可以通过
    Keras 提取该数据集，首次请求时它会自动下载。CIFAR-10 的源页面在 *https://www.cs.toronto.edu/\%7Ekriz/cifar.html*。'
- en: It’s worth perusing the page to learn more about where the dataset came from.
    It consists of 60,000 32×32 pixel RGB images from 10 classes, with 6,000 samples
    in each class. The training set contains 50,000 images, and the test set contains
    10,000 images. The 10 classes are shown here in [Table 5-2](ch05.xhtml#ch5tab2).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 值得浏览该页面，了解更多有关数据集来源的信息。它由 60,000 张 32×32 像素的 RGB 图像组成，来自 10 个类别，每个类别有 6,000
    个样本。训练集包含 50,000 张图像，测试集包含 10,000 张图像。这 10 个类别在这里的 [表 5-2](ch05.xhtml#ch5tab2)
    中展示。
- en: '**Table 5-2:** CIFAR-10 Class Labels and Names'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 5-2:** CIFAR-10 类别标签和名称'
- en: '| Label | Class |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 标签 | 类别 |'
- en: '| --- | --- |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 0 | airplane |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 飞机 |'
- en: '| 1 | automobile |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 汽车 |'
- en: '| 2 | bird |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 鸟 |'
- en: '| 3 | cat |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 猫 |'
- en: '| 4 | deer |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 鹿 |'
- en: '| 5 | dog |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 狗 |'
- en: '| 6 | frog |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 青蛙 |'
- en: '| 7 | horse |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 马 |'
- en: '| 8 | ship |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 船 |'
- en: '| 9 | truck |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 卡车 |'
- en: '[Figure 5-4](ch05.xhtml#ch5fig4) shows, row by row, a collection of representative
    images from each class. Let’s extract the dataset, store it for future use, and
    create vector representations, much as we did for MNIST.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5-4](ch05.xhtml#ch5fig4) 展示了每个类别的代表性图像，按行排列。让我们提取数据集，存储以供将来使用，并创建向量表示，就像我们对
    MNIST 所做的那样。'
- en: '![image](Images/05fig04.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/05fig04.jpg)'
- en: '*Figure 5-4: Representative CIFAR-10 images*'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 5-4：代表性的 CIFAR-10 图像*'
- en: The script to do all of this is in [Listing 5-4](ch05.xhtml#ch5lis4).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 执行这一切的脚本在 [清单 5-4](ch05.xhtml#ch5lis4) 中。
- en: import numpy as np
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: import keras
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: import keras
- en: from keras.datasets import cifar10
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: from keras.datasets import cifar10
- en: ❶ (xtrn, ytrn), (xtst, ytst) = cifar10.load_data()
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ (xtrn, ytrn), (xtst, ytst) = cifar10.load_data()
- en: idx = np.argsort(np.random.random(ytrn.shape[0]))
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: idx = np.argsort(np.random.random(ytrn.shape[0]))
- en: xtrn = xtrn[idx]
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: xtrn = xtrn[idx]
- en: ytrn = ytrn[idx]
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ytrn = ytrn[idx]
- en: idx = np.argsort(np.random.random(ytst.shape[0]))
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: idx = np.argsort(np.random.random(ytst.shape[0]))
- en: xtst = xtst[idx]
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: xtst = xtst[idx]
- en: ytst = ytst[idx]
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ytst = ytst[idx]
- en: np.save("cifar10_train_images.npy", xtrn)
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("cifar10_train_images.npy", xtrn)
- en: np.save("cifar10_train_labels.npy", ytrn)
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("cifar10_train_labels.npy", ytrn)
- en: np.save("cifar10_test_images.npy", xtst)
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("cifar10_test_images.npy", xtst)
- en: np.save("cifar10_test_labels.npy", ytst)
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("cifar10_test_labels.npy", ytst)
- en: ❷ xtrnv = xtrn.reshape((50000,32*32*3))
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ xtrnv = xtrn.reshape((50000,32*32*3))
- en: xtstv = xtst.reshape((10000,32*32*3))
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: xtstv = xtst.reshape((10000,32*32*3))
- en: np.save("cifar10_train_vectors.npy", xtrnv)
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("cifar10_train_vectors.npy", xtrnv)
- en: np.save("cifar10_test_vectors.npy", xtstv)
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("cifar10_test_vectors.npy", xtstv)
- en: '*Listing 5-4: Loading and building the various CIFAR-10 datasets*'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 5-4：加载并构建各类 CIFAR-10 数据集*'
- en: 'We first load CIFAR-10 from Keras ❶. As with MNIST, the dataset will download
    automatically the first time that the code is run. And, as with MNIST, we randomize
    the train and test splits. The training data is in xtrn as a (50,000; 32; 32;
    3) array. The last dimension is for the three color components for each pixel:
    red, green, and blue. The test data is similar, and is in xtst as a (10,000; 32;
    32; 3) array. Finally, we write the randomized train and test images to disk.
    Next, we unravel the images to produce 32 × 32 × 3 = 3072 element feature vectors
    representing the images ❷ and write them to disk.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先从 Keras 加载 CIFAR-10 ❶。与 MNIST 一样，数据集会在第一次运行代码时自动下载。而且，与 MNIST 一样，我们会随机化训练集和测试集的划分。训练数据存储在
    xtrn 中，形状为（50,000; 32; 32; 3）的数组。最后一个维度表示每个像素的三个颜色分量：红色、绿色和蓝色。测试数据也类似，存储在 xtst
    中，形状为（10,000; 32; 32; 3）的数组。最后，我们将随机化的训练图像和测试图像写入磁盘。接着，我们展开这些图像，生成 32 × 32 × 3
    = 3072 元素的特征向量，表示这些图像 ❷，并将它们写入磁盘。
- en: Data Augmentation
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据增强
- en: As we saw in [Chapter 4](ch04.xhtml#ch04), the dataset is everything, so it
    needs to be as complete as possible. You’ll normally achieve this by carefully
    selecting samples that fit within the range of inputs the model will encounter
    when you use it. Thinking back to our earlier analogy, we need the model to *interpolate*
    and not *extrapolate*. But sometimes, even though we have a wide range of possible
    samples, we don’t have a lot of actual samples. This is where data augmentation
    can help.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第4章](ch04.xhtml#ch04)中看到的，数据集至关重要，因此它需要尽可能完整。通常，你可以通过精心选择适合模型在使用时可能遇到的输入范围的样本来实现这一点。回想我们之前的类比，我们需要模型进行*插值*而不是*外推*。但是，有时候，即使我们有广泛的可能样本范围，实际样本的数量却很少。此时，数据增强可以提供帮助。
- en: '*Data augmentation* uses the data in the existing dataset to generate new possible
    samples to add to the set. These samples are always based, in some way, on the
    existing data. Data augmentation is a powerful technique and is particularly helpful
    when our actual dataset is small. In a practical sense, data augmentation should
    probably be used whenever it’s feasible.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '*数据增强*使用现有数据集中的数据生成新的可能样本，并将其添加到数据集中。这些样本总是以某种方式基于现有数据。数据增强是一种强大的技术，特别适用于实际数据集较小的情况。从实践角度来看，数据增强应该在可行的情况下尽量使用。'
- en: Data augmentation takes the data we already have and modifies it to create new
    samples that could have plausibly come from the same parent distribution as our
    actual data. That means that if we were patient enough to keep collecting real
    data, we could measure those new samples. Sometimes data augmentation can go beyond
    what we would actually measure, yet still help the model learn to generalize to
    the actual data. For example, a model using images as input might benefit from
    unrealistic colors or backgrounds when the actual inputs to the model would never
    use those colors or backgrounds.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强利用我们已有的数据，通过修改它来创建可能来自与实际数据相同母体分布的新样本。这意味着，如果我们足够耐心地继续收集真实数据，我们可以测量那些新样本。有时候，数据增强甚至可以超出我们实际测量的范围，但仍然有助于模型学会对实际数据进行泛化。例如，使用图像作为输入的模型可能会从不切实际的颜色或背景中受益，即使模型的实际输入永远不会使用这些颜色或背景。
- en: While data augmentation works in many situations and is a mainstay of deep learning,
    you won’t always be able to use it because not all data can be realistically enhanced.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然数据增强在许多情况下都有效，并且是深度学习的常用方法，但并不是所有数据都能进行现实的增强，因此你并不总能使用它。
- en: 'In this section, we’ll take a look at why we’d want to consider using data
    augmentation and how we might go about doing it. We’ll then augment two of the
    datasets we developed previously, so when we build models, we can see how augmentation
    affects the models’ learning. As far as augmentation is concerned, here’s a rule
    of thumb: in general, you should perform data augmentation whenever possible,
    especially if the dataset is small.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将探讨为什么我们要考虑使用数据增强以及如何实施。接下来，我们将增强之前开发的两个数据集，以便在构建模型时，我们能够看到数据增强如何影响模型的学习。关于数据增强，有一个经验法则：通常情况下，只要可能，特别是在数据集较小时，你应该进行数据增强。
- en: Why Should You Augment Training Data?
  id: totrans-171
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么要增强训练数据？
- en: In [Chapter 4](ch04.xhtml#ch04), we encountered the curse of dimensionality.
    We saw that the solution to it, for many models, is to fill in the space of possible
    inputs with more and more training data. Data augmentation is one way we can fill
    in this space. We’ll need to do this in the future; in [Chapter 6](ch06.xhtml#ch06),
    for example, we’ll meet the *k*-Nearest Neighbor classifier, perhaps the simplest
    of all classifiers.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第4章](ch04.xhtml#ch04)中，我们遇到了维度灾难的问题。我们看到，许多模型解决这一问题的方法是通过更多的训练数据填充可能输入的空间。数据增强是我们可以用来填充这个空间的一种方式。我们将来还需要这样做；例如，在[第6章](ch06.xhtml#ch06)中，我们将遇到*k*-最近邻分类器，它可能是所有分类器中最简单的一种。
- en: This classifier depends, critically, on having enough training data to adequately
    fill in the input feature space. If there are three features, then the space is
    three-dimensional and the training data will fit into some cube in that space.
    The more training data we have, the more samples we’ll have in the cube, and the
    better the classifier will do. That’s because the classifier measures the distance
    between points in the training data and that of a new, unknown feature vector
    and votes on what label to assign. The denser the space is with training points,
    the more often the voting process will succeed. Loosely speaking, data augmentation
    fills in this space. For most datasets, acquiring more data, more samples of the
    parent distribution, will not fill in every part of the feature space but will
    create a more and more complete picture of what the parent distribution looks
    like in the feature space.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这个分类器关键依赖于足够的训练数据，以充分填充输入特征空间。如果有三个特征，那么空间就是三维的，训练数据将填充到该空间中的某个立方体里。我们拥有的训练数据越多，立方体中的样本就越多，分类器的表现就越好。这是因为分类器通过衡量训练数据中的点与新的、未知的特征向量之间的距离来进行投票，决定分配哪个标签。空间中训练点的密度越大，投票过程成功的频率就越高。宽泛地说，数据增强填补了这个空间。对于大多数数据集，获取更多的数据，即更多的父分布样本，并不能填补特征空间的每个部分，但它会逐渐呈现出父分布在特征空间中的完整图景。
- en: When we work with modern deep learning models ([Chapter 12](ch12.xhtml#ch12)),
    we’ll see that data augmentation has additional benefits. During training, a neural
    network becomes conditioned to learn features of the training data. If the features
    the network learns to pay attention to are actually useful for distinguishing
    the classes, all is well. But, as we saw with the wolf and husky example of [Chapter
    4](ch04.xhtml#ch04), sometimes the network learns the wrong thing, which can’t
    be used to generalize to new inputs—like the fact that the wolf class images had
    snow in the background and the husky images did not.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用现代深度学习模型时（[第12章](ch12.xhtml#ch12)），我们会发现数据增强还有额外的好处。在训练过程中，神经网络会被训练去学习训练数据的特征。如果网络学习到的特征确实有助于区分不同类别，一切都很好。但正如我们在[第4章](ch04.xhtml#ch04)中看到的狼和哈士奇的例子，有时网络会学到错误的特征，这些特征不能用于对新输入进行泛化——比如狼类图像的背景有雪，而哈士奇图像则没有。
- en: Taking steps to avoid this tendency is known as *regularization*. Regularization
    helps the network learn important features of the training data, ones that generalize
    as we want them to. Data augmentation is—short of acquiring more actual data—perhaps
    the simplest way to regularize the network as it learns. It conditions the learning
    process to not pay attention to quirks of the particular samples selected for
    the training set but to instead focus on more general features of the data. At
    least, that is the hope.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 避免这种倾向的措施被称为*正则化*。正则化帮助网络学习训练数据中重要的特征，这些特征可以像我们希望的那样进行泛化。数据增强——除了获取更多的实际数据——或许是正则化网络学习过程的最简单方式。它使得学习过程不去关注选定训练集中特定样本的怪癖，而是转而关注数据的更一般性特征。至少，这是我们希望的结果。
- en: An additional benefit of data augmentation is that it lessens the likelihood
    of overfitting when training. We’ll discuss overfitting more in [Chapter 9](ch09.xhtml#ch09),
    but succinctly, it’s what happens when the model learns the training data nearly
    perfectly without learning to generalize to new inputs. Using a small dataset
    can lead to overfitting if the model is able to basically memorize the training
    data. Data augmentation increases the dataset size, reducing the probability of
    overfitting and possibly allowing use of a model with a larger capacity. (Capacity
    is a nebulous concept. Think “bigger,” in that the model can learn more of what
    is important in the training data, while still generalizing to new data.)
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强的另一个好处是它减少了训练时过拟合的可能性。我们将在[第9章](ch09.xhtml#ch09)中进一步讨论过拟合，但简而言之，过拟合指的是模型几乎完美地记住训练数据，但没有学会对新输入进行泛化。当模型能够基本记住训练数据时，使用小数据集可能会导致过拟合。数据增强增加了数据集的大小，降低了过拟合的概率，并且可能允许使用一个具有更大容量的模型。（容量是一个模糊的概念，可以理解为“更大”，即模型能够学习训练数据中更多重要的部分，同时仍能对新数据进行泛化。）
- en: 'One extremely important point needs to be made about data augmentation as it
    relates to the training/validation/test split of the dataset: you *must* be sure
    that every augmented sample belongs to the same set. For example, if we augment
    sample *X*[12345], and this sample has been assigned to the training set, then
    we must ensure that *all* of the augmented samples based on *X*[12345] are also
    members of the training set. This is so important that it’s worth reiterating:
    *be sure to never mix an augmented sample based on an original sample between
    the training, validation, and test sets.*'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个关于数据增强与训练/验证/测试数据集划分之间的极其重要的要点：你**必须**确保每个增强的样本都属于同一数据集。例如，如果我们增强了样本*X*[12345]，并且这个样本被分配到训练集中，那么我们必须确保基于*X*[12345]增强的**所有**样本也都属于训练集。这一点非常重要，值得再次强调：*确保绝对不要将基于原始样本的增强样本混入训练集、验证集和测试集中。*
- en: If we don’t follow this rule, our beliefs about the quality of the model will
    be unfounded, or at least partially unwarranted, because there will be samples
    in the validation and test sets that are, essentially, also in the training set,
    since they’re based on the training data. This warning may seem unnecessary, but
    it’s really easy to make this mistake, especially if working with others or with
    a database of some kind.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不遵守这个规则，我们对模型质量的信任将是没有根据的，或者至少是部分不合理的，因为在验证集和测试集中将会出现一些样本，这些样本本质上也出现在训练集中，因为它们是基于训练数据生成的。这个警告可能看起来不必要，但实际上很容易犯这个错误，尤其是在与他人合作或使用某种数据库时。
- en: The correct way to augment data is *after* the training, validation, and test
    splits have been made. Then, augment at least the training data and label all
    the new samples as training data.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 增强数据的正确方式是**在**训练集、验证集和测试集划分之后进行。然后，至少增强训练数据，并将所有新样本标记为训练数据。
- en: What about augmenting the validation and test splits? It isn’t wrong to do so,
    and might make sense if you don’t have a lot of either. I haven’t run across any
    studies that tried to be rigorous about the effects of augmenting the validation
    and test data, but, conceptually, it shouldn’t hurt, and might even help.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，增强验证集和测试集怎么样？这样做并没有错，如果你没有太多数据的话，可能也有意义。我还没有遇到过对增强验证集和测试数据的效果进行严格研究的论文，但从概念上讲，这样做应该不会造成坏影响，甚至可能有所帮助。
- en: Ways to Augment Training Data
  id: totrans-181
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 增强训练数据的方法
- en: To augment a dataset, we need to generate new samples from it that are plausible,
    meaning they could really occur in the dataset. For images, this is straightforward;
    you can often rotate the image, or flip it horizontally or vertically. Other times,
    you can manipulate the pixels themselves to change the contrast or alter the colors.
    Some have even gone so far as to simply swap entire color bands—swapping the red
    channel with the blue channel, for example.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 要增强一个数据集，我们需要从中生成新的、合理的样本，也就是说，它们在数据集中确实可能出现。对于图像来说，这很简单；你可以旋转图像，或者水平或垂直翻转它。有时，你也可以操作像素本身来改变对比度或调整颜色。有些人甚至将颜色通道完全交换——例如将红色通道和蓝色通道交换。
- en: Of course, the manipulations must make sense. A subtle rotation might mimic
    a change in the camera’s orientation, and a left-to-right flip might mimic the
    experience of looking in a mirror. But a top-to-bottom flip probably wouldn’t
    be as realistic. True, a monkey might hang upside-down in a picture, but flipping
    the picture would flip the tree and the ground, as well. On the other hand, you
    might be able to do a top-to-bottom flip in an aerial image, which shows objects
    in any orientation.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这些操作必须是合理的。细微的旋转可能模拟相机角度的变化，左右翻转可能模拟看镜子的体验。但上下翻转可能就不那么现实了。的确，一只猴子可能会倒挂在图片中，但翻转图片也会翻转树木和地面。另一方面，你可能能在航拍图像中进行上下翻转，因为它展示的物体可以是任何方向。
- en: Okay, so images are generally straightforward to augment, and it’s easy to understand
    whether the augmentation makes sense. Augmentation of a feature vector is more
    subtle. It’s not always clear how to do it, or if it’s even possible. What can
    we do in that case?
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，所以图像的增强通常是简单明了的，且很容易判断增强是否合理。特征向量的增强则更为微妙。它并不总是很清楚如何操作，或者是否甚至可能做到。那么在这种情况下我们该怎么办呢？
- en: Again, the guiding principle is that the augmentation makes sense. If we encoded
    color as a one-hot vector of, say, red, green, or blue, and an instance of a class
    can be red or green or blue, then one way to augment is to shift the color between
    red, green, and blue. If a sample can represent male or female, then we could
    also change those values to get a new sample of the same class but with a different
    gender.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，指导原则是增强必须有意义。如果我们将颜色编码为红色、绿色或蓝色的独热向量，而某个类别的实例可以是红色、绿色或蓝色，那么一种增强方式是改变颜色在红色、绿色和蓝色之间进行切换。如果一个样本可以表示男性或女性，那么我们也可以改变这些值，得到一个相同类别但性别不同的新样本。
- en: These are unusual things to do, however. Typically, you try to augment continuous
    values, creating a new feature vector that still represents the original class.
    We’ll examine one way to do this next by augmenting the iris dataset. After that,
    we’ll augment the CIFAR-10 dataset to see how to work with images.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些做法比较不常见。通常情况下，你会尝试增强连续值，创建一个新的特征向量，仍然代表原始类别。接下来，我们将通过增强鸢尾花数据集来探讨一种方法。之后，我们将增强CIFAR-10数据集，看看如何处理图像。
- en: Augmenting the Iris Dataset
  id: totrans-187
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 扩展鸢尾花数据集
- en: The iris dataset has 150 samples from three classes, each with four continuous
    features. We’ll augment it by using *principal component analysis (PCA)*. This
    is an old technique, in use for over a century. It was common in machine learning
    before the advent of deep learning to combat the curse of dimensionality, because
    it can reduce the number of features in a dataset. It also has a variety of uses
    outside of machine learning.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 鸢尾花数据集包含来自三个类别的150个样本，每个样本有四个连续的特征。我们将通过使用*主成分分析（PCA）*来增强它。这是一种使用了超过一个世纪的旧技术。在深度学习出现之前，它在机器学习中常用于应对维度灾难，因为它能够减少数据集中的特征数量。它也在机器学习以外有许多其他用途。
- en: Imagine that we have a dataset with only two features—for example, the first
    two features of the iris dataset. A scatter plot of these features will show us
    where the samples fall in 2D space. [Figure 5-5](ch05.xhtml#ch5fig5) shows a plot
    of the first two features of the iris dataset for classes 1 and 2\. The plot has
    shifted the origin to (0,0) by subtracting the mean value of each feature. This
    does not change the variance or scatter of the data, only its origin.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个只有两个特征的数据集——例如，鸢尾花数据集的前两个特征。这些特征的散点图将展示样本在二维空间中的分布。[图 5-5](ch05.xhtml#ch5fig5)展示了鸢尾花数据集前两个特征的散点图，涵盖类别1和2。该图通过减去每个特征的均值，将原点移至(0,0)。这不会改变数据的方差或散布，只是改变了原点位置。
- en: 'The plot in [Figure 5-5](ch05.xhtml#ch5fig5) also shows two arrows. These are
    the two principal components of the data. Since the data is 2D, we have two components.
    If we had 100 features, then we would have up to 100 principal components. This
    is what PCA does: it tells you the directions of the variance of the data. These
    directions are the *principal components*.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5-5](ch05.xhtml#ch5fig5)中的图表还显示了两条箭头。这是数据的两个主成分。由于数据是二维的，所以我们有两个成分。如果我们有100个特征，那么我们最多会有100个主成分。这就是PCA的作用：它告诉你数据方差的方向。这些方向就是*主成分*。'
- en: '![image](Images/05fig05.jpg)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/05fig05.jpg)'
- en: '*Figure 5-5: The first two iris features for classes 1 and 2, with their principal
    components*'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 5-5：类别1和2的鸢尾花前两个特征及其主成分*'
- en: 'The principal components also tell you how much of the variance of the data
    is explained by each of these directions. In the plot, the length of the arrow
    corresponds to the fraction of the total variance explained by each component.
    As you can see, the largest component is along the diagonal that matches the greatest
    scatter of the points. Traditional machine learning uses PCA to reduce the number
    of features while still, hopefully, representing the dataset well. This is how
    PCA can help fight the curse of dimensionality: find the principal components
    and then throw the less influential ones away. However, for data augmentation,
    we want to keep all the components.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 主成分还告诉你每个方向解释的数据方差的多少。在图中，箭头的长度对应于每个成分解释的总方差的比例。正如你所看到的，最大的成分沿着与数据点最大散布匹配的对角线方向。传统的机器学习使用PCA来减少特征数量，同时仍希望能够很好地表示数据集。这就是PCA如何帮助解决维度灾难的问题：找到主成分，然后丢弃那些影响较小的成分。然而，对于数据增强，我们希望保留所有的成分。
- en: The code that produced [Figure 5-5](ch05.xhtml#ch5fig5) is in [Listing 5-5](ch05.xhtml#ch5lis5).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 生成[图 5-5](ch05.xhtml#ch5fig5)的代码在[清单 5-5](ch05.xhtml#ch5lis5)中。
- en: import numpy as np
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: import matplotlib.pylab as plt
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: import matplotlib.pylab as plt
- en: from sklearn import decomposition
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn import decomposition
- en: ❶ x = np.load("../data/iris/iris_features.npy")[:,:2]
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ x = np.load("../data/iris/iris_features.npy")[:,:2]
- en: y = np.load("../data/iris/iris_labels.npy")
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: y = np.load("../data/iris/iris_labels.npy")
- en: idx = np.where(y != 0)
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: idx = np.where(y != 0)
- en: x = x[idx]
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: x = x[idx]
- en: x[:,0] -= x[:,0].mean()
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: x[:,0] -= x[:,0].mean()
- en: x[:,1] -= x[:,1].mean()
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: x[:,1] -= x[:,1].mean()
- en: ❷ pca = decomposition.PCA(n_components=2)
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ pca = decomposition.PCA(n_components=2)
- en: pca.fit(x)
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: pca.fit(x)
- en: v = pca.explained_variance_ratio_
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: v = pca.explained_variance_ratio_
- en: ❸ plt.scatter(x[:,0],x[:,1],marker='o',color='b')
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ plt.scatter(x[:,0],x[:,1],marker='o',color='b')
- en: ax = plt.axes()
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: ax = plt.axes()
- en: x0 = v[0]*pca.components_[0,0]
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: x0 = v[0]*pca.components_[0,0]
- en: y0 = v[0]*pca.components_[0,1]
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: y0 = v[0]*pca.components_[0,1]
- en: ax.arrow(0, 0, x0, y0, head_width=0.05, head_length=0.1, fc='r', ec='r')
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ax.arrow(0, 0, x0, y0, head_width=0.05, head_length=0.1, fc='r', ec='r')
- en: x1 = v[1]*pca.components_[1,0]
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: x1 = v[1]*pca.components_[1,0]
- en: y1 = v[1]*pca.components_[1,1]
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: y1 = v[1]*pca.components_[1,1]
- en: ax.arrow(0, 0, x1, y1, head_width=0.05, head_length=0.1, fc='r', ec='r')
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: ax.arrow(0, 0, x1, y1, head_width=0.05, head_length=0.1, fc='r', ec='r')
- en: plt.xlabel("$x_0$", fontsize=16)
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: plt.xlabel("$x_0$", fontsize=16)
- en: plt.ylabel("$x_1$", fontsize=16)
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: plt.ylabel("$x_1$", fontsize=16)
- en: plt.show()
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: plt.show()
- en: '*Listing 5-5: Iris PCA plot*'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '*示例5-5：鸢尾花PCA图*'
- en: Much of the preceding code is to make the plot ❸. The imports are standard except
    for a new one from sklearn, the decomposition module. We load the iris dataset
    we previously saved, keeping only the first two features in x and the labels in
    y. We then keep only class 1 and class 2 features by excluding class 0\. Next,
    we subtract the per feature means to center the data about the point (0,0) ❶.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的大部分是为了生成图表❸。导入部分是标准的，唯一不同的是新增了一个来自sklearn的`decomposition`模块。我们加载了之前保存的鸢尾花数据集，只保留x中的前两个特征以及y中的标签。然后，我们通过排除类别0，只保留类别1和类别2的特征。接下来，我们减去每个特征的均值，使数据围绕点(0,0)居中❶。
- en: 'Then we create the PCA object and fit the iris data to it ❷. There are two
    features, so the number of components in this case is also two. The PCA Python
    class mimics the standard approach of sklearn: it defines the model, then fits
    data to it. Once this is done, we have the principal components stored in pca
    and accessible via the components_ member variable. We set v to a vector representing
    the fraction of the variance in the data explained by each of the principal component
    directions. Since there are two components, this vector also has two components.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们创建PCA对象并将鸢尾花数据拟合到该对象上❷。有两个特征，因此此时的主成分数目也是两个。PCA Python类模仿了sklearn的标准方法：首先定义模型，然后将数据拟合到模型上。一旦完成，我们就可以通过`components_`成员变量访问到存储在pca中的主成分。我们将v设置为一个向量，表示数据中由每个主成分方向解释的方差比例。由于有两个主成分，这个向量也有两个分量。
- en: The components are always listed in decreasing order, so that the first component
    is the direction describing the majority of the variance, the second component
    is the next most important, and so on. In this case, the first component describes
    some 84 percent of the variance and the second describes the remaining 16 percent.
    We’ll use this ordering when we generate new augmented samples. Here we use the
    fraction to scale the length of the arrows in the plot showing the principal component
    directions and relative importance.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 主成分总是按降序排列，因此第一个主成分是描述大部分方差的方向，第二个主成分是下一个最重要的方向，依此类推。在这个例子中，第一个主成分描述了大约84%的方差，第二个主成分描述了剩余的16%。我们将在生成新的增强样本时使用这种排序。这里，我们使用方差比例来缩放图中表示主成分方向及其相对重要性的箭头长度。
- en: How is [Figure 5-5](ch05.xhtml#ch5fig5) useful for data augmentation? Once you
    know the principal components, you can use PCA to create derived variables, which
    means you rotate the data to align it with the principal components. The transform
    method of the PCA class does this by mapping an input—in our case, the original
    data—to a new representation where the variance is aligned with the principal
    components. This mapping is exact, and you can reverse it by using the inverse_transform
    method.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '[图5-5](ch05.xhtml#ch5fig5)如何用于数据增强？一旦你知道了主成分，就可以使用PCA创建派生变量，这意味着你将数据旋转，使其与主成分对齐。PCA类的`transform`方法通过将输入（在我们这个例子中是原始数据）映射到一个新的表示形式来完成这一步，使得方差与主成分对齐。这个映射是精确的，你可以使用`inverse_transform`方法将其反转。'
- en: Doing this alone doesn’t generate new samples for us. If we take the original
    data, x, transform it to the new representation, and then inverse transform it,
    we’ll end up where we started, with x. But, if we transform x and then, before
    calling the inverse transform, *modify* some of the principal components, we’ll
    return a new set of samples that are not x but are based on x. This is precisely
    what we want for data augmentation. Next, we’ll see which components to modify,
    and how.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅这样做并不会为我们生成新样本。如果我们将原始数据x转化为新的表示形式，然后再进行逆变换，我们最终会回到原点，得到x。但如果我们先转换x，然后在调用逆变换之前，*修改*一些主成分，我们将得到一组新的样本，这些样本不是x，但基于x。这正是我们所需要的数据增强。接下来，我们将看到修改哪些组件以及如何修改。
- en: The components are ordered in pca by their importance. We want to keep the most
    important components as they are, because we want the inverse transform to produce
    data that looks much like the original data. We don’t want to transform things
    too much, or the new samples won’t be plausible instances of the class we claim
    they represent. We’ll arbitrarily say that we want to keep the components that,
    cumulatively, represent some 90 percent to 95 percent of the variance in the data.
    These we won’t modify at all. The remaining components will be modified by adding
    normally distributed noise. Recall that *normally distributed* means it follows
    the bell curve so that most of the time the value will be near the middle, which
    we’ll set to 0, meaning no change to the component, and increasingly rarely to
    larger values. We’ll add the noise to the existing component and call the inverse
    transform to produce new samples that are very similar but not identical to the
    originals.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: pca中的组件按其重要性排序。我们希望保持最重要的组件不变，因为我们希望逆变换能产生与原始数据相似的数据。我们不希望对数据进行过多的变换，否则新样本将不再是我们所声称的类别的合理实例。我们可以任意地设定，保留那些累计表示数据方差约90%到95%的组件，这些组件我们将完全不做修改。剩余的组件将通过添加正态分布的噪声进行修改。回想一下，*正态分布*意味着数据遵循钟形曲线，大部分时间值会接近中间值，我们将中间值设为0，表示对该组件不做改变，只有极少数情况下才会有较大的值。我们将把噪声添加到现有的组件中，并调用逆变换来生成非常相似但不完全相同的新样本。
- en: The previous paragraph is pretty dense. The code will make things easier to
    understand. Our approach to generating augmented data is shown in [Listing 5-6](ch05.xhtml#ch5lis6).
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 前一段内容相当复杂，代码将使理解变得更加容易。我们生成增强数据的方法在[示例 5-6](ch05.xhtml#ch5lis6)中展示。
- en: import numpy as np
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: from sklearn import decomposition
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn import decomposition
- en: '❶ def generateData(pca, x, start):'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '❶ def generateData(pca, x, start):'
- en: original = pca.components_.copy()
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: original = pca.components_.copy()
- en: ncomp = pca.components_.shape[0]
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: ncomp = pca.components_.shape[0]
- en: a = pca.transform(x)
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: a = pca.transform(x)
- en: 'for i in range(start, ncomp):'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i in range(start, ncomp):'
- en: pca.components_[i,:] += np.random.normal(scale=0.1, size=ncomp)
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: pca.components_[i,:] += np.random.normal(scale=0.1, size=ncomp)
- en: b = pca.inverse_transform(a)
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: b = pca.inverse_transform(a)
- en: pca.components_ = original.copy()
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: pca.components_ = original.copy()
- en: return b
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: return b
- en: 'def main():'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 'def main():'
- en: ❷ x = np.load("../../../data/iris/iris_features.npy")
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ x = np.load("../../../data/iris/iris_features.npy")
- en: y = np.load("../../../data/iris/iris_labels.npy")
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: y = np.load("../../../data/iris/iris_labels.npy")
- en: N = 120
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: N = 120
- en: x_train = x[:N]
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: x_train = x[:N]
- en: y_train = y[:N]
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: y_train = y[:N]
- en: x_test = x[N:]
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: x_test = x[N:]
- en: y_test = y[N:]
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: y_test = y[N:]
- en: pca = decomposition.PCA(n_components=4)
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: pca = decomposition.PCA(n_components=4)
- en: pca.fit(x)
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: pca.fit(x)
- en: print(pca.explained_variance_ratio_)
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: print(pca.explained_variance_ratio_)
- en: start = 2
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: start = 2
- en: ❸ nsets = 10
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ nsets = 10
- en: nsamp = x_train.shape[0]
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: nsamp = x_train.shape[0]
- en: newx = np.zeros((nsets*nsamp, x_train.shape[1]))
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: newx = np.zeros((nsets*nsamp, x_train.shape[1]))
- en: newy = np.zeros(nsets*nsamp, dtype="uint8")
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: newy = np.zeros(nsets*nsamp, dtype="uint8")
- en: '❹ for i in range(nsets):'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '❹ for i in range(nsets):'
- en: 'if (i == 0):'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 'if (i == 0):'
- en: newx[0:nsamp,:] = x_train
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: newx[0:nsamp,:] = x_train
- en: newy[0:nsamp] = y_train
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: newy[0:nsamp] = y_train
- en: 'else:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 'else:'
- en: newx[(i*nsamp):(i*nsamp+nsamp),:] =
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: newx[(i*nsamp):(i*nsamp+nsamp),:] =
- en: generateData(pca, x_train, start)
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: generateData(pca, x_train, start)
- en: newy[(i*nsamp):(i*nsamp+nsamp)] = y_train
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: newy[(i*nsamp):(i*nsamp+nsamp)] = y_train
- en: ❺ idx = np.argsort(np.random.random(nsets*nsamp))
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ idx = np.argsort(np.random.random(nsets*nsamp))
- en: newx = newx[idx]
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: newx = newx[idx]
- en: newy = newy[idx]
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: newy = newy[idx]
- en: np.save("iris_train_features_augmented.npy", newx)
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("iris_train_features_augmented.npy", newx)
- en: np.save("iris_train_labels_augmented.npy", newy)
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("iris_train_labels_augmented.npy", newy)
- en: np.save("iris_test_features_augmented.npy", x_test)
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("iris_test_features_augmented.npy", x_test)
- en: np.save("iris_test_labels_augmented.npy", y_test)
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("iris_test_labels_augmented.npy", y_test)
- en: main()
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: main()
- en: '*Listing 5-6: Augmenting the iris data with PCA. See* iris_data_augmentation.py.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '*示例 5-6：使用PCA增强鸢尾花数据。请参见* iris_data_augmentation.py。'
- en: 'The main function ❷ loads the existing iris data, x, and the corresponding
    labels, y, and then calls PCA, this time using all four features of the dataset.
    This gives us the four principal components telling us how much of the variance
    is explained by each component:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 主函数❷加载现有的鸢尾花数据x和对应的标签y，然后调用PCA，这次使用数据集的所有四个特征。这样，我们就得到了四个主成分，告诉我们每个成分解释了方差的多少：
- en: 0.92461621 0.05301557 0.01718514 0.00518309
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 0.92461621 0.05301557 0.01718514 0.00518309
- en: The first two principal components describe over 97 percent of the variance.
    Therefore, we’ll leave the first two components alone, indices 0 and 1, and start
    with index 2 when we want to generate new samples.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 前两个主成分描述了97%以上的方差。因此，我们将保持前两个组件不变，即索引0和1，当我们要生成新样本时，从索引2开始。
- en: We next declare the number of sets we’ll define ❸. A *set* here means a new
    collection of samples. Since the samples are based on the original data, x, with
    150 samples, each new set will contain 150 samples as well. In fact, they’ll be
    in the same order as the original samples, so that the class label that should
    go with each of these new samples is in the same order as the class labels in
    y. We don’t want to lose our original data, either, so nsets=10 puts the original
    data and nine new sets of samples based on that original data—for a total of 1,500
    samples—in the new dataset. We grab the number of samples in x, 150, and define
    the arrays to hold our new features (newx) and associated labels (newy).
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们声明将要定义的集合数❸。这里的*集合*指的是一组新的样本集合。由于这些样本基于原始数据x，其中有150个样本，因此每个新集合也会包含150个样本。事实上，它们将与原始样本顺序相同，因此这些新样本的类别标签与y中的类别标签顺序一致。我们也不想丢失原始数据，因此nsets=10将原始数据和基于原始数据生成的九个新样本集合—总共1,500个样本—放入新数据集中。我们获取x中的样本数量150，并定义数组来存放我们的新特征(newx)和关联的标签(newy)。
- en: Next, we loop to generate the new samples, one set of 150 at a time ❹. The first
    pass simply copies the original data into the output arrays. The remaining passes
    are similar, updating the source and destination indices of the output arrays
    appropriately, but instead of assigning x, we assign the output of generateData.
    When the loop is done, we scramble the order of the entire dataset and write it
    to disk ❺.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们循环生成新的样本，每次生成一组150个样本❹。第一次循环只是将原始数据复制到输出数组中。后续的循环类似，适当更新输出数组的源和目标索引，但我们不再直接赋值给x，而是赋值为generateData的输出。当循环完成后，我们将整个数据集的顺序打乱并写入磁盘❺。
- en: All of the magic is in generateData ❶. We pass in the PCA object (pca), the
    original data (x), and the starting principal component index (start). We set
    the last argument to 2 to leave the two most important components alone. We keep
    a copy of the actual components so we can reset the pca object before we return.
    Then we define ncomp, the number of principal components, for convenience and
    call the forward transformation mapping the original data along the principal
    components.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的魔法都在generateData❶中。我们传入PCA对象(pca)、原始数据(x)以及起始主成分索引(start)。我们将最后一个参数设置为2，以保持最重要的两个组件不变。我们保留实际组件的副本，以便在返回之前重置pca对象。然后我们定义ncomp，即主成分的数量，便于使用，并调用前向变换，将原始数据映射到主成分上。
- en: The loop updates the two least important components by adding a random value
    drawn from a normal curve with mean value 0 and a standard deviation of 0.1\.
    Why 0.1? No special reason; if the standard deviation is small, then the new samples
    will be near the old samples, while if it’s larger, they’ll be farther away and
    possibly not representative of the class anymore. Next, we call the inverse transformation
    using the modified principal components and restore the actual components. Finally,
    we return the new set of samples.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 循环通过添加一个从均值为0，标准差为0.1的正态曲线中抽取的随机值，更新两个最不重要的组件。为什么是0.1？没有特别的原因；如果标准差较小，则新样本会接近旧样本，而如果标准差较大，则新样本会更远，可能不再具有代表性。接下来，我们使用修改后的主成分调用逆变换，并恢复实际的组件。最后，我们返回新的样本集。
- en: Let’s look at the new dataset, shown in [Figure 5-6](ch05.xhtml#ch5fig6). The
    big gray dots are from our original dataset, and the smaller black dots are the
    augmented samples. As we can readily see, they all fall near an existing sample,
    which is what we would expect from modifying only the weakest of the principal
    components. Since we copied the original data into the augmented dataset, each
    big dot has a small dot at the center.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下新的数据集，如[图 5-6](ch05.xhtml#ch5fig6)所示。大的灰色点来自我们原始的数据集，而较小的黑色点是增强后的样本。如我们所见，它们都靠近现有的样本，这是我们从仅修改最弱的主成分所预期的结果。由于我们将原始数据复制到了增强数据集中，因此每个大点的中心都有一个小点。
- en: '![image](Images/05fig06.jpg)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/05fig06.jpg)'
- en: '*Figure 5-6: The first two features of the original iris dataset (large dots)
    and the augmented features generated by [Listing 5-6](ch05.xhtml#ch5lis6) (small
    points)*'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 5-6：原始鸢尾花数据集的前两个特征（大点）和由[清单 5-6](ch05.xhtml#ch5lis6)生成的增强特征（小点）*'
- en: This approach is appropriate for continuous features only, as was previously
    stated, and you should be careful to modify only the weakest of the principal
    components, and only by a small amount. Experimentation is important here. As
    an exercise, try applying the same technique to augment the breast cancer dataset,
    which also consists of continuous features.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，这种方法仅适用于连续特征，且你应该小心只修改最弱的主成分，并且仅修改一个小量。这里的实验非常重要。作为练习，尝试将相同的技术应用于增强乳腺癌数据集，它也由连续特征组成。
- en: Augmenting the CIFAR-10 Dataset
  id: totrans-281
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 增强 CIFAR-10 数据集
- en: Augmenting the iris dataset involved a lot of discussion and some less than
    obvious math. Fortunately for us, augmenting images is generally a lot simpler,
    but still just as effective when training modern models. When we build convolutional
    neural network models ([Chapter 12](ch12.xhtml#ch12)), we’ll see how to do augmentation
    on the fly when training, a particularly helpful approach, but for now we’ll do
    the augmentation first and build a new dataset with additional versions of the
    existing images.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 增强鸢尾花数据集涉及了很多讨论和一些不太明显的数学运算。幸运的是，增强图像通常要简单得多，但在训练现代模型时仍然同样有效。当我们构建卷积神经网络模型（[第12章](ch12.xhtml#ch12)）时，我们将看到如何在训练时动态进行增强，这是一种特别有用的方法，但现在我们先进行增强，并通过现有图像的额外版本构建一个新的数据集。
- en: '[Figure 5-4](ch05.xhtml#ch5fig4) shows representative images from each class
    in the CIFAR-10 dataset. These are color images stored as RGB data for the red,
    green, and blue channels. They were taken from ground level, so top and bottom
    flips do not make sense here, while left and right flips do. Translations—shifting
    the image in the *x* or *y* direction, or both—are one common technique. Small
    rotations are another common technique.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5-4](ch05.xhtml#ch5fig4)显示了CIFAR-10数据集中每个类别的代表性图像。这些是存储为RGB数据的彩色图像，分别表示红色、绿色和蓝色通道。它们是从地面拍摄的，因此上下翻转在这里没有意义，而左右翻转是可以的。平移——将图像在
    *x* 或 *y* 方向上，或同时在两个方向上移动——是一种常见的技术。小角度旋转是另一种常见技术。'
- en: 'However, each of these raises an issue: what to do with pixels that have no
    data after the shift or rotate? If I shift an image 3 pixels to the left, I need
    to fill in the three columns on the right with something. Or, if I rotate to the
    right, there will be pixels at the upper right and lower left that need to be
    filled in. There are several ways to handle this. One is to simply leave the pixels
    black, or all 0 values, and let the model learn that there is no helpful information
    there. Another is to replace the pixels with the mean value of the image, which
    also provides no information and will, we hope, be ignored by the model. However,
    the most popular solution is to crop the image.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，每个问题都提出了一个问题：在位移或旋转后，对于没有数据的像素该怎么办？如果我将图像向左移动 3 个像素，我需要用某些东西填充右侧的三列。或者，如果我向右旋转，就会有右上角和左下角的像素需要填充。有几种方法可以处理这种情况。一种方法是简单地将这些像素保持为黑色，或全
    0 值，并让模型学习这些地方没有有用的信息。另一种方法是将像素替换为图像的均值，这也不会提供任何信息，我们希望模型能忽略它。然而，最流行的解决方案是裁剪图像。
- en: The image is 32×32 pixels. Pulling a random patch from the image of, say, 28×28
    pixels is the equivalent of shifting the image by a random *x* or *y* position
    of up to 4 pixels without needing to worry about filling in anything. If we rotate
    the image first, which will require interpolation of the pixels, and then crop
    to remove the edge regions, we’ll again have no empty pixels to worry about. Keras
    has tools for doing this via an image generator object used during training. When
    we’re using Keras to build models, we’ll make use of it, but for now, we’ll do
    all of the work ourselves in order to understand the process.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图片是32×32像素。比如，从图像中提取一个28×28像素的随机补丁，相当于将图像沿* x * 或 * y * 方向随机移动最多4个像素，而不需要担心填充问题。如果我们首先旋转图像（需要对像素进行插值），然后裁剪去除边缘区域，我们也不需要担心有空白像素。Keras
    提供了一个用于训练时的图像生成器对象来实现这一过程。当我们使用 Keras 构建模型时，会使用到这个工具，但目前我们自己动手做所有工作，以便理解这个过程。
- en: We need to mention one point here. So far, we’ve talked about building a dataset
    for training a model. What should we do when we want to use the model? Do we hand
    the model random croppings of the test inputs as well? No. Instead, we hand the
    model a cropping centered on the image. So, for CIFAR-10, we would take each 32
    × 32 test input and crop it to 28 × 28 by dropping the outer 6 pixels, then present
    that to the model. We do this because the center crop still represents the actual
    test image and not some augmented version of it.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要在这里提到一点。到目前为止，我们讨论了如何构建用于训练模型的数据集。那么，当我们想要使用模型时应该怎么办呢？我们是否也将随机裁剪的测试输入交给模型？不是。相反，我们将一个以图像为中心的裁剪交给模型。因此，对于
    CIFAR-10，我们会将每个 32 × 32 的测试输入裁剪成 28 × 28，去掉外部的 6 个像素，然后将其提供给模型。我们这样做是因为中心裁剪仍然代表实际的测试图像，而不是某个增强版本的图像。
- en: '[Figure 5-7](ch05.xhtml#ch5fig7) illustrates what we mean by rotations, flips,
    random croppings for training, and center cropping for testing. In (a) we rotate
    the image and take a center crop. The output image is in the white square. In
    (b) we flip left to right and crop randomly. In (c), we take two random crops
    without flipping, and in (d) we take a center crop for testing, without any rotation
    or flip. Some people augment test images, but we won’t do so here.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5-7](ch05.xhtml#ch5fig7)展示了我们所说的旋转、翻转、训练时的随机裁剪和测试时的中心裁剪。在（a）中，我们旋转图像并进行中心裁剪。输出图像位于白色方框中。在（b）中，我们进行左右翻转并随机裁剪。在（c）中，我们进行两次随机裁剪，但不进行翻转，而在（d）中，我们进行测试时的中心裁剪，不进行旋转或翻转。有些人会增强测试图像，但我们这里不会这么做。'
- en: '![image](Images/05fig07.jpg)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/05fig07.jpg)'
- en: '*Figure 5-7: Rotate, then center crop (a). Flip left to right, then crop randomly
    (b). Two random crops during training (c). Center crop for testing, with no rotation
    or flip (d).*'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 5-7：首先旋转，然后中心裁剪（a）。从左到右翻转，然后随机裁剪（b）。训练时的两个随机裁剪（c）。测试时的中心裁剪，无旋转或翻转（d）。*'
- en: '[Listing 5-7](ch05.xhtml#ch5lis7) shows how to augment the CIFAR-10 training
    set with random crops, rotations, and flips.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单 5-7](ch05.xhtml#ch5lis7)展示了如何通过随机裁剪、旋转和翻转来增强 CIFAR-10 训练集。'
- en: import numpy as np
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: from PIL import Image
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: from PIL import Image
- en: '❶ def augment(im, dim):'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '❶ def augment(im, dim):'
- en: img = Image.fromarray(im)
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: img = Image.fromarray(im)
- en: 'if (np.random.random() < 0.5):'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 'if (np.random.random() < 0.5):'
- en: img = img.transpose(Image.FLIP_LEFT_RIGHT)
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: img = img.transpose(Image.FLIP_LEFT_RIGHT)
- en: 'if (np.random.random() < 0.3333):'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 'if (np.random.random() < 0.3333):'
- en: z = (32-dim)/2
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: z = (32-维度)/2
- en: r = 10*np.random.random()-5
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: r = 10*np.random.random()-5
- en: img = img.rotate(r, resample=Image.BILINEAR)
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: img = img.rotate(r, resample=Image.BILINEAR)
- en: img = img.crop((z,z,32-z,32-z))
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: img = img.crop((z,z,32-z,32-z))
- en: 'else:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 'else:'
- en: x = int((32-dim-1)*np.random.random())
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: x = int((32-维度-1)*np.random.random())
- en: y = int((32-dim-1)*np.random.random())
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: y = int((32-维度-1)*np.random.random())
- en: img = img.crop((x,y,x+dim,y+dim))
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: img = img.crop((x,y,x+dim,y+dim))
- en: return np.array(img)
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: return np.array(img)
- en: 'def main():'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 'def main():'
- en: ❷ x = np.load("../data/cifar10/cifar10_train_images.npy")
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ x = np.load("../data/cifar10/cifar10_train_images.npy")
- en: y = np.load("../data/cifar10/cifar10_train_labels.npy")
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: y = np.load("../data/cifar10/cifar10_train_labels.npy")
- en: factor = 10
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: factor = 10
- en: dim = 28
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: dim = 28
- en: z = (32-dim)/2
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: z = (32-维度)/2
- en: newx = np.zeros((x.shape[0]*factor, dim,dim,3), dtype="uint8")
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: newx = np.zeros((x.shape[0]*factor, dim,dim,3), dtype="uint8")
- en: newy = np.zeros(y.shape[0]*factor, dtype="uint8")
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: newy = np.zeros(y.shape[0]*factor, dtype="uint8")
- en: k=0
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: k=0
- en: '❸ for i in range(x.shape[0]):'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '❸ for i in range(x.shape[0]):'
- en: im = Image.fromarray(x[i,:])
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: im = Image.fromarray(x[i,:])
- en: im = im.crop((z,z,32-z,32-z))
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: im = im.crop((z,z,32-z,32-z))
- en: newx[k,...] = np.array(im)
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: newx[k,...] = np.array(im)
- en: newy[k] = y[i]
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: newy[k] = y[i]
- en: k += 1
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: k += 1
- en: 'for j in range(factor-1):'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 'for j in range(factor-1):'
- en: newx[k,...] = augment(x[i,:], dim)
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: newx[k,...] = augment(x[i,:], dim)
- en: newy[k] = y[i]
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: newy[k] = y[i]
- en: k += 1
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: k += 1
- en: idx = np.argsort(np.random.random(newx.shape[0]))
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: idx = np.argsort(np.random.random(newx.shape[0]))
- en: newx = newx[idx]
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: newx = newx[idx]
- en: newy = newy[idx]
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: newy = newy[idx]
- en: np.save("../data/cifar10/cifar10_aug_train_images.npy", newx)
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("../data/cifar10/cifar10_aug_train_images.npy", newx)
- en: np.save("../data/cifar10/cifar10_aug_train_labels.npy", newy)
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("../data/cifar10/cifar10_aug_train_labels.npy", newy)
- en: ❹ x = np.load("../data/cifar10/cifar10_test_images.npy")
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ x = np.load("../data/cifar10/cifar10_test_images.npy")
- en: newx = np.zeros((x.shape[0], dim,dim,3), dtype="uint8")
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: newx = np.zeros((x.shape[0], dim,dim,3), dtype="uint8")
- en: 'for i in range(x.shape[0]):'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i in range(x.shape[0]):'
- en: im = Image.fromarray(x[i,:])
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: im = Image.fromarray(x[i,:])
- en: im = im.crop((z,z,32-z,32-z))
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: im = im.crop((z,z,32-z,32-z))
- en: newx[i,...] = np.array(im)
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: newx[i,...] = np.array(im)
- en: np.save("../data/cifar10/cifar10_aug_test_images.npy", newx)
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("../data/cifar10/cifar10_aug_test_images.npy", newx)
- en: '*Listing 5-7: Augmenting the CIFAR-10 dataset. See* cifar10_augment.py.'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '*Listing 5-7: 增强CIFAR-10数据集。请参见* cifar10_augment.py。'
- en: The main function loads the existing dataset and defines our augmentation factor,
    crop size, and a constant for defining a center crop ❷.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 主函数加载现有数据集，并定义我们的增强因子、裁剪大小和用于定义中心裁剪的常量❷。
- en: 'The new image will be put in newx, which has the following dimensions: (500,000;28;28;3);
    there are 50,000 training images, each with 32×32 pixels and three color bands.
    We set the augmentation factor to 10\. Similarly, there will be 500,000 labels.
    The counter, k, will index into this new dataset. For every image in the old dataset,
    we’ll create nine completely new versions and center crop the original ❶ ❸.'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 新的图像将放入newx中，其维度为：(500,000;28;28;3)；共有50,000张训练图像，每张图像为32×32像素，并且有三个颜色通道。我们将增强因子设置为10。同样，也会有500,000个标签。计数器k将索引到这个新数据集中。对于旧数据集中的每一张图像，我们将创建九个全新的版本，并对原始图像进行中心裁剪❶❸。
- en: As the dataset consists of images, it’s easiest to work with the data in image
    form, so we make the current sample an actual PIL image in order to easily crop
    it. This is the center crop of the original image. We store it in the new output
    array.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据集由图像组成，最方便的方式是以图像的形式处理数据，因此我们将当前样本转换为实际的PIL图像，以便于裁剪。这是原始图像的中心裁剪。我们将其存储在新的输出数组中。
- en: 'There are two Python idioms here that we’ll see more than once. The first is
    to turn a NumPy array representing an image into a PIL image:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有两个Python惯用法，我们会多次看到。第一个是将表示图像的NumPy数组转换为PIL图像：
- en: im = Image.fromarray(arr)
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: im = Image.fromarray(arr)
- en: 'The second is to go the other way and turn a PIL image into a NumPy array:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法是将PIL图像转换为NumPy数组：
- en: arr = np.array(im)
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: arr = np.array(im)
- en: We must be sure that the NumPy array is a valid image data type like unsigned
    byte (uint8). Use the astype NumPy array method to cast between types, remembering
    that you bear all responsibility for understanding what that casting entails.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须确保NumPy数组是有效的图像数据类型，比如无符号字节（uint8）。使用astype NumPy数组方法在类型之间转换时，要记得你需要完全理解这种转换的含义。
- en: Referring back to [Listing 5-7](ch05.xhtml#ch5lis7), we are creating the nine
    versions of the current image. For each of these, we simply copy the label and
    assign the output array an augmented version. We’ll describe the augment function
    shortly. Once the new dataset has been constructed, we scramble the order and
    write the augmented training dataset to disk ❸.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾[Listing 5-7](ch05.xhtml#ch5lis7)，我们正在创建当前图像的九个版本。对于每一个版本，我们只需复制标签，并将增强版本分配给输出数组。稍后我们会详细描述augment函数。新数据集构建完成后，我们将打乱顺序并将增强后的训练数据集写入磁盘❸。
- en: We’re not quite done, however. We created an augmented training set that cropped
    the original 32 × 32 images to 28 × 28\. We must, therefore, at least crop the
    original test set ❹. As we stated previously, we use a center crop and no augmentation
    of the test data. Therefore, we simply load the test dataset, define the new output
    test dataset, and run a loop that crops the 32 × 32 images to 28 × 28\. When done,
    we write the cropped test data to disk. Note that we did not modify the *order*
    of the images in the test set; we simply cropped them, so we do not need to write
    a new file for the test labels.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们还没有完全完成。我们创建了一个增强的训练集，将原始的32 × 32图像裁剪为28 × 28。因此，我们必须至少裁剪原始的测试集❹。如前所述，我们使用中心裁剪，并且不对测试数据进行增强。因此，我们只需加载测试数据集，定义新的输出测试数据集，并运行一个循环，将32
    × 32的图像裁剪为28 × 28。完成后，我们将裁剪后的测试数据写入磁盘。注意，我们没有修改测试集中图像的*顺序*；我们只是裁剪了它们，因此不需要为测试标签写入新文件。
- en: The augment function ❶ is where all the action is. We immediately change the
    input NumPy array into an actual PIL image object. We next decide, with a 50-50
    chance, whether or not we will flip the image left to right. Note that we do not
    crop the image just yet.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: augment函数❶是所有操作的核心。我们立即将输入的NumPy数组转换为一个实际的PIL图像对象。接下来，我们以50%的概率决定是否左右翻转图像。请注意，我们暂时不对图像进行裁剪。
- en: Next, we ask whether we should rotate the image or not. We select rotation with
    a probability of 33 percent (1 in 3 chance). Why 33 percent? No particular reason,
    but it seems that we might want to crop randomly more often than we rotate. We
    could even drop this probability down to 20 percent (1 in 5 chance). If we do
    rotate, we select the rotation angle, [*–*5,5] and then call the rotate method
    using bilinear interpolation to make the rotated image look a bit nicer than simply
    using the nearest neighbor, which is the PIL default. Next, we center crop the
    rotated image. This way, we will not get any black pixels on the edges where the
    rotation had no image information to work with.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们要判断是否旋转图像。我们以33%的概率（1/3的机会）选择旋转。为什么是33%？没有特别的原因，不过看起来我们可能更倾向于随机裁剪，而不是旋转。我们甚至可以将这个概率降低到20%（1/5的机会）。如果进行旋转，我们选择旋转角度，[*–*5,5]，然后使用双线性插值方法调用旋转函数，以使旋转后的图像比简单的使用最近邻插值（PIL默认方式）看起来更好。接下来，我们对旋转后的图像进行中心裁剪。这样，我们就不会在旋转过程中没有图像信息的边缘区域看到黑色像素。
- en: If we do not rotate, we are free to select a random crop. We choose the upper-left
    corner of this random crop, ensuring that the cropped square will not exceed the
    dimensions of the original image. Finally, we convert the data back to a NumPy
    array and return.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不进行旋转，我们可以自由选择一个随机裁剪。我们选择这个随机裁剪的左上角，确保裁剪后的正方形不会超出原始图像的尺寸。最后，我们将数据转换回NumPy数组并返回。
- en: Summary
  id: totrans-352
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we built four datasets that we’ll use as examples throughout
    the rest of the book. The first two, irises and breast cancer histology, are based
    on feature vectors. The last two, MNIST and CIFAR-10, are represented as images.
    We then learned about two data augmentation methods: augmenting a feature vector
    of continuous values using PCA and, more critical for deep learning, augmenting
    images by basic transformations.'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们构建了四个数据集，将在接下来的章节中作为示例使用。前两个，鸢尾花数据集（irises）和乳腺癌组织学数据集（breast cancer histology），基于特征向量。最后两个，MNIST和CIFAR-10，表示为图像。然后我们学习了两种数据增强方法：使用PCA增强连续值的特征向量，以及对深度学习更为关键的，通过基本变换增强图像。
- en: In the next chapter, we’ll transition to our discussion of classical machine
    learning models. In the chapter after that, we’ll use these datasets with those
    models.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将过渡到经典机器学习模型的讨论。在接下来的章节中，我们将使用这些数据集与这些模型进行结合。
